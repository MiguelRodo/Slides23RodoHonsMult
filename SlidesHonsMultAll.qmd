---
title: Correspondence analysis
date: "2024-05-02"
bibliography: zotero.bib
format:
  beamer:
    embed-resources: true
    aspectratio: 169
    urlcolor: cyan
    linkcolor: blue
    filecolor: magenta
    include-in-header:
      file: preamble-reg.tex
nocite: |
    @johnson_wichern07
---

# Introduction to correspondence analysis (CA)

- Context: $\symbf{Y}$ (abundance)
  - Goal: Graphically display the relationships between and/or within the rows and columns

<!-- \pause !-->

:::: {.columns}

::: {.column width="45%" text-align="center"}

\begin{center} 

We go from this dataset:

\end{center}

```{r}
#| results: asis
#| echo: false
data("smoke", package = "ca")
smoke |> knitr::kable()
```

:::

::: {.column width="2.5%"}

:::

::: {.column width="52.5%" text-align="center"}

\begin{center}
to this plot:
\end{center}

\vspace{-1cm}

```{r}
#| echo: false
#| results: asis
path_pdf_cover <- projr::projr_path_get("cache", "p-cover.pdf")
pdf(path_pdf_cover, width = 3.5, height = 3.5)
ca_obj <- ca::ca(smoke)
plot(
  ca::ca(smoke), mass = TRUE, contrib = "absolute",
  map = "symmetric", arrows = rep(TRUE, 2)
  )
suppressMessages(suppressWarnings(invisible(dev.off())))

pander::pandoc.image(path_pdf_cover)
```

:::

::::

::: {.comment}
- So, the key thing so far is that the rows are not displayed in terms of how similar they are overall
  - Rather, they're displayed in terms of the similarity of their profiles
:::

# Introduction to correspondence analysis (CA)

- Context: $\symbf{Y}$ (abundance)
  - Goal: Graphically display the relationships between and/or within the rows and columns

:::: {.columns}

::: {.column width="45%" text-align="center"}

\begin{center} 

We go from this dataset:

\end{center}


```{r}
#| results: asis
#| echo: false
smoke_dem <- smoke
for (i in seq_len(nrow(smoke_dem))) {
  smoke_rep <- as.matrix(smoke_dem)[i, ] / sum(smoke_dem[i, ])
  for (j in seq_len(ncol(smoke_dem))) {
    smoke_dem[i, j] <- smoke_rep[j]
  }
}
smoke_dem |> round(2) |> knitr::kable()
```

:::

::: {.column width="2.5%"}

:::

::: {.column width="52.5%" text-align="center"}

\begin{center}
to this plot:
\end{center}

\vspace{-1cm}

```{r}
#| echo: false
#| results: asis
path_pdf <- projr::projr_path_get("cache", "p-cover.pdf")
pdf(path_pdf, width = 3.5, height = 3.5)
ca_obj <- ca::ca(smoke)
plot(
  ca::ca(smoke), mass = TRUE, contrib = "absolute",
  map = "symmetric", arrows = rep(TRUE, 2)
  )
suppressMessages(suppressWarnings(invisible(dev.off())))

pander::pandoc.image(path_pdf)
```

:::

::::

::: {.comment}
- So, the key thing so far is that the rows are not displayed in terms of how similar they are overall
  - Rather, they're displayed in terms of the similarity of their profiles
:::


# Example applications {.smaller}

**Datasets**

- Rows are various dams, and columns are counts of waterbird species
- Rows are various immune compartments (e.g. blood, spleen, lymph), and columns are frequencies of immune cell types (e.g. T cells, B cells, NK cells)
- Rows are company brands (e.g. Cadbury, Beacon, Lindt), and columns are consumer ratings on a 1-5 scale (e.g. quality, price, taste)

**Key characteristics**

- Non-negative
- Natural zero (i.e. zero means literally nothing and not simply that two quantities are equal, for example)
- Same units (e.g. counts all in thousands)

<!-- \pause !-->

The key property of the data is that proportions make sense throughout.

# Correspondence matrix, $\symbf{P}$ {.smaller}

- Suppose that we have some matrix $\symbf{X}:I \times J$ where each element 
  - Rows can be thought of as observations and columns as variables

- The correspondence matrix $\symbf{P}:I \times J$ is the matrix of overall proportions where

$$
P_{ij}= \frac{x_{ij}}{\sum_{i=1}^I \sum_{j=1}^J x_{ij}} = \frac{x_{ij}}{n}
$$

<!-- \pause !-->

:::: {.columns}

::: {.column width="45%" text-align="center"}

\begin{center} 

We go from $\symbf{X}$

\end{center}

\vspace{-0.5cm}

```{r}
#| results: asis
#| echo: false
smoke |> knitr::kable()
```

:::

::: {.column width="2.5%"}

:::

::: {.column width="52.5%" text-align="center"}

\begin{center}

to $\symbf{P}$

\vspace{-0.5cm}

\end{center}

```{r}
#| echo: false
#| results: asis
smoke_dem <- smoke
for (i in seq_len(nrow(smoke_dem))) {
  smoke_rep <- as.matrix(smoke_dem)[i, ] / sum(smoke_dem[i, ])
  for (j in seq_len(ncol(smoke_dem))) {
    smoke_dem[i, j] <- smoke_rep[j]
  }
}
n <- sum(colSums(as.matrix(smoke)))
P <- (smoke / n)
P |> round(2) |> knitr::kable()
```

:::

::::

# Independence of rows and columns

- Let $\symbf{r}$ be the vector of row totals, i.e. $r_i=\sum_{j=1}^J P_{ij} = \symbf{P} \symbf{1}$
- Let $\symbf{c}$ be the vector of column totals, i.e. $c_j=\sum_{i=1}^I P_{ij} = \symbf{P}' \symbf{1}$
- Then if the rows are independent of the cells, we have that

\begin{align*}
p_{ij} &= r_ic_j,
\implies \symbf{P}_{\mathrm{ind}} = \symbf{r}\symbf{c}'
\end{align*}

```{r}
#| echo: false
#| results: asis
r_vec <- matrix(rowSums(P), ncol = 1)
c_vec <- matrix(colSums(P), ncol = 1)
P_ind <- r_vec %*% t(c_vec)
P_ind_disp <- P
for (i in seq_len(nrow(P))) {
  for (j in seq_len(ncol(P))) {
    P_ind_disp[i, j] <- P_ind[i,j] |> round(2)
  }
}
knitr::kable(P_ind_disp)
```

# Matrix of residuals
- Under the assumption of independence, we can calculate residuals:

$$
\symbf{P} - \symbf{P}_{\mathrm{ind}} = \symbf{P} - \symbf{r}\symbf{c}'.
$$

<!-- \pause !-->

- Continuing the smoking example, we then have

:::: {.columns}

::: {.column width="45%" text-align="center"}

\begin{center} 

$\symbf{P}$

\end{center}

\vspace{0.05cm}


```{r}
#| results: asis
#| echo: false
P |> round(2) |> knitr::kable()
```

:::

::: {.column width="2.5%"}

:::

::: {.column width="52.5%" text-align="center"}

\begin{center}

$\symbf{P} - \symbf{r}\symbf{c}'$

\end{center}

\vspace{0.05cm}


```{r}
#| echo: false
#| results: asis
S <- P - P_ind
S |> round(3) |> knitr::kable()
```

:::

::::

<!-- \pause !-->

- Residuals are naturally larger for the more abundant rows (employee ranks)

# Standardised residuals

- To avoid the more abundant rows and columns from dominating downstream analyses, we normalise by row and column size.
- For each residual $P_{ij} - P_{\mathrm{ind}_{ij}}$, we standardise by 

$$
\frac{P_{ij} - P_{\mathrm{ind}_{ij}}}{\sqrt{r_ic_j}} = \frac{P_{ij} - r_ic_j}{\sqrt{r_ic_j}}
$$

- Define the diagonal matrices $\symbf{D}_r=\mathrm{diag}(\symbf{r})$ and $\symbf{D}_c=\mathrm{diag}(\symbf{c})$.
- We then have that the matrix of standardised residuals is given by

$$
\symbf{S} = \symbf{D}_r^{-1/2}(\symbf{P} - \symbf{P}_{\mathrm{ind}} ) \symbf{D}_c^{-1/2}.
$$

# Motivation for this form of residual I: Count distribution

- Consider the following residual:

$$
\frac{\mathrm{Observed}-\mathrm{Expected}}{\sqrt{\mathrm{Expected}}}
$$

- This is equal to a residual with mean 0 and unit variance if the data are Poisson distributed, as for the Poisson distribution, the mean is equal to the variance.
<!-- \pause !-->

  - Considering that we are dealing with abundance (e.g. count) data, the Poisson distribution seems appropriate.
  - We are accounting for the mean-variance relationship.

# Motivation for this form of residual II: The $\chi^2$ statistic

- Now if we square the residuals, we get a $\chi^2$-related statistic:

$$
\frac{(\mathrm{Observed}-\mathrm{Expected})^2}{\mathrm{Expected}}
$$

- This tracks the deviation from the model with mean $\mathrm{Expected}$ and variance $\mathrm{Expected}$.
- We can calculate this for all the elements in the matrix $P$ under the assumption that $P$ arose under independent rows and columns.
  - When we add these up, the sum is equal to the $\chi^2$ statistic on the counts, divided by $n$:

$$
\Chi^2 = \sum_{i=1}^I\sum_{j=1}^J \frac{(\mathrm{Observed}_{ij}-\mathrm{Expected}_{ij})^2}{\mathrm{Expected}_{ij}}
$$

# Role of independence assumption

- The assumption of independence is likely not true, but that is in fact the point:
  - We've created a way to highlight observation-variable (row-column) combinations that are more common than expected
    - In particular, the abundance of the row or column is now cancelled out
- We are *not* performing inference for a test of association between rows and columns
- Rather, this deviation-from-independence information (i.e. $\symbf{S}$) will be used to represent rows in a lower-dimensional space that we can then interpret

# Correspondence analysis as PCA on a transformed matrix

- What we've done up until this point is, essentially, transform the data appropriately
  - Calculated proportions ($\symbf{X}\rightarrow \symbf{P}$)
  - "Centred" it ($\symbf{P}-\symbf{r}\symbf{c}'$)
  - "Standardised" it ($\symbf{S} = \symbf{D}_r^{-1/2}(\symbf{P}-\symbf{r}\symbf{c}')\symbf{D}_c^{-1/2}$)
- We then use the SVD to obtain a low-rank approximation to $\symbf{S}$
  - Find the low-rank $s<\min(I-1,J-1)$ rank matrix $\tilde{S}$ such that the sum of squared differences is minimised
    - $\tilde{S}=\sum_{k=1}^{s}\lambda_i\symbf{u}_k\symbf{v}_k'$
  - PCA creates the same approximating matrix as the SVD, and so CA is essentially PCA on a transformed matrix
- Since this is an exploratory technique, we plot the points (transformed again) in a biplot
  - We have various options for how exactly to calculate the points, which we'll discuss

# Views of correspondence analysis

- The previous description of CA is a bit handwavy, but serves to (help) give an intuition for what CA is doing
- There are two formal approaches to (or ways of developing) correspondence analysis:
  - Matrix approximation
  - Profile approximation

# Matrix approximation view of correspondence analysis I

- The matrix approximation view to CA regards it as solving the following weighted least squares problem:

:::{.callout-important icon="false"}
# Matrix approximation angle on CA

- Find a reduced rank matrix  $\hat{\symbf{P}}$ such that

$$
\sum_{i=1}^I\sum_{j=1}^J \frac{(p_{ij}-\hat{p}_{ij})^2}{r_ic_j} 
$$

is minimised.

:::

- We upweight errors arising from more commonly observed rows and cells.
  - This makes sense from the mean-variance relationship we mentioned before.

# Matrix approximation view of correspondence analysis II

\small

Since 

$$
\sum_{i=1}^I\sum_{j=1}^J \frac{(p_{ij}-\hat{p}_{ij})^2}{r_ic_j}  = \mathrm{tr}[(\symbf{D}_r^{-1/2}(\symbf{P} - \hat{\symbf{P}})\symbf{D}_c^{-1/2})(\symbf{D}_r^{-1/2}(\symbf{P} - \hat{\symbf{P}})\symbf{D}_c^{-1/2})'],
$$

we have that 


\begin{align*}
\sum_{i=1}^I\sum_{j=1}^J \frac{(p_{ij}-\hat{p}_{ij})^2}{r_ic_j}  &= \mathrm{tr}[(\symbf{D}_r^{-1/2}\symbf{P}\symbf{D}_c^{-1/2} - \symbf{D}_r^{-1/2}\hat{\symbf{P}}\symbf{D}_c^{-1/2})(\symbf{D}_r^{-1/2}\symbf{P}\symbf{D}_c^{-1/2} - \symbf{D}_r^{-1/2}\hat{\symbf{P}}\symbf{D}_c^{-1/2})'], \\
&=
\mathrm{tr}[(\symbf{D}_r^{-1/2}\symbf{P}\symbf{D}_c^{-1/2} - \hat{\symbf{P}}^{\ast})(\symbf{D}_r^{-1/2}\symbf{P}\symbf{D}_c^{-1/2} - \hat{\symbf{P}}^{\ast})'],
\end{align*},

where $\hat{\symbf{P}}^{\ast} =  \symbf{D}_r^{-1/2}\hat{\symbf{P}}\symbf{D}_c^{-1/2}$.

\vspace{-0.2cm}

- Thus the weighted least squares problem is essentially the same as an unweighted least squares problem, which we know how to solve - using the SVD on $\symbf{D}_r^{-1/2}\symbf{P}\symbf{D}_c^{-1/2}$.

# Results from matrix approximation view I

\normalsize

1. The reduced rank $s$ approximation to $\symbf{P}$ is given by 

$$
\sum_{k=1}^s\tilde{\lambda}_k(\symbf{D}_r^{1/2}\tilde{\symbf{u}}_k)(\symbf{D}_c^{1/2}\tilde{\symbf{v}}_k)',
$$

where $\tilde{\lambda}_k$, $\tilde{\symbf{u}}_k$ and $\tilde{\symbf{v}}_k$ arise from the SVD of $\symbf{D}_r^{-1/2}\symbf{P}\symbf{D}_c^{-1/2}$ and the approximation error is $\sum_{k=s+1}^J\tilde{\lambda}_k^2$.

2. We always have that 

$$
\tilde{\lambda}_k(\symbf{D}_r^{1/2}\tilde{\symbf{u}}_1)(\symbf{D}_c^{1/2}\tilde{\symbf{v}}_1) = \symbf{r}\symbf{c}'.
$$

# Results from matrix approximation view II

3. The reduced rank $K>1$ approximation to $\symbf{P}-\symbf{r}\symbf{c}'$ is given by 

$$
\sum_{k=1}^K\lambda_k(\symbf{D}_r^{1/2}\symbf{u}_k)(\symbf{D}_c^{1/2}\symbf{v}_k)',
$$

where  $\lambda_k$, $\symbf{u}_k$ and $\symbf{v}_k$ arise from the SVD of $\symbf{D}_r^{-1/2}(\symbf{P}-\symbf{r}\symbf{c}')\symbf{D}_c^{-1/2}$. 

4.  $\symbf{D}_r^{-1/2}\symbf{P}\symbf{D}_c^{-1/2}$ and  $\symbf{D}_r^{-1/2}(\symbf{P}-\symbf{r}\symbf{c}')\symbf{D}_c^{-1/2}$ share singular vectors and singular values, in that $\tilde{\lambda}_{k+1}= \lambda_k$, $\tilde{u}_{k+1}= u_k$ and $\tilde{v}_{k+1}= v_k$.

# Comment on the previous results

- It's a lot - but I mention them because they shed light on a couple of things in CA, rather than that you memorise them all.
- The main point is that CA is an application of PCA on a transformed matrix, which is equivalent to a weighted least-squares problem whose solution we find by the singular -vectors and -values of the matrix of standardised residuals, $\symbf{S} = \symbf{D}_r^{-1/2}(\symbf{P}-\symbf{r}\symbf{c}')\symbf{D}_c^{-1/2}$. 

# Row profiles

- We now build up to the profile approximation view of correspondence analysis.
- To do so, we need to define row and column profiles.
- First, we define row profiles as the row proportions of the correspondence matrix, $\symbf{P}$.
  - In other words, conditional on the row, what is the relative frequency of the columns? 
- The formula for the row profiles is $\symbf{D}_r^{-1}\symbf{P}$, with $i$-th row profile denoted $\tilde{\symbf{r}}_i$.

:::: {.columns}

::: {.column width="45%" text-align="center"}

\begin{center} 

\vspace{-0.35cm}

From this dataset:

\end{center}

```{r}
#| results: asis
#| echo: false
data("smoke", package = "ca")
smoke |> knitr::kable()
```

:::

::: {.column width="45%" text-align="center"}

\begin{center} 

\vspace{-0.35cm}

We get these row profiles:

\end{center}

```{r}
#| results: asis
#| echo: false
smoke_dem <- smoke
for (i in seq_len(nrow(smoke_dem))) {
  smoke_rep <- as.matrix(smoke_dem)[i, ] / sum(smoke_dem[i, ])
  for (j in seq_len(ncol(smoke_dem))) {
    smoke_dem[i, j] <- smoke_rep[j]
  }
}
smoke_dem |> round(2) |> knitr::kable()
```

:::

::::

# Column profiles

- Column profiles are defined analogously to row profiles.
- The formula for the column profiles is $\symbf{D}_c^{-1}\symbf{P}^T$, with $j$-th column profile denoted $\tilde{\symbf{c}}_j$.

# The $\chi^2$ distance

- The $\chi^2$ distance is a weighted Euclidean distance. In our case, we use it to down-weight more-abundant columns.
- Let $\tilde{\symbf{r}}_i$ be the $i$-th row profile. 
- In correspondence analysis, the $\chi^2$ distance between the $i$-th and $i'$-th row profiles is given by

$$
\sum_{j=1}^J \frac{(\tilde{r}_{ij}-\tilde{r}_{i'j})^2}{c_j}.
$$

# CA as profile approximation I {.smaller}

- We'll consider CA in terms of row profiles, but results follow analogously for column profiles.
- The goal is to find a low-dimensional representation of the row profiles that preserves the $\chi^2$ distances between the rows.
- The earlier least squares criterion can be written as

$$
\sum_i\sum_j\frac{(p_{ij} - \hat{p}_{ij})^2}{r_ic_j} = \sum_ir_i\sum_j\frac{(p_{ij}/r_i - p_{ij}^*)^2}{c_j}, 
$$

- i.e. the row-mass weighted sum of squared $\chi^2$ distances between the original and approximated row profiles.

# CA as profile approximation I {.smaller}

- It can be shown that the least-squares criterion is minimised for 

$$
\symbf{P}^{\ast} = \symbf{1}\symbf{c}^T + \sum_{k=2}^K \tilde{\lambda}_k\symbf{D}_r^{-1/2}\tilde{\symbf{u}}_k(\symbf{D}_c^{1/2}\tilde{\symbf{v}}_k)^T
$$

for $\tilde{\lambda}_k$, $\tilde{\symbf{u}}_k$ and $\tilde{\symbf{v}}_k$ from the SVD of $\symbf{D}_r^{-1/2}\symbf{P}\symbf{D}_c^{-1/2}$ and $\symbf{c}$ the average row profile - the equivalent result to before (multiply through by $\symbf{D}_r$ to see this).

# Inertia

- For $\symbf{S} = \symbf{D}_r^{-1/2}(\symbf{P}-\symbf{r}\symbf{c}')\symbf{D}_c^{-1/2}$, total inertia is the weighted sum of squares of residuals:

$$
\mathrm{tr}[\symbf{S}\symbf{S}'] = \sum_i\sum_j\frac{(p_{ij} - r_ic_j)^2}{r_ic_j} = \sum_{k=1}^{J-1}\lambda_k^2 
$$

- $\lambda_k$ is the $k$-th singular value of $\symbf{S}$
  - This follows from the fact that $\symbf{r}\symbf{c}'$ is in fact the best rank-one approximation to $\symbf{P}$
    - It is therefore the first term in the SVD approximation, and the rank one SVD matrice's approximation error is equal to $\sum_{k=2}^{J}\tilde{\lambda}_k^2 = \sum_{k=1}^{J-1}\lambda_k^2$.
- The inertia captured by the first $s$ components represents the variation captured in $\symbf{S}$, and the proportion of variation captured is given by $(\sum_{k=1}^{s}\lambda_k^2)/(\sum_{k=1}^{J-1}\lambda_k^2)$.

```{r}
#| echo: false
#| eval: false
data_tidy_smoke_long_init <- smoke |>
  dplyr::mutate(Rank = rownames(smoke)) |>
  tidyr::pivot_longer(
    cols = none:heavy,
    names_to = "Smoking habit",
    values_to = "Count"
  )
data_tidy_smoke_long <- purrr::map_df(
  seq_len(nrow(data_tidy_smoke_long_init)), function(i) {
    data_row <- data_tidy_smoke_long_init[i, ]
    purrr::map_df(seq_len(data_row$Count), function(j) {
      data_row[, c("Rank", "Smoking habit")]
  })
})
data_tidy_smoke_long <- data_tidy_smoke_long |>
  dplyr::mutate(
    Rank = factor(
      data_tidy_smoke_long$Rank,
      c("SM", "JM", "SE", "JE", "SEC"),
      ordered = TRUE
      ),
    `Smoking habit` = factor(
      data_tidy_smoke_long$`Smoking habit`,
      c("none", "light", "medium", "heavy"),
      ordered = TRUE
      )
  ) |>
  dplyr::arrange(Rank)
```

# Examining the raw data using CA concepts

![](_data_raw/img/smoke_example_intro.png)

# Calculating the solution to the least-squares problem

![](_data_raw/img/smoke_example.png)

# Displaying the results of a correspondence analysis

- We've discussed the first three steps in performing CA:
  1. Calculating the matrix of standardised residuals, $\symbf{S} = \symbf{D}_r^{-1/2}(\symbf{P} - \symbf{r}\symbf{c}')\symbf{D}_c^{-1/2}$.
  2. Calculating SVD of this (to obtain its least-squares approximation).
  3. Assessing the quality of the dimensionality reduction (via inertia).

- We now turn our attention to to displaying the rows and columns:

\centering

\vspace{-1.95cm}

```{r}
#| echo: false
#| results: asis
#| eval: true
pander::pandoc.image(path_pdf_cover)
```

# Choosing coordinate scales

- A biplot is a graphical display of a matrix $\symbf{X}_{m \times n}$ such that

$$
\symbf{X}_{m\times n} = \symbf{F}_{m\times k}(\symbf{G}_{n \times k})' \text{ for } k \leq \min(m, n),
$$

where each row of $\symbf{F}$ is a lower-dimensional representation of a row in $\symbf{X}$ and each column of $\symbf{G}$ is a lower-dimensional representation of a column in $\symbf{X}$.

- Here we term $\symbf{F}$ and $\symbf{G}$ the *row* and *column* coordinates, respectively.

- Clearly, in our case $\symbf{X}=\symbf{S}=\symbf{D}_r^{-1/2}(\symbf{P} - \symbf{r}\symbf{c}')\symbf{D}_c^{-1/2}$.

# Principal vs standard coordinates

- We need to choose the scaling for our row coordinates. Typical choices:
  - *Principal* coordinates:
    - Rows: $\symbf{F} = \symbf{D}_r^{-1/2}\symbf{U}\symbf{\Lambda}$
    - Columns: $\symbf{G} = \symbf{D}_c^{-1/2}\symbf{V}\symbf{\Lambda}$
  - *Standard* coordinates: 
    - Rows: $\symbf{F} = \symbf{D}_r^{-1/2}\symbf{U}$
    - Columns: $\symbf{G} = \symbf{D}_c^{-1/2}\symbf{V}$
- When rows are plotted using principal coordinates, the $\chi^2$ distance between the rows is optimally displayed. Similarly for columns.

# Symmetric vs asymmetric biplots

- In symmetric biplots, the both rows and columns are plotted using principal coordinates.
  - This ensures that the $\chi^2$ distances between rows and columns are optimally displayed.
  - However, the row and column points are not strictly related, so interpreting rows (observations) in terms of variables (columns) is not strictly correct.
- In asymmetric biplots, one uses principal coordinates and the other uses standard coordinates.
  - The motivation is that the relationships between rows and columns are more accurately displayed.
  - The standard coordinates of the columns (variables) represent "extreme" observations. For each variable, they are what that an observation would look like if all its members/weight/mass were on that variable. In other words, if the row profile for an observation had a 1 for that variable and 0 everywhere else.
  - However:
    - The standard and principal coordinates may be on fairly different scales, depending on the size of the singular values.
    - Low-frequency categories may appear to be more important than they are.

# Approaches

:::: {.columns}

::: {.column width="45%"}

- One approach may be to have two coordinate axes [@greenacre13a]:

\begin{figure}
\centering
\includegraphics[width=\textwidth]{_data_raw/img/dual-axis-example.png}
\end{figure}

:::

::: {.column width="45%"}

- Another approach is to use an alternate scaling, such as that in contribution biplots [@greenacre13a] which multiplies the column vector lengths by their masses.
  - This is the `rowgreen` or `colgreen` argument to the `map` parameter in the `ca::plot` function.
- A final approach is to use $\symbf{F} = \symbf{D}_r^{-1/2}\symbf{U}\symbf{\Lambda}^{\alpha}$ and $\symbf{G}= \symbf{D}_c^{-1/2}\symbf{V}\symbf{\Lambda}^{1-\alpha}$ for some $\alpha \in (0, 1)$.

:::

::::

# Terminology for scaling choices:

- Both principal: symmetric 
- Row principal and column standard: row-principal
- Column principal and row standard: column-principal

# `ca.plot` function

```{r}
#| echo: true
#| eval: false
library(ca)
?plot.ca
```

# Example: symmetric plot

```{r}
#| results: asis
pander::pandoc.image("_data_raw/img/ca_example_2.png")
```

# Example: asymmetric plot I

```{r}
#| results: asis
pander::pandoc.image("_data_raw/img/ca_example-asymmetric_biplots_1.png")
```

# Example: asymmetric plot II

```{r}
#| results: asis
pander::pandoc.image("_data_raw/img/ca_example-asymmetric_biplots_2.png")
```

# Example: asymmetric plot III

```{r}
#| results: asis
pander::pandoc.image("_data_raw/img/ca_example-asymmetric_biplots_2.png")
```

# Example: quality of approximation

```{r}
#| results: asis
pander::pandoc.image("_data_raw/img/ca_example-quality.png")
```

# Multiple correspondence analysis (MCA)

```{r}
#| results: asis
pander::pandoc.image("_data_raw/img/ca-multilevel_1.png")
```

# MCA: example introduction

```{r}
#| results: asis
pander::pandoc.image("_data_raw/img/ca-multilevel_2.png")
```

# MCA: example application

```{r}
#| results: asis
pander::pandoc.image("_data_raw/img/ca-multilevel_3.png")
```

# MCA: example plot

```{r}
#| results: asis
pander::pandoc.image("_data_raw/img/ca-multilevel_4.png")
```

# Summary

```{r}
#| results: asis
pander::pandoc.image("_data_raw/img/summary.png")
```





---
title: Principal component analysis
subtitle: Modern multivariate statistical techniques
author: Miguel Rodo
date: "2024-04-04"
bibliography: zotero.bib
format:
  beamer:
    embed-resources: true
    aspectratio: 169
    urlcolor: cyan
    linkcolor: blue
    filecolor: magenta
    include-in-header:
      file: preamble.tex
nocite: |
    @johnson_wichern07
---

```{r }
#| include: false
library(tibble)
library(ggplot2)
for (x in list.files("R", full.names = TRUE)) {
  source(x)
}
```

# Context

- Multivariate analysis helps us uncover patterns and relationships within datasets containing multiple variables.
- **Example**: Meteorologists analyze temperature, humidity, wind speed, and more to understand weather systems.
  - Possible questions:
    - What sets of variable(s) are correlated?
    - Can we predict the likelihood of rain based on these variables?
    - What variables most distinguish between different weather patterns?

# Basis of approach

- **Linear combinations**:

$$
\symbf{x}_1\symbf{u}_1 + \symbf{x}_2\symbf{u}_2 + \ldots + \symbf{x}_p\symbf{u}_p
$$

- Why linear combinations?
  - Simplicity >> strong theoretical results >> robustness, speed and interpretability

# Types of multivariate analysis

- $\symbf{X} \sim \symbf{X}$: PCA, factor analysis, correspondence analysis (count data)
  - **Examples**: identify correlates sets of variables
- $\symbf{Y} \sim \symbf{X}$: Multiple/multivariate regression, canonical correlation analysis, discrimination and classification

# Principal component analysis

- Principal components analysis (PCA) identifies the directions of greatest variation, allowing you to:
  - Represent data more compactly, and
  - Interpret relationships between variables.

- PCA is a linear technique with an algebraic solution, so it is fast to fit.

# Example applications

- **Market analysis**: Identify correlated stocks to ensure a diversified portfolio
- **Image compression**: Represent images with fewer pixels while preserving accuracy
- **Bioinformatics**: Identify sets of co-expressed genes responsible for different functions in the body


# Projection onto a vector ("line")

::: {.columns}

::: {.column width="50%"}

- Suppose that we wish to project the vector $\symbf{b}$ onto the vector $\symbf{a}$.
  - The projection is the nearest point to $\symbf{b}$ on the line spanned by $\symbf{a}$.
- The projection of $\symbf{b}$ onto $\symbf{a}$ is given by:

$$
\text{proj}_{\symbf{a}}(\symbf{b}) = \frac{\symbf{a}^T \symbf{b}}{\symbf{a}^T \symbf{a}} \symbf{a}
$$

- Here is a diagram of such a projection. Suppose that we want to project the vector $\begin{pmatrix} 2 \\ 1 \end{pmatrix}$ onto the vector $\begin{pmatrix} 1 \\ 1 \end{pmatrix}$.

:::

::: {.column width="50%"}

```{r}
#| echo: false
a <- c(2, 2)
b <- c(2, 1)
proj <- as.numeric((a %*% b) / (a %*% a)) * a
plot_tbl <- tibble::tibble(
  x = c(0, a[1], 0, b[1], 0, proj[1]),
  y = c(0, a[2], 0, b[2], 0, proj[2]),
  label = rep(c("a", "b", "proj(a, b)"), each = 2)
)
plot_tbl <- tibble::tibble(
  x_start = rep(0, 3),
  y_start = rep(0, 3),
  x_end = c(a[1], b[1], proj[1]),
  y_end = c(a[2], b[2], proj[2]),
  label = rep(c("a", "b", "proj(a, b)"), each = 1)
)
plot_tbl_error <- tibble::tibble(
  x_start = b[1],
  y_start = b[2],
  x_end = proj[1],
  y_end = proj[2],
  label = "error"
)
p <- ggplot(
  plot_tbl,
  aes(x = x, y = y, label = label, group = label)
) +
  ggrepel::geom_text_repel(
    data = plot_tbl |> dplyr::filter(label %in% c("a", "b")),
    mapping = aes(x = x_end, y = y_end),
    nudge_x = -0.25, nudge_y = 0.15,
    col = "gray25", min.segment.length = 2
  ) +
  ggrepel::geom_text_repel(
    data = plot_tbl |> dplyr::filter(label == "proj(a, b)"),
    mapping = aes(x = x_end, y = y_end),
    nudge_x = -0.33, nudge_y = 0.23,
    col = "gray25", min.segment.length = 2
  ) +
  cowplot::theme_cowplot() +
  cowplot::background_grid(major = "xy") +
  geom_segment(
    data = plot_tbl |> dplyr::filter(label != "proj(a, b)"),
    aes(x = x_start, y = y_start, xend = x_end, yend = y_end),
    arrow = grid::arrow(length = unit(0.3, "cm"))
  ) +
  geom_segment(
    data = plot_tbl |> dplyr::filter(label == "proj(a, b)"),
    aes(x = x_start, y = y_start, xend = x_end, yend = y_end),
    arrow = grid::arrow(length = unit(0.3, "cm")), col = "dodgerblue"
  ) +
  geom_segment(
    data = plot_tbl_error,
    aes(x = x_start, y = y_start, xend = x_end, yend = y_end),
    col = "red", linetype = "dotted"
  ) +
  theme( 
    panel.background = element_rect(fill = "white"), 
    plot.background = element_rect(fill = "white"),
    axis.title = element_blank(),
    axis.ticks = element_blank(),
    axis.text = element_blank()
  ) +
  coord_equal()
path_p_proj_init <- projr::projr_path_get("cache", "fig", "p-proj-init.png")
cowplot::save_plot(
  plot = p,
  filename = path_p_proj_init,
  units = "cm",
  base_height = 6,
  base_width = 6
)
```

\centering
```{r}
knitr::include_graphics(path_p_proj_init)
```

:::

::::

# Projecting onto a unit vector

- If $\symbf{a}$ is a unit vector, then the projection of $\symbf{b}$ onto $\symbf{a}$ is given by:

$$
\text{proj}_{\symbf{a}}(\symbf{b}) = (\symbf{a}^T \symbf{b})\symbf{a}.
$$

- This means that the length of the projection is the dot product of $\symbf{a}$ and $\symbf{b}$.
  - This is a linear combination of the elements of $\symbf{b}$: $\sum_{i=1}^p a_i b_i$.
- Note that the length of $\symbf{a}$ does not affect the projection.

# Two-variable example

:::: {.columns}

::: {.column width="50%"}

- Suppose we have a dataset with two correlated variables, perhaps weight and height.
- Then, the scatterplot of these may look something like this:

```{r}
set.seed(2106)
plot_tbl <- tibble::tibble(
  height = rnorm(100, 160, 10)
) |>
  dplyr::mutate(
    weight = height / 2.5 + rnorm(100, 0, 3.5 * height / 160)
  ) |>
  dplyr::mutate(
    across(everything(), function(x) (x - mean(x)) / sd(x))
  ) |>
  dplyr::arrange(height)
hw_tbl <- plot_tbl
p <- ggplot(
  aes(x = height, y = weight),
  data = plot_tbl
) +
  geom_vline(
    xintercept = 0, col = "gray50", linetype = "solid"
  ) +
  geom_hline(
    yintercept = 0, col = "gray50", linetype = "solid"
  ) +
  cowplot::theme_cowplot() +
  cowplot::background_grid(major = "xy") +
  theme( 
    panel.background = element_rect(fill = "white"), 
    plot.background = element_rect(fill = "white")
  ) +
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank()
  ) +
  labs(
    x = "Height",
    y = "Weight"
  ) +
  geom_point(alpha = 0.85, col = "dodgerblue")
path_p_height_weight <- projr::projr_path_get("cache", "fig", "p-height-weight.png")
cowplot::save_plot(
  plot = p,
  filename = path_p_height_weight,
  units = "cm",
  base_height = 5,
  base_width = 5
)
```

\centering

```{r}
knitr::include_graphics(path_p_height_weight)
```

:::

::: {.column width="50%"}

- We could combine these two variables into a single variable, representing size: $a_1 \times \text{height} + a_2 \times \text{weight}$.
  - If we restrict $a_1^2 + a_2^2 = 1$, then this equivalent to projecting the height and weight vector ($\symbf{b} = [\text{height}, \text{weight}]$) onto the line spanned by the vector $\symbf{a} = [a_1, a_2]$.
  - Since the projection is independent of the length of whatever you're projecting onto, this is equivalent to projecting onto any vector in the same direction as $[a_1, a_2]$.

:::

::::

# Two-variable example (cont.) {.smaller}

:::: {.columns}

::: {.column width="50%"}

- The new variable is then the *length and sign of the projection*.
- For example, suppose we have the point $[1.2, 0.3]$:

```{r}
set.seed(2106)
plot_tbl <- tibble::tibble(
  height = rnorm(100, 160, 10)
) |>
  dplyr::mutate(
    weight = height / 2.5 + rnorm(100, 0, 3.5 * height / 160)
  ) |>
  dplyr::mutate(
    across(everything(), function(x) (x - mean(x)) / sd(x))
  ) |>
  dplyr::arrange(height)
proj_list <- list(c(1 / sqrt(2), 1 / sqrt(2)), c(1, 0), c(0, 1))[1]
plot_tbl_ind <- tibble::tibble(
  height = 1.2,
  weight = 0.3
)
plot_tbl_proj_ind <- proj_list |>
  purrr::map_df(~ mutate_proj(plot_tbl_ind, .x))

p <- ggplot(
  data = plot_tbl
) +
  geom_vline(
    xintercept = 0, col = "gray50", linetype = "solid"
  ) +
  geom_hline(
    yintercept = 0, col = "gray50", linetype = "solid"
  ) +
  cowplot::theme_cowplot() +
  cowplot::background_grid(major = "xy") +
  theme(
    panel.background = element_rect(fill = "white"),
    plot.background = element_rect(fill = "white")
  ) +
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank()
  ) +
  labs(
    x = "Height",
    y = "Weight"
  ) +
  geom_point(aes(x = height, y = weight), alpha = 0.1, col = "dodgerblue") +
  geom_point(
    data = plot_tbl_proj_ind,
    aes(x = height, y = weight), col = "mediumseagreen"
  ) +
  geom_segment(
    data = plot_tbl_proj_ind,
    aes(x = height, y = weight, xend = proj_x, yend = proj_y, group = proj),
    col = "gray25", arrow = arrow(length = unit(0.3, "cm")), linetype = "solid"
  ) +
  geom_abline(
    data = plot_tbl_proj_ind |>
      dplyr::mutate(slope = a2 / a1) |>
      dplyr::filter(slope %in% c(0,1)),
    aes(intercept = 0, slope = slope),
    col = "purple"
  ) +
  coord_cartesian(xlim = c(0, 2.5), ylim = c(0, 2.5)) +
  ggrepel::geom_text_repel(
    data = plot_tbl_proj_ind |>
      dplyr::mutate(a1 = a1 |> signif(2), a2 = a2 |> signif(2)),
    aes(x = a1, y = a2, label = proj),
    nudge_x = 0.1, nudge_y = 0.1
  )
path_p_height_weight_ind <- projr::projr_path_get("cache", "fig", "p-height-weight-ind.png")
cowplot::save_plot(
  plot = p,
  filename = path_p_height_weight_ind,
  units = "cm",
  base_height = 5,
  base_width = 5
)
```

\centering

```{r}
#| results: asis
knitr::include_graphics(path_p_height_weight_ind)
```

:::

::: {.column width="50%"}

- If we set $\symbf{a} = [1/\sqrt{2}, 1/\sqrt{2}]$, then the new variable is $1/\sqrt{2} \times 1.2 + 1/\sqrt{2} \times 0.3$.
- We go from the matrix of vairables

```{r}
tibble::tibble(height = 1.2, weight = 0.3) |> as.data.frame()
```

- to

```{r}
tibble::tibble(size = 1.2 / sqrt(2) + 0.3 / sqrt(2)) |> as.data.frame()
```

:::

::::



# Projections and axes

:::: {.columns}

::: {.column width="50%"}

- If we set $\symbf{a} = [1, 0]$, then the "new" variable is the height.
- If we set $\symbf{a} = [0, 1]$, then the "new" variable is the weight.

- The point of these last two is that original coordinates are, in a sense, themselves projections.
- So, if we consider $[1, 0]$ and $[0, 1]$ as axes, then we can consider $[1/\sqrt{2}, 1/\sqrt{2}]$ as a new axis.

:::

::: {.column width="50%"}


```{r}
set.seed(2106)
plot_tbl <- tibble::tibble(
  height = rnorm(100, 160, 10)
) |>
  dplyr::mutate(
    weight = height / 2.5 + rnorm(100, 0, 3.5 * height / 160)
  ) |>
  dplyr::mutate(
    across(everything(), function(x) (x - mean(x)) / sd(x))
  ) |>
  dplyr::arrange(height)
proj_list <- list(c(1 / sqrt(2), 1 / sqrt(2)), c(1, 0), c(0, 1))[2:3]
plot_tbl_ind <- tibble::tibble(
  height = 1.2,
  weight = 0.3
)
plot_tbl_proj_ind <- proj_list |>
  purrr::map_df(~ mutate_proj(plot_tbl_ind, .x))

p <- ggplot(
  data = plot_tbl
) +
  geom_vline(
    xintercept = 0, col = "gray50", linetype = "solid"
  ) +
  geom_hline(
    yintercept = 0, col = "gray50", linetype = "solid"
  ) +
  cowplot::theme_cowplot() +
  cowplot::background_grid(major = "xy") +
  theme(
    panel.background = element_rect(fill = "white"),
    plot.background = element_rect(fill = "white")
  ) +
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank()
  ) +
  labs(
    x = "Height",
    y = "Weight"
  ) +
  geom_point(aes(x = height, y = weight), alpha = 0.1, col = "dodgerblue") +
  geom_point(
    data = plot_tbl_proj_ind,
    aes(x = height, y = weight), col = "mediumseagreen"
  ) +
  geom_segment(
    data = plot_tbl_proj_ind,
    aes(x = height, y = weight, xend = proj_x, yend = proj_y, group = proj),
    col = "gray25", arrow = arrow(length = unit(0.3, "cm")), linetype = "solid"
  ) +
  geom_abline(
    data = plot_tbl_proj_ind |>
      dplyr::mutate(slope = a2 / a1) |>
      dplyr::filter(slope %in% c(0,1)),
    aes(intercept = 0, slope = slope),
    col = "purple"
  ) +
  geom_vline(
    xintercept = 0,
    col = "purple"
  ) +
  coord_cartesian(xlim = c(0, 2.5), ylim = c(0, 2.5)) +
  ggrepel::geom_text_repel(
    data = plot_tbl_proj_ind |>
      dplyr::mutate(a1 = a1 |> signif(2), a2 = a2 |> signif(2)),
    aes(x = a1, y = a2, label = proj),
    nudge_x = 0.1, nudge_y = 0.1
  )
path_p_height_weight_ind_orig <- projr::projr_path_get("cache", "fig", "p-height-weight-ind-orig.png")
cowplot::save_plot(
  plot = p,
  filename = path_p_height_weight_ind_orig,
  units = "cm",
  base_height = 5,
  base_width = 5
)
```

\centering

```{r}
#| results: asis
knitr::include_graphics(path_p_height_weight_ind_orig)
```

:::

::::

# Alternate axes example

:::: {.columns}

::: {.column width="50%"}

- Clearly, if we just have one axis (e.g. $[1/\sqrt{2}, -1/\sqrt{2}]$) and two original variables (e.g. height and weight), then we cannot represent the data exactly.
- An additional linearly independent coordinate axis (e.g.  $[1/\sqrt{2}, -1/\sqrt{2}]$) would then provide us two values for each point, and be an alternative way to represent the data exactly.

:::

::: {.column width="50%"}

```{r}
set.seed(2106)
plot_tbl <- tibble::tibble(
  height = rnorm(100, 160, 10)
) |>
  dplyr::mutate(
    weight = height / 2.5 + rnorm(100, 0, 3.5 * height / 160)
  ) |>
  dplyr::mutate(
    across(everything(), function(x) (x - mean(x)) / sd(x))
  ) |>
  dplyr::arrange(height)
proj_list <- list(c(1 / sqrt(2), 1 / sqrt(2)), c(1 / sqrt(2), -1 / sqrt(2)))
plot_tbl_ind <- tibble::tibble(
  height = 1.2,
  weight = 0.3
)
plot_tbl_proj_ind <- proj_list |>
  purrr::map_df(~ mutate_proj(plot_tbl_ind, .x))

p <- ggplot(
  data = plot_tbl
) +
  geom_vline(
    xintercept = 0, col = "gray50", linetype = "solid"
  ) +
  geom_hline(
    yintercept = 0, col = "gray50", linetype = "solid"
  ) +
  cowplot::theme_cowplot() +
  cowplot::background_grid(major = "xy") +
  theme(
    panel.background = element_rect(fill = "white"),
    plot.background = element_rect(fill = "white")
  ) +
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank()
  ) +
  labs(
    x = "Height",
    y = "Weight"
  ) +
  geom_point(aes(x = height, y = weight), alpha = 0.1, col = "dodgerblue") +
  geom_point(
    data = plot_tbl_proj_ind,
    aes(x = height, y = weight), col = "mediumseagreen"
  ) +
  geom_segment(
    data = plot_tbl_proj_ind,
    aes(x = height, y = weight, xend = proj_x, yend = proj_y, group = proj),
    col = "gray25", arrow = arrow(length = unit(0.3, "cm")), linetype = "solid"
  ) +
  geom_abline(
    data = plot_tbl_proj_ind |>
      dplyr::mutate(slope = a2 / a1) ,
    aes(intercept = 0, slope = slope),
    col = "purple"
  ) +
  coord_cartesian(xlim = c(-2.5, 2.5), ylim = c(-2.5, 2.5)) +
  ggrepel::geom_text_repel(
    data = plot_tbl_proj_ind |>
      dplyr::mutate(a1 = a1 |> signif(2), a2 = a2 |> signif(2)),
    aes(x = a1, y = a2, label = proj),
    nudge_x = 0.1, nudge_y = 0.1
  )
path_p_height_weight_ind_alt <- projr::projr_path_get("cache", "fig", "p-height-weight-ind-alt.png")
cowplot::save_plot(
  plot = p,
  filename = path_p_height_weight_ind_alt,
  units = "cm",
  base_height = 5,
  base_width = 5
)
```


\centering

```{r}
#| results: asis
knitr::include_graphics(path_p_height_weight_ind_alt)
```

:::

::::


# How do we choose $a_1$ and $a_2$? {.smaller}

- Consider projecting onto the lines $[0.5, 2]$, $[1, -1]$ and $[1, 1]$:

```{r}
proj_list <- list(c(0.5, 2), c(1, 1), c(1, -1))
plot_tbl_proj <- purrr::map_df(proj_list, ~mutate_proj(plot_tbl, .x))

p <- ggplot(
  data = plot_tbl_proj
) +
  geom_vline(
    xintercept = 0, col = "gray50", linetype = "solid"
  ) +
  geom_hline(
    yintercept = 0, col = "gray50", linetype = "solid"
  ) +
  cowplot::theme_cowplot() +
  cowplot::background_grid(major = "xy") +
  theme( 
    panel.background = element_rect(fill = "white"), 
    plot.background = element_rect(fill = "white")
  ) +
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank()
  ) +
  labs(
    x = "Height",
    y = "Weight"
  ) +
  geom_point(aes(x = height, y = weight), alpha = 0.85, col = "dodgerblue") +
  geom_segment(
    data = plot_tbl_proj |>
      dplyr::group_by(proj) |>
      dplyr::slice(seq(1, 100, length.out = 5) |> round()) |>
      dplyr::ungroup(),
    aes(x = height, y = weight, xend = proj_x, yend = proj_y, group = proj),
    col = "gray25", arrow = arrow(length = unit(0.3, "cm")), linetype = "solid"
  ) +
  geom_abline(
    data = plot_tbl_proj |>
      dplyr::group_by(proj) |>
      dplyr::slice(1) |>
      dplyr::ungroup() |>
      dplyr::mutate(slope = a2 / a1),
    aes(intercept = 0, slope = slope),
  ) +
  facet_wrap(~proj, ncol = 3)
path_p_proj <- projr::projr_path_get("cache", "fig", "p-proj.png")
cowplot::save_plot(
  plot = p,
  filename = path_p_proj,
  units = "cm",
  base_height = 6,
  base_width = 12
)
```

\begin{figure}
\centering
\includegraphics[width=0.55\textwidth]{`r path_p_proj`}
\end{figure}

::::{.columns}

::: {.column width="50%"}
- Height and weight increase together, so $a_1$ and $a_2$ should both be positive.
- Beyond that, we want to choose $a_1$ and $a_2$ so that the projection captures the most variation in the data.
:::

::: {.column width="50%"}
- The variability of the (lengths of the) projections is 1.3 (0.5, 2), 0.3 (1, -1) and 1.7 (1, 1), respectively.
- The projection onto $[1, 1]$ captures the most variation.
:::

::::

# Further motivation for capturing variation

- On its own grounds, imagine that you could retain only one of two variables: one has zero variance (all the observations are the same) and the other has some positive variance. Clearly, you would choose the latter.

\pause

- Later, we will see that capturing maximal variation:
  - Minimises the distance from the original data to the projection.
  - Allows us to reconstruct the original variables with minimal error.

# Objective function: the first principal component

- Assume that our data $\symbf{X} \in \mathcal{R}^p$ are distributed with mean $\symbf{\symbf{\mu}}$ and covariance matrix $\symbf{\Sigma}$.
- We seek to find the linear combination of the variables that captures the most variation.
- For the first principal component, we seek to find $\symbf{a}_1$ such that

$$
\text{Var}(\symbf{a}_1'\symbf{X}) = \symbf{a}_1^T\symbf{\Sigma}\symbf{a}_1
$$

- is maximized, subject to the constraint that $||\symbf{a}_1|| = 1$.
  - This is necessary as otherwise we could meaninglessly increase the variation without bound by making $\symbf{a}_1$ longer.
- Applied to the previous example, the first principal component would be a linear combination of height and weight that captures the most variation.

# First principal component for health and weight data

```{r}
set.seed(2106)
plot_tbl <- tibble::tibble(
  height = rnorm(100, 160, 10)
) |>
  dplyr::mutate(
    weight = height / 2.5 + rnorm(100, 0, 3.5 * height / 160)
  ) |>
  dplyr::mutate(
    across(everything(), function(x) (x - mean(x)) / sd(x))
  ) |>
  dplyr::arrange(height)
proj_list <- list(c(1 / sqrt(2), 1 / sqrt(2)), c(1 / sqrt(2), -1 / sqrt(2)))[1]
plot_tbl_ind <- tibble::tibble(
  height = 1.2,
  weight = 0.3
)
plot_tbl_proj_ind <- proj_list |>
  purrr::map_df(~ mutate_proj(plot_tbl_ind, .x))

p <- ggplot(
  data = plot_tbl
) +
  geom_vline(
    xintercept = 0, col = "gray50", linetype = "solid"
  ) +
  geom_hline(
    yintercept = 0, col = "gray50", linetype = "solid"
  ) +
  cowplot::theme_cowplot() +
  cowplot::background_grid(major = "xy") +
  theme(
    panel.background = element_rect(fill = "white"),
    plot.background = element_rect(fill = "white")
  ) +
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank()
  ) +
  labs(
    x = "Height",
    y = "Weight"
  ) +
  geom_point(aes(x = height, y = weight), alpha = 0.1, col = "dodgerblue") +
  geom_point(
    data = plot_tbl_proj_ind,
    aes(x = height, y = weight), col = "mediumseagreen"
  ) +
  geom_segment(
    data = plot_tbl_proj_ind,
    aes(x = height, y = weight, xend = proj_x, yend = proj_y, group = proj),
    col = "gray25", arrow = arrow(length = unit(0.3, "cm")), linetype = "solid"
  ) +
  geom_abline(
    data = plot_tbl_proj_ind |>
      dplyr::mutate(slope = a2 / a1) ,
    aes(intercept = 0, slope = slope),
    col = "purple"
  ) +
  coord_cartesian(xlim = c(-2.5, 2.5), ylim = c(-2.5, 2.5)) +
  ggrepel::geom_text_repel(
    data = plot_tbl_proj_ind |>
      dplyr::mutate(a1 = a1 |> signif(2), a2 = a2 |> signif(2)),
    aes(x = a1, y = a2, label = proj),
    nudge_x = 0.1, nudge_y = 0.1
  )
path_p_height_weight_ex_1 <- projr::projr_path_get("cache", "fig", "p-height-weight-ex-1.png")
cowplot::save_plot(
  plot = p,
  filename = path_p_height_weight_ex_1,
  units = "cm",
  base_height = 8,
  base_width = 8
)
```

\centering

```{r}
#| results: asis
knitr::include_graphics(path_p_height_weight_ex_1)
```

# Objective function: the second principal component

- The second principal component is the linear combination of the variables that captures the most variation, subject to the constraint that it is orthogonal to the first principal component.
- In other words, for the second principal component we seek to find $\symbf{a}_2$ such that

$$
\text{Var}(\symbf{a}_2'\symbf{X}) = \symbf{a}_2^T\symbf{\Sigma}\symbf{a}_2
$$

- is maximized, subject to the constraints that $||\symbf{a}_2|| = 1$ and $\symbf{a}_2^T\symbf{a}_1 = 0$.

# Second principal component for health and weight data

```{r}
set.seed(2106)
plot_tbl <- tibble::tibble(
  height = rnorm(100, 160, 10)
) |>
  dplyr::mutate(
    weight = height / 2.5 + rnorm(100, 0, 3.5 * height / 160)
  ) |>
  dplyr::mutate(
    across(everything(), function(x) (x - mean(x)) / sd(x))
  ) |>
  dplyr::arrange(height)
proj_list <- list(c(1 / sqrt(2), 1 / sqrt(2)), c(1 / sqrt(2), -1 / sqrt(2)))
plot_tbl_ind <- tibble::tibble(
  height = 1.2,
  weight = 0.3
)
plot_tbl_proj_ind <- proj_list |>
  purrr::map_df(~ mutate_proj(plot_tbl_ind, .x))

p <- ggplot(
  data = plot_tbl
) +
  geom_vline(
    xintercept = 0, col = "gray50", linetype = "solid"
  ) +
  geom_hline(
    yintercept = 0, col = "gray50", linetype = "solid"
  ) +
  cowplot::theme_cowplot() +
  cowplot::background_grid(major = "xy") +
  theme(
    panel.background = element_rect(fill = "white"),
    plot.background = element_rect(fill = "white")
  ) +
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank()
  ) +
  labs(
    x = "Height",
    y = "Weight"
  ) +
  geom_point(aes(x = height, y = weight), alpha = 0.1, col = "dodgerblue") +
  geom_point(
    data = plot_tbl_proj_ind,
    aes(x = height, y = weight), col = "mediumseagreen"
  ) +
  geom_segment(
    data = plot_tbl_proj_ind,
    aes(x = height, y = weight, xend = proj_x, yend = proj_y, group = proj),
    col = "gray25", arrow = arrow(length = unit(0.3, "cm")), linetype = "solid"
  ) +
  geom_abline(
    data = plot_tbl_proj_ind |>
      dplyr::mutate(slope = a2 / a1) ,
    aes(intercept = 0, slope = slope),
    col = "purple"
  ) +
  coord_cartesian(xlim = c(-2.5, 2.5), ylim = c(-2.5, 2.5)) +
  ggrepel::geom_text_repel(
    data = plot_tbl_proj_ind |>
      dplyr::mutate(a1 = a1 |> signif(2), a2 = a2 |> signif(2)),
    aes(x = a1, y = a2, label = proj),
    nudge_x = 0.1, nudge_y = 0.1
  )
path_p_height_weight_ex_2 <- projr::projr_path_get("cache", "fig", "p-height-weight-ex-2.png")
cowplot::save_plot(
  plot = p,
  filename = path_p_height_weight_ex_2,
  units = "cm",
  base_height = 8,
  base_width = 8
)
```

\centering

```{r}
#| results: asis
knitr::include_graphics(path_p_height_weight_ex_2)
```

# Objective function for all principal components

- For the $k$-th principal component (for $k\leq p$), we seek to find $\symbf{a}_k$ such that

$$
\text{Var}(\symbf{a}_k'\symbf{X}) = \symbf{a}_k^T\symbf{\Sigma}\symbf{a}_k
$$

- is maximized, subject to the constraints that $||\symbf{a}_k|| = 1$ and $\symbf{a}_k^T\symbf{a}_j = 0$ for $j < k$.

\pause

- Up until this point, we've explored various options for $\symbf{a}_k$ without any method. We'll now discuss how to compute these principal components.
- First, we'll discuss a more general result.

# Theorem 1: Maximisation of quadratic forms for points on the unit sphere

Let $\symbf{B}:p\times p$ be a positive semi-definite matrix with the $i$-th largest eigenvalue $\lambda_i$ and associated eigenvector $\symbf{e}_i$. Then we have that 

$$
\max_{\symbf{x}\neq \symbf{0}} \frac{\symbf{x}'\symbf{B}\symbf{x}}{\symbf{x}'\symbf{x}}=\lambda_1 \; (\mathrm{attained}\; \mathrm{when} \; \symbf{x}=\symbf{e}_1)
$$

and that

$$
\max_{\symbf{x}\neq \symbf{0},\symbf{x}\perp \symbf{e}_1, \symbf{e}_2, \ldots, \symbf{e}_{i-1}} \frac{\symbf{x}'\symbf{B}\symbf{x}}{\symbf{x}'\symbf{x}}=\lambda_i \; (\mathrm{attained}\; \mathrm{when} \; \symbf{x}=\symbf{e}_{i})
$$

for $i\in\{2,3, \ldots, p\}$, where $\lambda_i$ is the $i$-th largest eigenvalue and $\symbf{e}_i$ is the associated eigenvector.

# Proof of Theorem 1 {.smaller}

Let $\symbf{P}:p\times p$ be the orthogonal matrix whose $i$-th column is the $i$-th eigenvector and $\Lambda$ be the diagonal matrix with ordered eigenvalues along the diagonal. Let $\symbf{B}^{1/2}=\symbf{P}\symbf{\Lambda}^{1/2}\symbf{P}'$ and $\symbf{y}=\symbf{P}'\symbf{x}$.

First, we show that the quadratic form can never be larger than $\lambda_1$:

\begin{align*} 
\frac{\symbf{x}'\symbf{B}\symbf{x}}{\symbf{x}'\symbf{x}} &=  \frac{\symbf{x}'\symbf{B}^{1/2}\symbf{B}^{1/2}\symbf{x}}{\symbf{x}'\symbf{P}\symbf{P}'\symbf{x}} 
= \frac{\symbf{x}'\symbf{P}\symbf{\Lambda}^{1/2}\symbf{P}'\symbf{P}\symbf{\Lambda}^{1/2}\symbf{P}'\symbf{x}}{\symbf{y}'\symbf{y}}
=  \frac{\symbf{y}'\symbf{\Lambda}\symbf{y}}{\symbf{y}'\symbf{y}} \\ 
 &=  \frac{\sum_{i=1}^p\lambda_iy_i^2}{\sum_{i=1}^py_i^2} 
 \leq \lambda_1 \frac{\sum_{i=1}^py_i^2}{\sum_{i=1}^py_i^2}
 = \lambda_1 
\end{align*}

# Proof of Theorem 1 (cont.) {.smaller}

Now we show that this is actually attained for $\symbf{x}=\symbf{e}_1$. Since eigenvectors are by convention length 1, we consider $\symbf{e}_1'\symbf{B}\symbf{e}_1$.

First, let $\symbf{c}_i$ be the unit vector with a 1 in the $i$-th position (and 0's everywhere else).

Expanding $\symbf{B}$ by the eigen decomposition, we have that

$$
\symbf{e}_1'\symbf{B}\symbf{e}_1=\symbf{e}_1'\symbf{P}\symbf{\Lambda}\symbf{P}'\symbf{e}_1.
$$

Since $B$ is symmetric, the eigenvectors can be chosen orthogonal.
We do so, which implies that $\symbf{e}_i'P=\symbf{c}_i'$, where $\symbf{c}_i$ is the unit vector with a 1 in the $i$-th position (and 0 everwhere else).
Consequently,

$$
\symbf{e}_i'\symbf{P}\symbf{\Lambda}\symbf{P}'\symbf{e}_i
= \symbf{c}_i'\symbf{\Lambda}\symbf{c}_i=\lambda_i,
$$

and so $\symbf{e}_1'\symbf{P}\symbf{\Lambda}\symbf{P}'\symbf{e}_1=\lambda_1$.

# Proof of Theorem 1 (cont.) {.smaller}

Now, we consider the case where $\symbf{x}$ is orthogonal to $\symbf{e}_1, \symbf{e}_2, \ldots, \symbf{e}_{i-1}$.

Each component in the vector $\symbf{y}$ is the dot product of $x$ and an eigenvector $\symbf{e}_i$.
Since we choose $x$ orthogonal to the first $i-1$ eigenvectors, the first $i-1$ entries of $\symbf{y}$ are zero.

Returning to considering the quadratic form, we have that

\begin{align}
\frac{\symbf{x}'\symbf{B}\symbf{x}}{\symbf{x}'\symbf{x}} 
&= \frac{\sum_{j=1}^p\lambda_jy_j^2}{\sum_{j=1}^py_j^2} \\
&= \frac{\sum_{j=i}^p\lambda_jy_j^2}{\sum_{j=i}^py_j^2} 
 \leq \lambda_i \frac{\sum_{j=i}^py_j^2}{\sum_{j=i}^py_j^2} 
 = \lambda_i
\end{align}

Using the same argument as before, we can show that this is actually attained for $\symbf{x}=\symbf{e}_i$.

# Connection with principal components

- Theorem 1 has already done the heavy lifting, as we can simply set $\symbf{B}=\symbf{\Sigma}$.
  - To see this, consider that $\frac{\symbf{x}'\symbf{\Sigma}\symbf{x}}{\symbf{x}'\symbf{x}}=\frac{\symbf{x}'\symbf{\Sigma}\symbf{x}}{||\symbf{x}||^2} = (\symbf{x}/||\symbf{x}||)'\symbf{\Sigma}(\symbf{x}/||\symbf{x}||)$, so implicitly the constraint $||\symbf{x}||=1$ is already present.
- All that's left to do is place Theorem 1 in a probabilistic context, and relate it to the variance and covariance of linear combinations of a random vector.

# Theorem 2: Selecting principal components {.smaller}

Let $\symbf{X}:\symbf{p}\times 1$ be the random vector with variance-covariance matirx $\symbf{\Sigma}:p\times p$, which has the $i$-th largest eigenvalue as $\lambda_i$ and associated eigenvector $\symbf{e}_i$.

Then the $i$-th principal component (defined as before) is given by 

$$
Y_i = \symbf{e}_i'\symbf{X},
$$

implying that

\begin{align*}
\mathrm{Var}[Y_i] &= \lambda_i \;\forall \; i \in \{1,2, \ldots, p\}, \; \mathrm{and} \\
\mathrm{Cov}[Y_i, Y_j] &= 0 \; \mathrm{for} \; i\neq j.
\end{align*}

# Proof of Theorem 2 {.smaller}

From Theorem 1, we know that 

$$
\max_{\symbf{x}} \frac{\symbf{x}'\symbf{\Sigma}\symbf{x}}{\symbf{x}'\symbf{x}} = \lambda_1,
$$

which we achieve by setting $\symbf{x}=\symbf{e}_1$.

Since $||\symbf{e}_1||=1$, $\frac{\symbf{e}_1'\symbf{\Sigma}\symbf{e}_1}{\symbf{e}_1'\symbf{e}_1}=\symbf{e}_1'\symbf{\Sigma}\symbf{e}_1$, which is equal to $\mathrm{Var}[\symbf{a}'\mathrm{X}]=\mathrm{Var}[Y_1]$ if we set $\symbf{a}=\symbf{e}_1$. This implies that $\symbf{a}=\symbf{e}_1$ maximises the variance, which is $\lambda_1$.

Analagous reasoning shows that $\mathrm{Var}[\symbf{a}_i'\symbf{X}]$ has a maximum value $\lambda_i$ for $i\in\{2,3,\ldots,p\}$ that is attained when we set $\symbf{a}_i=\symbf{e}_i$, under the restriction $\symbf{a}_i$ to be orthogonal to $\symbf{a}_1, \symbf{a}_2, \ldots, \symbf{a}_{i-1}$.

For $i\neq j$, $\mathrm{Cov}[\symbf{e}_i'\symbf{X},\symbf{e}_j'\symbf{X}] = \symbf{e}_i'\symbf{\Sigma}\symbf{e}_j = 0$.

# A note on terminology

- Principal components (the $\symbf{Y}_i$'s) may also be referred to as "scores".
- The coefficient vectors $\symbf{a}_i$ may also be referred to as "loadings" (thinking algebraically) or as "principal axes"/"principal directions" (thinking geometrically).

# Example {.smaller}

::::{.columns}

:::{.column width="50%"}

Suppose that we have the following variance-covariance matrix:

```{r}
#| echo: true
vcov_mat <- matrix(
  c(1, 0.9, 0.9, 0, 0, 0, 0,
    0.9, 1, 0.9, 0, 0, 0, 0,
    0.9, 0.9, 1, 0, 0, 0, 0,
    0, 0, 0, 1, 0.9, 0.9, 0.9,
    0, 0, 0, 0.9, 1, 0.9, 0.9,
    0, 0, 0, 0.9, 0.9, 1, 0.9,
    0, 0, 0, 0.9, 0.9, 0.9, 1),
  byrow = TRUE,
  nrow = 7
)
colnames(vcov_mat) <- paste0("V", 1:7)
rownames(vcov_mat) <- paste0("V", 1:7)
```

:::

:::{.column width="50%"}

```{r}
#| results: asis
knitr::kable(vcov_mat)
```

:::

::::

# Example (cont.) {.smaller}

::::{.columns}

:::{.column width="50%"}

- We apply the eigen decomposition to the variance-covariance matrix:

```{r}
#| echo: true
eig_obj <- eigen(vcov_mat)
```

- to obtain the following eigenvectors:

```{r}
#| echo: true
eig_vec_mat <- eig_obj$vectors
rownames(eig_vec_mat) <- paste0("V", 1:7)
colnames(eig_vec_mat) <- paste0("PC", 1:7)
```

:::

:::{.column width="50%"}

```{r}
eig_vec_mat |> signif(2)
```

:::

::::

# Example (cont.) {.smaller}

::::{.columns}

:::{.column width="50%"}

- By the variance formula, these linear combinations have the following variances:

```{r}
#| echo: true
(eig_vec_mat |> t()) %*%
  vcov_mat %*%
  (eig_vec_mat) |>
  diag() |>
  signif(2)
```

:::

:::{.column width="50%"}

- These are the same as the eigenvalues (by theorem 2):

```{r}
#| echo: true
eig_obj$values |>
  signif(2) |>
  setNames(paste0("PC", 1:7))
```

- Clearly, due to the correlation structure, the orthogonality constraint implies that only two PCs have meaningful variance.

:::

::::

# Theorem three: Total variance {.smaller}

Let $\symbf{X}' = [X_1, X_2, \ldots, X_p]'$ be a random vector with covariance matrix $\symbf{\Sigma}$ that has eigenvalue-eigenvector pairs $(\lambda_1, \symbf{e}_1), \ldots, (\lambda_p, \symbf{e}_p)$, ordered such that $\lambda_1 \geq \lambda_2 \geq \ldots \geq \lambda_p \geq 0$. Define the principal components $Y_i = \symbf{e}'_i\symbf{X}$ for $i = 1, \ldots, p$. Then, the sum of variances of the original variables is equal to the sum of the eigenvalues of $\symbf{\Sigma}$, which is also equal to the sum of variances of the principal components:

$$
\sum_{i=1}^{p} \sigma_{ii} = \sum_{i=1}^{p} \text{Var}(X_i) = \sum_{i=1}^{p} \lambda_i = \sum_{i=1}^{p} \text{Var}(Y_i).
$$

# Proof of theorem three {.smaller}

The trace of the covariance matrix $\symbf{\Sigma}$, which is the sum of the variances of the original variables.

Since $\symbf{\Sigma}$ can be decomposed as $\symbf{\Sigma} = \symbf{PDP}'$, where $\symbf{D}$ is the diagonal matrix of eigenvalues and $\symbf{P}$ is the matrix of eigenvectors, we have:

$$
\text{trace}(\symbf{\Sigma}) = \text{trace}(\symbf{PDP}') = \text{trace}(\symbf{D}) = \sum_{i=1}^{p} \lambda_i = \sum_{i=1}^{p} \text{Var}(Y_i).
$$

# Contribution per component

Theorem 2 showed that

$$
\text{trace}(\symbf{\Sigma}) = \sum \lambda_i = \sum \text{Var}(Y_i).
$$

This implies that the proportion of the total variance due to the $k^{th}$ principal component is

$$
\frac{\lambda_k}{\sum \lambda_i },\quad k=1,...p,
$$

and (due to being uncorrelated) the proportion of variance accounted for by the first $r$ principal components is given by

$$
\frac{\sum_{i=1}^r \lambda_i }{\sum \lambda_i }.
$$

# The scree plot

:::: {.columns}

::: {.column width="50%"}

- The scree plot is a graphical representation of the proportion of variance accounted for by each principal component.
- It plots $(i, \lambda_i/\sum_{j=1}^{p} \lambda_j)$ for $i = 1, \ldots, p$.
- Typically, we look for the "elbow" in the plot, which indicates the number of principal components to retain.
- Beyond this point, each additional principal component captures little additional variation.

:::

::: {.column width="50%"}

```{r}
plot_tbl <- tibble::tibble(
  pc = 1:7,
  prop_var = eig_obj$values / sum(eig_obj$values)
)
plot_tbl_point <- purrr::map_df(
  seq(5.5, 5.502, length.out = 9), function(x) {
    plot_tbl |>
      dplyr::filter(pc == 2) |>
      dplyr::mutate(size_point = x)
  }
)
p <-  ggplot(
  data = plot_tbl
  ) +
    geom_line(
      aes(x = pc, y = prop_var),
      stat = "identity",
      colour = "dodgerblue"
    ) +
    geom_point(
      data = plot_tbl_point |> dplyr::slice(5:9),
      aes(x = pc, y = prop_var, size = size_point),
      colour = "mediumseagreen",
      alpha = 0.9,
      shape = 1,
      show.legend = FALSE
    ) +
    geom_point(
      aes(x = pc, y = prop_var),
      colour = "dodgerblue",
      alpha = 0.9,
      size = 1.5
    ) +
    labs(
      x = "Principal component index",
      y = "Proportion of variance"
    ) +
    cowplot::theme_cowplot() +
    cowplot::background_grid(major = "xy") +
    theme(
      panel.background = element_rect(fill = "white"),
      plot.background = element_rect(fill = "white")
    ) +
    scale_x_continuous(
      breaks = 1:7
    )
path_p_scree <- projr::projr_path_get("cache", "fig", "p-scree.pdf")
cowplot::save_plot(
  plot = p,
  filename = path_p_scree,
  units = "cm",
  base_height = 7,
  base_width = 8 * (7/6)
)
```

```{r}
#| results: asis
knitr::include_graphics(path_p_scree)
```

:::

::::

# Theorem 4: Correlation with original variables

For $Y_i$ and $X_k$ the $i$-th principal component and the $k$-th original variable, respectively, we have that

$$
\rho_{Y_i,X_k} = \frac{\sqrt{\lambda_i}e_{ik}}{\sqrt{\sigma_{kk}}}.
$$


# Proof of theorem 3

Let $\symbf{a}_k'$ equal 1 in position $k$ and 0 otherwise, so that $X_k = \symbf{f}_k'\symbf{X}$. We note that

$$
\text{Cov}(X_k,Y_i) = \text{Cov}(\symbf{a}_k'\symbf{X},\symbf{e}_i'\symbf{X})=\symbf{a}_k'\symbf{\Sigma}\symbf{e}_i=\symbf{a}_k'\symbf{PDP'}\symbf{e}_i=\lambda_i\symbf{a}_k'\symbf{e}_i=\lambda_ie_{ik}.
$$

We know that $\text{Var}(Y_i)= \lambda_i$ and $\text{Var}(X_k)=\sigma_{kk}$, so

$$
\rho_{Y_i,X_k}=\frac{\text{Cov}(Y_i,X_k)}{\sqrt{\text{Var}(Y_i)}\sqrt{\text{Var}
(X_k)}}= \frac{\sqrt{\lambda_i}e_{ik}}{\sqrt{\sigma_{kk}}}
$$

# Example {.smaller}

::::{.columns}

:::{.column width="70%"}

- From our previous example where we had two sets of correlated variables, we obtain the following correlation matrix: 

```{r}
#| results: asis
P <- eig_obj$vectors
D <- diag(eig_obj$values)
var_mat <- diag(diag(vcov_mat))
numerator <- P %*% sqrt(D)
corr_mat <- var_mat %*% numerator
rownames(corr_mat) <- paste0("V", 1:7)
colnames(corr_mat) <- paste0("PC", 1:7)
corr_mat |> signif(2) |> knitr::kable()
```

:::

:::{.column width="30%"}

- As all variables have unit variance, the only things that matter are the eigenvalue and the eigenvector.
  - High loading and high eigenvalue $\rightarrow$ high correlation.
- Higher variance (hypothetically) decorrelates PCs and original variables.

:::

::::

# Constant-density contours I {.smaller}

By the definition of the MVN PDF, the density of $X$ is constant on the ellipsoid defined by the equation

$$
(\symbf{x} - \mu)'\Sigma^{-1}(\symbf{x} - \mu) = c^2.
$$

The ellipsoid's shape, size and orientation are determined by the eigendecomposition of $\symbf{\Sigma}$.

WLOG, we assume $\symbf{\mu} = \symbf{0}$. Then, the ellipsoid is defined by

$$
c^2 = x'\Sigma^{-1}x = \frac{1}{\lambda_1}(\symbf{e}_1'\symbf{x})^2 + \frac{1}{\lambda_2}(\symbf{e}_2'\symbf{x})^2 + \ldots + \frac{1}{\lambda_p}(\symbf{e}_p'\symbf{x})^2
$$

after multiplying out.

# Constant-density contours (cont.) {.smaller}

The axes of the ellipse are then given by

$$
\pm c\sqrt{\lambda_i}\symbf{e}_i, \quad i = 1,2,\ldots,p.
$$

The eigenvectors therefore define the direction and the eigenvalues the length of the axes of the ellipsoid.

Furthermore, if we set $y_i = \symbf{e}_i'\symbf{x}$ (the $i$-th principal componet for $\symbf{x}$), we have

$$
c^2 = \frac{1}{\lambda_1}y_1^2 + \ldots + \frac{1}{\lambda_p}y_p^2.
$$

The equation is therefore satisfied by the principal components of $\symbf{x}$.

# Variance standardisation

- Before applying PCA, one can standardise the variances to have unit variance.
- This may be appropriate when:
  - the variances have different units (e.g. weight in kg vs annual income in rands), or
  - the variances have different scales (e.g. some genes are extremely rare whereas others are abundantly expressed), or
  - we wish to eliminate any effect of differences in variance on downstream analyses, e.g. when applying penalised regression.
- Similar comments go for logging the data, or applying other transformations.
- All results go through in exactly the same way, with adjustments made in interpreting exactly what variables the principal components capture.

# Standardisation example

::::{.columns}

:::{.column width="50%"}

We have the following covariance matrix:

```{r}
cov_mat <- matrix(
  c(1, 4, 4, 100), nrow = 2
)
cov_mat
```

which produces the following eigendecomposition:

```{r}
eigen(cov_mat) |> lapply(function(x) signif(x, 2))
```

:::

:::{.column width="50%"}

The associated correlation matrix:

```{r}
diag_mat_sd_inv <- diag(diag(cov_mat), nrow = nrow(cov_mat)) |> sqrt() |> solve()
rho_mat <- diag_mat_sd_inv %*% cov_mat %*% diag_mat_sd_inv
rho_mat 
```

has the following eigendecomposition:

```{r}
eigen(rho_mat) |> lapply(function(x) signif(x, 2))
```

The eigenvectors are in very different directions.

:::

::::

# Sample principal components {.smaller}

::::{.columns}

:::{.column width="50%"}

In practice, we do not know $\symbf{\Sigma}$ and must estimate it by the sample covariance matrix $\symbf{S}=n^{-1}\symbf{X}'\symbf{X}$ (assuming $\symbf{X}$ is mean-centred; adjust otherwise).

For example, sample covariance matrix of the height and weight data is:

```{r}
#| echo: true
#| results: asis
cov_mat <- hw_tbl |>
  as.matrix() |>
  cov()
cov_mat |> signif(2) |> knitr::kable()
```

:::

:::{.column width="50%"}

- The eigen decomposition is then:

```{r}
#| echo: true
eigen(cov_mat) |>
  lapply(function(x) signif(x, 2))
```

So, the first principal component is given by projections onto $[1, 1]$.

:::

::::


# Sample principal components (cont.) {.smaller}

If $\symbf{X} \sim \mathcal{N}(\symbf{0}, \symbf{\Sigma})$, then we have that that

$$
Y_i \sim \mathcal{N}(0, \lambda_i) \quad \text{and} \quad \symbf{Y} \sim \mathcal{N}_p(\symbf{0}, \symbf{\Lambda}).
$$

In the height and weight example then, we estimate this to be

$$
\symbf{Y} \sim \mathcal{N}_p(\symbf{0}, \begin{bmatrix} 1.71 & 0 \\ 0 & 0.31 \end{bmatrix}).
$$

# Approximating a matrix $\symbf{X}$

Suppose we have a matrix $\symbf{X}$ of size $n \times p$ with rank $t$ and we wish to approximate it with a matrix $\hat{\symbf{X}}$ of rank $s < t$.

In other words, we wish to find a matrix

$$
\hat{\symbf{X}} = \symbf{A}\symbf{B},
$$

where $\symbf{A}$ is of size $n \times s$ and $\symbf{B}$ is of size $s \times p$, such that

$$
\symbf{X} \approx \hat{\symbf{X}}.
$$

We want a rank $s$ approximation to the matrix $\symbf{X}$.

# Approximation example 

In the height and weight dataset, we obtained a size variable (the first principal component) that was an equally-weighted combination of weight and height, as follows:

$$
\text{size} = \frac{1}{\sqrt{2}}\text{height} + \frac{1}{\sqrt{2}}\text{weight}. = \begin{pmatrix} \text{height} && \text{weight} \end{pmatrix} \begin{pmatrix} \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} \end{pmatrix}.
$$

We'll show that can approximate the original height and weight variables using the size variable:

$$
[\text{height}, \text{weight}] \approx \text{size} \begin{pmatrix} \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \end{pmatrix}  = \begin{pmatrix} 1/\sqrt{2} \text{size} & 1/\sqrt{2} \text{size} \end{pmatrix}.
$$

We therefore have a "rule" for mapping size onto weight and height.

# Approximation example (cont.) {.smaller}

Suppose that a given observation has height 0.7 and weight 0.5.

This implies a first principal component (size) of

$$
\text{size} = \frac{1}{\sqrt{2}}\times 0.7 + \frac{1}{\sqrt{2}}\times 0.5 = 0.85.
$$

We can approximate the height and weight as follows:

$$
\begin{pmatrix} \hat{\text{height}} & \hat{\text{weight}} \end{pmatrix} \approx \begin{pmatrix} 1/\sqrt{2} \times 0.85 & 1/\sqrt{2} \times 0.85 \end{pmatrix} = \begin{pmatrix} 0.6 & 0.6 \end{pmatrix}.
$$

Considering the standard deviation is (by construction) 1, the error (0.1 in both cases) is small.

If weight and height were very different, the error would be larger (why?).

# Why are about approximation?

Whether or not PCA is a good approximation matters because

a) we may actually want to reconstitute the original data after compression (e.g. image compression),
b) we want to know we're not losing too much information and
c) it helps provide some intuition about what we're doing.

# Minimising the least squares error of the approximation

We wish to find a matrix $\hat{\symbf{X}}$ of rank $s$ that that minimises the least squares error:

$$
\text{min}_{\hat{\symbf{X}}} \sum_{i=1}^{n} \sum_{j=1}^{p} (x_{ij} - \hat{x}_{ij})^2 = \text{min}_{\hat{\symbf{X}}} \text{trace}( (\symbf{X} - \hat{\symbf{X}})(\symbf{X} - \hat{\symbf{X}})').
$$

We are going to

1. prove that SVD can be used to find $\hat{\symbf{X}}$, and
2. that the PCA and SVD are equivalent in this context.

# Theorem 5a: SVD approximation

Let $\symbf{X}$ be a matrix of size $n \times p$ with rank $t$ and SVD $\symbf{X} = \symbf{U}\symbf{D}\symbf{V}'$. Then the best rank $s$ approximation to $\symbf{X}$ is given by

$$
\hat{\symbf{X}} = \symbf{U}\symbf{D}\symbf{J}_s\symbf{V}',
$$

where $\symbf{J}_s$ is the matrix of size $p \times p$ with the first $s$ diagonal elements equal to 1 and the rest equal to 0.

This is equivalent to

$$
\hat{\symbf{X}} = \sum_{i=1}^{s} d_i\symbf{u}_i\symbf{v}_i',
$$

where $\symbf{u}_i$ and $\symbf{v}_i$ are the left- and right-singular vectors and $d_i$ is the $i$-th singular value of $\symbf{X}$.

# Proof of Theorem 5a {.smaller} 

\begin{align*}
& \text{We use } UU^* = I_m \text{ and } VV^* = I_k \text{ to write the sum of squares as} \\
& \text{tr}[(A - B)(A - B)'] = \text{tr}[UU'(A - B)VV'(A - B)'] \text{ (by orthonormality)}, \\
& = \text{tr}[U'(A - B)VV'(A - B)U] \text{ (by cyclic trace property)}, \\
& = \text{tr}[U'(A - B)V(U'(A - B)V)'] \text{ (as }\mathrm{tr}(AB)=B'A'\text{)},\\
& = \text{tr}[(U'AV-U'BV)(U'AV-U'BV)^*] \text{ (multiplying in)}. \\
& \text{As } U'AV=U'UDV'V=D \text{ and letting } C = U'BV, \text{ from above we have that} \\
& \text{tr}[(A - B)(A - B)'] = \text{tr}[(D - C)(D - C)'] \\
& = \sum_{i,j=1}^{m} (d_{ij} - c_{ij})^2, \\
& = \sum_{i=1}^{m} (d_i - c_{ii})^2 + \sum_{i\neq j} c_{ij}^2 \text{ (as D is diagonal).}
\end{align*}

# Proof of Theorem 5a (cont.) {.smaller} 

The quantity $\sum_{i=1}^{m} (d_i - c_{ii})^2 + \sum_{i\neq j} c_{ij}^2$ is minimised when 

1. $c_{ij} = 0$ for $i \neq j$, 
2. $c_{ii} = d_i$ for $i \leq s$, and
3. $c_{ii}=0$ otherwise.

We choose the $s$ largest singular values as these reduce the error terms along the diagonals the most.
We cannot choose more than $s$ as we $\symbf{U}'\symbf{B}\symbf{V}$ is diagonal, meaning the number of non-zero diagonal elements is its rank and its rank must be $s$ or less.

This implies that 

$\symbf{U}'\symbf{B}\symbf{V} = \symbf{D}\symbf{J}_s \Rightarrow \symbf{B} = \symbf{U}\symbf{D}\symbf{J}_s\symbf{V}'.$

# Theorem 5b: PCA leads to same approximation as the SVD {.smaller}

The PCA approximation to $\symbf{X}$ is given by $\tilde{\symbf{X}}=\symbf{Y}\symbf{J}_k\symbf{P}'$, where $\symbf{J}_k$ is the matrix of size $p \times p$ with the first $k$ diagonal elements equal to 1 and the rest equal to 0.

This is the same as the SVD approximation, i.e.

$$
\hat{\symbf{X}} = \symbf{U}\symbf{D}\symbf{J}_k\symbf{V}' = \tilde{\symbf{X}} = \symbf{Y}\symbf{J}_k\symbf{P}'.
$$

# Theorom 5b: proof {.smaller}

The principal components are given by

$$
\symbf{Y} = \symbf{X}\symbf{P}.
$$

The first $k$ principal components are given by

$$
\symbf{Y}\symbf{J}_k = \symbf{X}\symbf{P}\symbf{J}_k,
$$

setting the last $p-k$ columns of $\symbf{P}$ to 0.

This implies that the approximation to $\symbf{X}$ is given by 

$$
\tilde{\symbf{X}} = \symbf{Y}\symbf{J}_k\symbf{P}'.
$$

# Theorom 5b: proof (cont.) {.smaller}

The columns of $\symbf{P}$ are the eigenvectors of $\symbf{X}'\symbf{X}$, which are the same as the right singular vectors of $\symbf{X}$, i.e.

$$
\symbf{P} = \symbf{V}.
$$

This implies that

$$
\tilde{\symbf{X}} = \symbf{Y}\symbf{J}_k\symbf{P} = \symbf{XV}\symbf{J}_k\symbf{V}' = \symbf{U}\symbf{D}\symbf{VV}'\symbf{J}_k\symbf{V}' = \symbf{U}\symbf{D}\symbf{J}_k\symbf{V}' = \hat{\symbf{X}}.
$$

# Example

:::: {.columns}

::: {.column width="57%"}

We read in data of measurements Painted Turtles [@jolicoeur_mosimann60]:

```{r}
#| eval: false
#| echo: true
if (!requireNamespace(
  "remotes", quietly = TRUE
  )) {
  install.packages("remotes")
}
"MiguelRodo/DataTidy23RodoHonsMult@2024" |>
  remotes::install_github()
```

:::

::: {.column width="43%"}

```{r}
#| echo: true
data(
  "data_tidy_turtle",
  package = "DataTidy23RodoHonsMult"
)
data_tidy_turtle
```

:::

::::

# Measurements highly correlated

::::{.columns}

:::{.column width="50%"}

```{r}
#| include: false
plot_tbl_turtle <- data_tidy_turtle |>
  tidyr::pivot_longer(
    length:width,
    names_to = "x_var",
    values_to = "x_val"
  )
p <- ggplot(
  plot_tbl_turtle,
  aes(x = x_val, y = height, col = gender)
) +
  geom_point(alpha = 0.75) +
  scale_colour_manual(
    values = c(male = "dodgerblue", female = "mediumseagreen")
   ) +
  facet_wrap(~x_var, scales = "free", ncol =1 ) +
  cowplot::theme_cowplot() +
  cowplot::background_grid(major = "xy") +
  theme( 
    panel.background = element_rect(fill = "white"), 
    plot.background = element_rect(fill = "white")
  ) +
  labs(
    x = "Measurement",
    y = "Height"
  ) +
  theme(
    legend.position = "bottom",
    legend.title = element_blank()
  ) +
  theme(
    axis.ticks = element_blank(),
    axis.text = element_blank()
  ) +
  theme(
    strip.background = element_rect(fill = "white", colour = "gray25")
  )
path_p_turtle <- projr::projr_path_get("cache", "fig", "p-turtle.png")
cowplot::save_plot(
  plot = p,
  filename = path_p_turtle,
  units = "cm",
  base_height = 7,
  base_width = 5
)
```

```{r}
#| results: "asis"
knitr::include_graphics(path_p_turtle)
```

:::

:::{.column width="50%"}

- Due to the correlation, we may likely summarise the data very well using principal components.
- We'll use the `prcomp` function in R, and do it from first principles.

:::

::::

# Using the `prcomp` function I {.smaller}

::::{.columns}

:::{.column width="70%"}

```{r}
#| echo: true
pr_obj <- prcomp(
  ~log(length) + log(width) + log(height),
  data = data_tidy_turtle,
  retx = TRUE
)
pr_obj
```

:::

:::{.column width="30%"}

- The first principal component has a markedly higher standard deviation.
- It is weighted roughly evenly across the variables, indicating it is a general "size" variable.

:::

::::

# Using the `prcomp` function II {.smaller}

::::{.columns}

:::{.column width="70%"}

```{r}
#| echo: true
summary(pr_obj)
```

:::

:::{.column width="30%"}

- The `summary` function provides the per-component standard deviation and variance, as well as the cumulative variance.

:::

::::

# Using the `prcomp` function III {.smaller}

::::{.columns}

:::{.column width="70%"}

```{r}
#| echo: true
plot(pr_obj)
```

:::

:::{.column width="30%"}

- The `plot` command provides the scree plot.
- Clearly, unless we're specifically interested in non-size variables we should only retain the first principal component.

:::

::::

# Using the `prcomp` function IV {.smaller}

Here are the principal components:

```{r}
#| echo: true
pr_obj$x |> head() |> signif(2)
```

# From first principles I {.smaller}

Using the SVD, we can obtain the same results as the `prcomp` function.

Removing gender, logging and centering the data:

```{r}
#| echo: true
data_turtle_mat <- data_tidy_turtle |>
  dplyr::select(-gender) |>
  dplyr::mutate(across(everything(), log)) |>
  dplyr::mutate(across(everything(), function(x) x - mean(x))) |>
  as.matrix()
```

We then calculate the SVD:

```{r}
#| echo: true
svd_turtle <- svd(data_turtle_mat)
```

# From first principles II {.smaller}

The eigenvalues are the squares of the singular values:

```{r}
#| echo: true
svd_turtle$d^2 |> signif(3)
```

The right-singular vectors are the eigenvectors of the covariance matrix, and hence are the loading vectors, implying the principal components are given by:

```{r}
#| echo: true
(data_turtle_mat %*% svd_turtle$v) |> head() |> signif(2)
```

# Biplots

- Since each data point has many variables, graphical display is cumbersome.
- However, we may use PCA to provide an approximate graphical display of the data.
- It will show both:
  - Each observation, and
  - Each variable.
- Hence, "bi"plot.

# Definition of a biplot

Suppose we have a rank $s$ matrix $\hat{\symbf{X}}:n\times p$.
Then we can write

$$
\hat{\symbf{X}} = \symbf{A}\symbf{B},
$$

where $\symbf{A}$ is of size $n \times s$ and $\symbf{B}$ is of size $s \times p$.

The biplot is a plot of the rows of $\symbf{A}$ and the columns of $\symbf{B}$.

Typically, we plot the rows as points and the columns as vectors.

# A PCA biplot

The PCA approximation to a matrix $\symbf{X}$ is given by

$$
\symbf{Y}\symbf{J}_k\symbf{P}'=\symbf{Y}\symbf{J}_k\symbf{J}_k\symbf{P}'=\symbf{A}\symbf{B},
$$

where $\symbf{A}=\symbf{Y}\symbf{J}_k$ and $\symbf{B}=\symbf{J}_k\symbf{P}'$.

Typically, we set $k=2$ and ignore the all-zero columns/rows of $\symbf{A}$ and $\symbf{B}$.

The rows of $\symbf{Y}$ are the scores (representing the observations) and the rows of $\symbf{P}'$ are the loadings (representing the variables).

# Interpreting a biplot

- An observation has (approximately) an above-average value of a variable if it is close to the tip of the vector representing that variable.
- Two variables are (approximately) correlated if their vectors are close together.
  - They are (approximately) uncorrelated if they are orthogonal.

\centering

\includegraphics[width=0.65\textwidth]{_data_raw/img/biplot.png}

# Resources

- Source code is on [GitHub](https://github.com/MiguelRodo/Slides23RodoHonsMult/blob/2024/SlidesHonsMultL7PCA.qmd).
  - Suppressed code (e.g. to create figures) is available there.

## References

---
title: Least-squares regression
subtitle: Multiple, multivariate, PCR and PLS approaches
author: Miguel Rodo (reworking slides by Stefan Britz)
date: "2024-04-11"
bibliography: zotero.bib
format:
  beamer:
    embed-resources: true
    aspectratio: 169
    urlcolor: cyan
    linkcolor: blue
    filecolor: magenta
    include-in-header:
      file: preamble-reg.tex
nocite: |
    @johnson_wichern07
---

# Linear regression

- Simple regresssion: one predictor, one response
  - Typical notation: $Y = \beta_0 + \beta_1 X + \epsilon$
  - Example: Ice-cream sales against temperature
\pause
- Multiple regression: multiple predictors, one response
  - Typical notation: $Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_p X_p + \epsilon$, OR $Y = \symbf{X}'\symbf{\beta} + \epsilon$
  - Example: House price against the number of rooms, the size of the garden, the distance to the city center, etc.
\pause
- Multivariate regression: multiple predictors, multiple responses
  - Typical notation: $\symbf{\mathcal{Y}} = \symbf{X}'\symbf{\mathcal{B}} + \symbf{\mathcal{E}}$, where $\symbf{\mathcal{Y}}$, $\symbf{\mathcal{B}}$ and $\symbf{\mathcal{E}}$ have $r$ columns
  - Example: Rainfall and wind speed against hurricane category, altitude and distance from the coast

# Transformations of input variables

- We've often transformed input variables individually, e.g. squaring them or taking the logarithm
  - For example, if we have a predictor $X$, we can use $X^2$ as a predictor and have $Y = \beta_0 + \beta_1 X^2 + \epsilon$
\pause
- We will also now consider taking *linear combinations* of different input variables, e.g. creating the variable $X_1^{\ast}=\alpha_1X_1+\alpha_2X_2$ and using it as a predictor in the model
  - We consider two ways of deciding on the linear combinations:
    - Principal components analysis: maximising variance
    - Partial least squares: maximising association with the response

# A noteworthy note on notable notation

- Number of...
  - Observations: $n$
  - Responses: $r$, where $r=1$ for multiple regression ($\symbf{y}:n\times 1$) but $r\geq 1$ for multivariate regression ($\symbf{Y}:n\times r$)
  - Predictors: $k$, i.e. $\symbf{\beta}:k\times 1$ and $\symbf{\mathcal{B}}:k\times r$
  - Predictors, apart from the intercept: $p=k-1$, i.e. $\symbf{x}'\symbf{\beta} = \beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p$

# Observational unit

:::: {.columns}

::: {.column width=50%}

## Multiple regression

- Each observational unit consists of a single response variable ($y_i$) and multiple predictors ($\symbf{x}_i= \begin{bmatrix} x_{i1} & x_{i2} & \ldots & x_{ip} \end{bmatrix}'$)
- Example responses per observational unit:
  - Annual revenue of a company
  - Number of pigeons at a single university
  - Blood pressure of an individual
:::

\pause

::: {.column width=50%}

## Multivariate regression

- Each observational unit consists of multiple response variables ($\symbf{y}_i= \begin{bmatrix} y_{i1} &y_{i2} & \ldots & y_{ir} \end{bmatrix}'$) and multiple predictors ($\symbf{x}_i= \begin{bmatrix} x_{i1} & x_{i2} & \ldots & x_{ip} \end{bmatrix}'$)
- Example responses per observational unit:
  - Stock prices of a single company at multiple timepoints
  - Number of pigeons and starlings at a single university
  - Measurements of 1000 genes of an individual
:::

::::

# Model assumptions {.smaller}

:::: {.columns}

::: {.column width=50%}

## Multiple regression

- **Linearity**: $\mathrm{E}[Y] = \beta_0 + \beta_1 X+1 + \ldots + \beta_p X_p$
- **Constant variance**: $\mathrm{Var}[\epsilon] = \sigma^2$
- **Independence**: $\mathrm{Cov}[\epsilon_i, \epsilon_j] = 0 \text{ for } i \neq j$
  - Different observations entirely independent
  - For example, different individuals in a study have independent BMI measurements

:::

\pause

::: {.column width=50%}

## Multivariate regression

- **Linearity**: $\mathrm{E}[\symbf{\mathcal{Y}}] = \symbf{X}'\symbf{\mathcal{B}}$
- **Constant variance**: $\mathrm{Var}[\symbf{\mathcal{E}}] = \symbf{\Sigma}$
- **Independence**: $\mathrm{Cov}[\symbf{\mathcal{E}}_i, \symbf{\mathcal{E}}_j] = \symbf{0} \text{ for } i \neq j$
  - Different observational units are independent, but different responses within the same observational unit may be correlated
  - For example, different individuals have independent BMI measurements, but multiple measurements of BMI from the same individual are clearly correlated

:::

::::

# Estimating the regression coefficients

:::: {.columns}

::: {.column width=50%}

## Multiple regression

Choose $\hat{\symbf{\beta}}$ to minimise the sum of squared residuals:

\begin{align*}
 &\sum_{i=1}^n (y_i - \hat{y}_i)^2 \\
=\; &(\symbf{y} - \hat{\symbf{y}})'(\symbf{y} - \hat{\symbf{y}}), \\
=\; &(\symbf{y} - \symbf{X}\hat{\symbf{\beta}})'(\symbf{y} - \symbf{X}\hat{\symbf{\beta}}).
\end{align*}

This implies that

$$
\hat{\symbf{\beta}} = (\symbf{X}'\symbf{X})^{-1}\symbf{X}'\symbf{y}.
$$

:::

\pause

::: {.column width=50%}

## Multivariate regression

Choose $\hat{\symbf{\mathcal{B}}}$ to minimise the sum of squared residuals:

\begin{align*}
 &\sum_{i=1}^n\sum_{j=1}^r (y_{ij} - \hat{y}_{ij})^2 \\
=\; &\mathrm{tr}[(\symbf{\mathcal{Y}} - \symbf{X}\hat{\symbf{\mathcal{B}}})'(\symbf{\mathcal{Y}} - \symbf{X}\hat{\symbf{\mathcal{B}}})].
\end{align*}

This implies that 

$$
\hat{\symbf{\mathcal{B}}} = (\symbf{X}'\symbf{X})^{-1}\symbf{X}'\symbf{\symbf{Y}}.
$$

:::

::::

# Fitted values

- The fitted values are the values of the response variable predicted by the model:
  - **Multiple regression**: $\hat{y}_i = \symbf{x}_i'\hat{\symbf{\beta}}$
  - **Multivariate regression**: $\hat{\symbf{\mathcal{Y}}}_{[i]} = \symbf{x}_i'\hat{\symbf{\mathcal{B}}}$
    - By convention, we use square brackets (e.g. $\symbf{A}_{[i]}$) to denote the $i$-th row of a matrix
\pause

# The hat matrix {.smaller}

- A special matrix appears in both cases (now predicting the response for all observations at once):
  - **Multiple regression**: $\hat{\symbf{y}} = \symbf{X}\hat{\symbf{\beta}}=\symbf{X}(\symbf{X}'\symbf{X})^{-1}\symbf{X}'\symbf{y}= \symbf{H}\symbf{y}$
  - **Multivariate regression**: $\hat{\symbf{\mathcal{Y}}}=\symbf{X}\hat{\symbf{\mathcal{B}}}=\symbf{X}(\symbf{X}'\symbf{X})^{-1}\symbf{X}'\symbf{\mathcal{Y}}= \symbf{H}\symbf{\mathcal{Y}}$
\pause
- This matrix is called the **hat matrix** and is denoted by $\symbf{H}$
  - The reason for the name is that it "puts a hat" on the response vector $\symbf{y}$ or $\symbf{\mathcal{Y}}$
- Special properties:
  - It is a projection matrix, i.e. $\symbf{H}\symbf{y}=\mathrm{proj}_{\symbf{X}}(\symbf{y})$
  - It is idempotent, i.e. $\symbf{H}^2=\symbf{H}$

# Estimator properties

:::: {.columns}

::: {.column width=50%}

## Multiple regression

- **Unbiased**: $\mathrm{E}[\hat{\symbf{\beta}}] = \symbf{\beta}$
- **Zero-mean residual**: $\mathrm{E}[\symbf{\epsilon}]=\mathrm{E}[\symbf{X}'\symbf{\beta} - \symbf{X}'\hat{\symbf{\beta}}] = \symbf{0}$
- **Correlated estimation errors**: $\mathrm{Cov}[\hat{\symbf{\beta}}] = \sigma^2(\symbf{X}'\symbf{X})^{-1}$
- **Correlated residuals**: $\mathrm{Cov}[\hat{\symbf{\epsilon}}] = \sigma^2(\symbf{I} - \symbf{X}(\symbf{X}'\symbf{X})^{-1}\symbf{X}')  =\sigma^2(\symbf{I} - \symbf{H})$

:::

::: {.column width=50%}

## Multivariate regression

- **Unbiased**: $\mathrm{E}[\hat{\symbf{\mathcal{B}}}] = \symbf{\mathcal{B}}$
- **Zero-mean residual**: $\mathrm{E}[\symbf{\mathcal{E}}]=\mathrm{E}[\symbf{X}'\symbf{\mathcal{B}} - \symbf{X}'\hat{\symbf{\mathcal{B}}}] = \symbf{0}$
- **Correlated estimation errors**: $\mathrm{Cov}[\hat{\symbf{\mathcal{B}}}_i, \hat{\symbf{\mathcal{B}}}_j] = \sigma^2_{ij}(\symbf{X}'\symbf{X})^{-1}$ for $i,j\in\{1,\ldots,r\}$, where $\symbf{\mathcal{B}}_i$ is the $i$th column of $\symbf{\mathcal{B}}$
- **Correlated residuals**: $\mathrm{Cov}[\hat{\symbf{\mathcal{E}}}_i, \hat{\symbf{\mathcal{E}}}_j] = \sigma_{ij}(\symbf{I} - \symbf{H})$ for $i,j\in\{1,\ldots,r\}$
  - Now the correlation is both between different responses (columns of $\symbf{\mathcal{Y}}$) and between different observations (rows of $\symbf{\mathcal{Y}}$)

:::

::::

# Estimating the variance

:::: {.columns}

::: {.column width=50%}

## Multiple regression

- The estimator of the variance is

$$
\hat{\sigma^2} = s^2 = \frac{1}{n-k}(\symbf{y} - \symbf{X}\hat{\symbf{\beta}})'(\symbf{y} - \symbf{X}\hat{\symbf{\beta}}).
$$

- Properties:
  - **Unbiased**: $\mathrm{E}[s^2] = \sigma^2$
  - **Independent of $\hat{\symbf{\beta}}$**: $\mathrm{Cov}[s^2, \hat{\symbf{\beta}}] = \symbf{0}$

:::

\pause

::: {.column width=50%}

## Multivariate regression

- The estimator of the variance $\symbf{\Sigma}$ is

$$
\hat{\symbf{\Sigma}} = \symbf{\symbf{S}}= \frac{1}{n-k}(\symbf{\mathcal{Y}} - \symbf{X}\hat{\symbf{\mathcal{B}}})'(\symbf{\mathcal{Y}} - \symbf{X}\hat{\symbf{\mathcal{B}}}).
$$

- Properties:
  - **Unbiased**: $\mathrm{E}[\symbf{S}] = \symbf{\Sigma}$
  - **Independent of $\hat{\symbf{\mathcal{B}}}$**: $\symbf{S}$ and $\hat{\symbf{\mathcal{B}}}$ are independent

:::

::::

# Sums of squares: Multiple regression {.smaller}

:::: {.columns}

::: {.column width=50%}

The total sum of squares is

$$
\mathrm{SST}= \sum_{i=1}^n(y_i - \bar{y})^2,
$$

and can be decomposed into

$$
\mathrm{SSR} + \mathrm{SSE} =
\sum_{i=1}^n(\hat{y}_i - \bar{y})^2 + \sum_{i=1}^n(y_i - \hat{y}_i)^2,
$$

where SST, SSR and SSE are the sums of squares due to the total, regression and error, respectively.

:::

\pause

::: {.column width=50%}

In matrix notation, these are given by

\begin{align*}
\mathrm{SST} &= \symbf{y}'\symbf{y} - n\bar{y}^2, \\
\mathrm{SSR} &= \hat{\symbf{y}}'\hat{\symbf{y}} - n\bar{y}^2, \text{ and }\\
\mathrm{SSE} &= \symbf{y}'\symbf{y} - \hat{\symbf{y}}'\hat{\symbf{y}}.
\end{align*}

:::

::::

# Sums of squares: Multivariate regression {.smaller}

In multivariate regression, there is no universal definition of the sums of squares, but we can consider the following:

$$
\mathrm{SST}= \sum_{i=1}^n\sum_{j=1}^r(y_{ij} - \bar{y}_j)^2,
$$

which can be decomposed into

$$
\mathrm{SSR} + \mathrm{SSE} =
\sum_{i=1}^n\sum_{j=1}^r(\hat{y}_{ij} - \bar{y}_j)^2 + \sum_{i=1}^n\sum_{j=1}^r(y_{ij} - \hat{y}_{ij})^2,
$$

where SST, SSR and SSE are the sums of squares due to the total, regression and error, respectively.

# Assessment of the model: $R^2$ {.smaller}

:::: {.columns}

::: {.column width=50%}

## Multiple regression

The $R^2$ statistic is defined as

$$
R^2 = \frac{\mathrm{SSR}}{\mathrm{SST}} = 1 - \frac{\mathrm{SSE}}{\mathrm{SST}}.
$$

- $R^2$ is the proportion of the total variance explained by the model
- $0 \leq R^2 \leq 1$

:::

\pause

::: {.column width=50%}

## Multivariate regression

One can envision an appropriate multivariate extension to $R^2$, but it is not universally defined.

- One possibility is to consider the proportion of the total variance explained by the model for each response variable, and average this.

*Note*: we won't really use the sums of squares or $R^2$ for multivariate regression in this module.

:::

::::

# Multiple regression example {.smaller}

:::: {.columns}

::: {.column width=50%}

We model the sales price of a house against its size and assessed value.

We load the data as follows:

```{r}
#| echo: true
data(
  "data_tidy_house_price", 
  package = "DataTidy23RodoHonsMult"
)
```

```{r}
#| results: asis
data_tidy_house_price[1:4, ] |> knitr::kable()
```

:::

\pause

::: {.column width=50%}

We extract the response matrix, $\symbf{y}$:

```{r}
#| echo: true
y_vec <- data_tidy_house_price |> 
  dplyr::pull(selling_price)
```

We extract the data for the predictor matrix, $\symbf{X}$, and prepend a column of 1s:

```{r}
#| echo: true
X_mat <- cbind(
  1,
  data_tidy_house_price |>
    dplyr::select(-selling_price)
) |>
  as.matrix()
colnames(X_mat)[1] <- "intercept"
```

:::

::::


# Multiple regression example (cont.)

:::: {.columns}

::: {.column width=50%}

To fit the model, we first obtain the estimated regression coefficients:

```{r}
#| echo: true
beta_vec <- (
  solve(t(X_mat) %*% X_mat) %*%
  t(X_mat)
  ) %*%
  y_vec
colnames(beta_vec) <- "estimate"
```

```{r}
#| results: asis
beta_vec |> signif(2) |> knitr::kable()
```

:::

\pause

:::{.column width=50%}

This matches the output from the `lm` function:

```{r}
#| echo: true
#| results: asis
lm(
  selling_price ~ .,
  data = data_tidy_house_price
) |> 
  coef() |> signif(2) |> knitr::kable()
```

:::

::::



# Multiple regression example (cont.) {.smaller}

:::: {.columns}

::: {.column width=50%}

To assess the model, we calculate the $R^2$ statistic:

```{r}
#| echo: true
SST <- sum((y_vec - mean(y_vec))^2)
SSE <- (y_vec - X_mat %*% beta_vec)^2 |>
  sum()
SSR <- SST - SSE
R2 <- SSR / SST
R2 |> signif(2)
```

:::

\pause

::: {.column width=50%}

- An $R^2$ of 0.83 indicates that 83% of the variance in house prices is explained by the size and assessed value of the house.
- This is a high value, suggesting that the model explains the response "well".

:::

::::

# Multivariate example {.smaller}

Consider a dataset of four wood-pulp variables and four paper properties:

```{r}
#| echo: true
data(
  "data_tidy_paper", 
  package = "DataTidy23RodoHonsMult"
)
```

```{r}
#| results: asis
data_tidy_paper[1:4, ] |> knitr::kable()
```


# Multivariate example (cont.) {.smaller}

:::: {.columns}

::: {.column width=50%}

We extract the response matrix, $\symbf{Y}$:

```{r}
#| echo: true
Y_mat <- data_tidy_paper |> 
  dplyr::select(-starts_with("pulp")) |> 
  as.matrix()
```

```{r}
#| results: asis
Y_mat[1:4, ] |> signif(2) |> knitr::kable()
```

:::

\pause

::: {.column width=50%}

We extract the data for the predictor matrix, $\symbf{X}$, and prepend a column of 1s:

```{r}
#| echo: true
X_mat <- as.matrix(cbind(1,
  data_tidy_paper |>
    dplyr::select(starts_with("pulp"))
))
colnames(X_mat)[1] <- "intercept"
```

```{r}
#| results: asis
X_mat[1:3, , drop = FALSE] |> signif(2) |> knitr::kable()
```

:::

::::

# Multivariate example (cont.) {.smaller}

:::: {.columns}

::: {.column width=50%}

We first obtain the estimated regression coefficients:

```{r}
#| echo: true
B_mat <- (
  solve(t(X_mat) %*% X_mat) %*%
  t(X_mat)) %*% Y_mat
```

```{r}
#| results: asis
B_mat[, 1:3] |> signif(2) |> knitr::kable()
```

:::

\pause

::: {.column width=50%}

This matches the output from the `lm` function (exc. fourth column for space):

```{r}
#| echo: true
#| results: asis
lm(
  cbind(paper_1, paper_2, paper_3, paper_4) ~ .,
  data = data_tidy_paper
)$coefficients[, 1:3] |> signif(2) |> knitr::kable()
```

:::

::::

# Inference for multiple regression

:::: {.columns}

::: {.column width=50%}

- Up to now, we have not assumed a full distribution for the errors ($\symbf{\epsilon}$).
- To perform inference (calculate CI's and p-values), we make the additional assumption that the errors ($\symbf{\epsilon}$) are normally distributed, implying that

$$
\symbf{\epsilon} \sim \mathcal{N}(\symbf{0}, \sigma^2\symbf{I}).
$$

:::

\pause

:::{.column width=50%}

- This implies that

$$
\hat{\symbf{\beta}} \sim \mathcal{N}(\symbf{\beta}, \sigma^2(\symbf{X}'\symbf{X})^{-1}).
$$

- As well as that

$$
\frac{n-k}{\sigma^2}s^2 \sim \chi^2_{n-k}.
$$

- We note again that $\hat{\symbf{\beta}}$ and $s^2$ are independent.

:::

::::

# Confidence intervals for multiple regression {.smaller}

Using the previous results, a $100(1-\alpha)\%$ confidence ellipsoid for $\symbf{\beta}$ is given by

$$
\frac{
  (\symbf{\beta} - \hat{\symbf{\beta}})'
  \symbf{X}'\symbf{X}
  (\symbf{\beta} - \hat{\symbf{\beta}})
}{
  ks^2
}
\leq F_{k, n-k}^{1-\alpha},
$$

implying that a confidence interval for $\beta_j$ is

$$
\hat{\beta}_j \pm t_{n-k}^{\alpha/2}\sqrt{s^2(\symbf{X}'\symbf{X})^{-1}_{jj}}.
$$

# Hypothesis testing {.smaller}

:::: {.columns}

::: {.column width=50%}

## Univariate

- To test the null hypothesis $H_0: \beta_j = 0$, we use the test statistic

$$
t = \frac{\hat{\beta}_j}{\sqrt{s^2(\symbf{X}'\symbf{X})^{-1}_{jj}}}.
$$

- and reject at significance-level $\alpha$ if

$$
|t| > t_{n-k}^{1-\alpha/2}.
$$

:::

\pause

::: {.column width=50%}

## Simultaneous

- To test the null hypothesis $H_0: \symbf{\beta}^{(1)}=\symbf{0}$ where $\symbf{\beta}^{(1)}:q\times 1$ and $\symbf{\beta}=\begin{pmatrix} \symbf{\beta}^{(1)} & \symbf{\beta}^{(2)} \end{pmatrix}'$, we use

$$
F = \frac{\hat{\symbf{\beta}}^{(1)\prime}C_{11}^{-1}\hat{\symbf{\beta}}^{(1)}}{qs^2},
$$

- where $(\symbf{X}'\symbf{X})^{-1} = \symbf{C} = \begin{bmatrix} \symbf{C}_{11} & \symbf{C}_{12} \\ \symbf{C}_{21} & \symbf{C}_{22}  \end{bmatrix}$.

- Accordingly, we reject at significance-level $\alpha$ if

$$
F \geq F_{q, n-k}^{1-\alpha}.
$$

:::

::::

# Inference example {.smaller}

Consider the dataset of four wood-pulp variables and four paper properties:

```{r}
#| echo: true
data(
  "data_tidy_paper", 
  package = "DataTidy23RodoHonsMult"
)
```

We'll select only the first paper variable:

```{r}
#| results: asis
data_tidy_paper <- data_tidy_paper |>
  dplyr::select(matches("pulp|paper_1"))
data_tidy_paper[1:4, ] |> knitr::kable()
```

# Inference example (cont.) {.smaller}

:::: {.columns}

::: {.column width=50%}

We extract the response matrix, $\symbf{Y}$:

```{r}
#| echo: true
Y_mat <- data_tidy_paper[, "paper_1", drop = FALSE] |>
  as.matrix()
colnames(Y_mat) <- "paper_1"
```

```{r}
#| results: asis
Y_mat[1:4, ] |> signif(2) |> knitr::kable()
```

:::

\pause

::: {.column width=50%}

We extract the data for the predictor matrix, $\symbf{X}$, and prepend a column of 1s:

```{r}
#| echo: true
X_mat <- as.matrix(cbind(1,
  data_tidy_paper |>
    dplyr::select(starts_with("pulp"))
))
colnames(X_mat)[1] <- "intercept"
```

```{r}
#| results: asis
X_mat[1:3, , drop = FALSE] |> signif(2) |> knitr::kable()
```

:::

::::

# Inference example (cont.) {.smaller}

::::{.columns}

:::{.column width=50%}

## Estimates

We first estimate the coefficients:

```{r}
#| echo: true
beta_vec <- 
  (solve(t(X_mat) %*% X_mat) %*%
    t(X_mat)) %*% Y_mat
colnames(beta_vec) <- "estimate"
```

```{r}
#| results: asis
beta_vec |> signif(2) |> knitr::kable()
```

:::

:::{.column width=50%}

## $\beta_{\mathrm{pulp_1}}$ confidence interval

To get the 95% confidence interval for the $\beta_{\mathrm{pulp_1}}$ coefficient, we first get the standard error:

```{r}
#| echo: true
XtX_mat <- t(X_mat) %*% X_mat
XtX_mat_inv <- solve(XtX_mat)
var_resp <- sum((Y_mat - X_mat %*% beta_vec)^2) /
  (nrow(Y_mat) - ncol(X_mat))
sd_beta_pulp1 <- sqrt(var_resp) * sqrt(XtX_mat_inv[2, 2])
sd_beta_pulp1 |> signif(2)
```

:::

::::

# Inference example (cont.) {.smaller}

:::: {.columns}

::: {.column width=50%}

## $\beta_{\mathrm{pulp_1}}$ confidence interval (cont.)

We then get the relevant $t$-distributed quantile:

```{r}
#| echo: true
t_beta_pulp1 <- qt(0.975, nrow(Y_mat) - ncol(X_mat))
t_beta_pulp1 |> signif(2)
```

The confidence interval is then:

```{r}
#| echo: true
(beta_vec[2] + c(-1, 1) * t_beta_pulp1 * sd_beta_pulp1) |>
  signif(2)
```

:::

::: {.column width=50%}

## Hypothesis testing: $H_0: \beta_{\mathrm{pulp_1}} = 0$

The test statistic is given by

```{r}
#| echo: true
t_stat <- beta_vec[2] / sd_beta_pulp1
t_stat |> signif(2)
```

The p-value is therefore:

```{r}
#| echo: true
(pt(-abs(t_stat),
  nrow(Y_mat) - ncol(X_mat)) * 2
) |> signif(2)
```

:::

::::

# Inference example (cont.) {.smaller}

:::: {.columns}

::: {.column width=50%}

## Hypothesis testing: $H_0: \symbf{\beta}^{(1)} = \symbf{0}$

We now test the hypothesis that $\symbf{\beta}^{(1)} = \beta_1=\beta_3=\beta_4=0$.

We first extract $\symbf{C}_11$:

```{r}
#| echo: true
C11_mat <- XtX_mat_inv[2:5, 2:5]
C11_mat <- C11_mat[-2, -2]
C11_mat |> signif(2)
```

:::

::: {.column width=50%}

## Hypothesis testing: $H_0: \symbf{\beta}^{(1)} = \symbf{0}$

We now extract the relevant estimates:

```{r}
#| echo: true
beta_vec_1 <- beta_vec[
  c(2, 4:5), , drop = FALSE
]
beta_vec_1 |> signif()
```

:::

::::


# Inference example (cont.) {.smaller}

:::: {.columns}

::: {.column width=50%}

## Hypothesis testing: $H_0: \symbf{\beta}^{(1)} = \symbf{0}$

The test statistic is then:

```{r}
#| echo: true
f_stat <- (t(beta_vec_1) %*%
  solve(C11_mat) %*%
  beta_vec_1) / length(beta_vec_1) /
  var_resp
f_stat |> signif(2)
```

:::

:::{.column width=50%}

## Hypothesis testing: $H_0: \symbf{\beta}^{(1)} = \symbf{0}$

This yields the following p-value:

```{r}
#| echo: true
pf(f_stat, length(beta_vec_1),
  nrow(Y_mat) - ncol(X_mat),
  lower.tail = FALSE
) |> signif(2)
```

:::

::::

# Inference for multivariate regression {.smaller}

- Multiple and multivariate regression produce *exactly* the same inference when coefficients are examined within the context of a single response variable.
- For example, suppose that we model BMI and blood pressure against age, weight and height:
  - The coefficients, p-values and confidence intervals for age, weight and height will be the same in both multiple (fitting two models) and multivariate regression.
  - The simultaneous hypothesis tests will be the same when the considering only groups of coefficients iwth respect to a particular response variable. As an example, the p-value for the hypothesis that neither age nor weight has an effect on BMI will be the same in both models.
- The main difference of interest is that we can perform simultaneous hypothesis tests for regression coefficients across different response variables in multivariate regression.
  - To continue the example above, in multivariate regression we can test that the effects of age on BMI and blood pressure are both zero.

# Hypothesis testing in multivariate regression

:::: {.columns}

::: {.column width=50%}

Suppose we wish to test that

$$
\symbf{\mathcal{B}}^{(1)} = \symbf{0},
$$

where 

$$
\symbf{\mathcal{B}} = \begin{pmatrix} \symbf{\mathcal{B}}^{(1)} \\ \symbf{\mathcal{B}}^{(2)} \end{pmatrix}
$$

and $\symbf{\mathcal{B}}^{(1)}:q\times r$.

This is the hypothesis that the first $q$ predictors have no effect on any of the $r$ responses.

:::

\pause

::: {.column width=50%}

- We'll need to fit a model without the first $q$ predictors and compare the fit of the two models.
- Let's define $\symbf{X}^{(2)}:n\times (k-q) \text{ for } i = 1,2$ as follows:

$$
\symbf{X}= \begin{pmatrix} \symbf{X}^{(1)} & \symbf{X}^{(2)} \end{pmatrix}
$$

:::

::::

# Hypothesis testing in multivariate regression {.smaller}

:::: {.columns}

::: {.column width=50%}

- Under the implicit assumption that $\symbf{\mathcal{B}}^{(1)} = \symbf{0}$, we obtain the estimated regression parameters as

$$
\hat{\symbf{\mathcal{B}}}^{(2)} = (\symbf{X}^{(2)\prime}\symbf{X}^{(2)})^{-1}\symbf{X}^{(2)\prime}\symbf{\mathcal{Y}},
$$

- and the MLE variance-covariance matrices as

$$
\hat{\Sigma}_{\mathrm{MLE}} = \frac{1}{n}(\symbf{\mathcal{Y}} - \symbf{X}\hat{\symbf{\mathcal{B}}})'(\symbf{\mathcal{Y}} - \symbf{X}\hat{\symbf{\mathcal{B}}}).
$$

- and

$$
\hat{\symbf{\Sigma}}^{(2)}_{\mathrm{MLE}} = \frac{1}{n}(\symbf{\mathcal{Y}} - \symbf{X}^{(2)}\hat{\symbf{\mathcal{B}}}^{(2)})'(\symbf{\mathcal{Y}} - \symbf{X}^{(2)}\hat{\symbf{\mathcal{B}}}^{(2)}).
$$

:::

\pause

::: {.column width=50%}

- The test statistic is then

\begin{align*}
\Lambda &= \frac{
  \displaystyle\max_{\symbf{\mathcal{B}}^{(2)},\symbf{\Sigma^{(2)}}}
  L(\symbf{\mathcal{B}}^{(2)},\symbf{\Sigma^{(2)}})
}{
  \displaystyle\max_{\symbf{\mathcal{B}},\symbf{\Sigma}}
  L(\symbf{\mathcal{B}},\symbf{\Sigma})
}, \\
&= \left(\frac{|\hat{\symbf{\Sigma}}|}{|\hat{\symbf{\Sigma^{(2)}}}|}\right)^{n/2}.
\end{align*}


- For $n-r$ and $n-k$ sufficiently large, the following approximation holds:

$$
-[n-k-0.5(r-p+q+1)]\ln(\Lambda^{2/n})\sim \chi^2_{r(p-1)}.
$$

:::

::::

# Multivariate inference example

Using the paper dataset, we will test whether pulp_2 or pulp_3 have an effect on any paper type.

Loading the data as follows:

```{r}
#| echo: true
data(
  "data_tidy_paper", 
  package = "DataTidy23RodoHonsMult"
)
```

```{r}
#| results: asis
data_tidy_paper[1:4, ] |> knitr::kable()
```

# Multivariate example (cont.) {.smaller}

:::: {.columns}

::: {.column width=50%}

We extract the response matrix, $\symbf{Y}$:

```{r}
#| echo: true
Y_mat <- data_tidy_paper |> 
  dplyr::select(-starts_with("pulp")) |> 
  as.matrix()
```

```{r}
#| results: asis
Y_mat[1:4, ] |> signif(2) |> knitr::kable()
```

:::

\pause

::: {.column width=50%}

We extract the data for the predictor matrix, $\symbf{X}$, and prepend a column of 1s:

```{r}
#| echo: true
X_mat <- as.matrix(cbind(1,
  data_tidy_paper |>
    dplyr::select(starts_with("pulp"))
))
colnames(X_mat)[1] <- "intercept"
```

```{r}
#| results: asis
X_mat[1:3, , drop = FALSE] |> signif(2) |> knitr::kable()
```

:::

::::

# Multivariate example (cont.) {.smaller}

:::: {.columns}

::: {.column width=50%}

We extract the data for the predictor matrix excluding pulp_2 and pulp_3, $\symbf{X}^{(2)}$:

```{r}
#| echo: true
X_mat_2 <- X_mat[
  , -(3:4), drop = FALSE
]
```

```{r}
#| results: asis
X_mat_2[1:3, , drop = FALSE] |> signif(2) |> knitr::kable()
```

:::

::: {.column width=50%}

We obtain estimated regression coefficients under both the full and reduced models:

```{r}
#| echo: true
B_mat <- (
  solve(t(X_mat) %*% X_mat) %*%
  t(X_mat)) %*% Y_mat
```

```{r}
#| echo: true
B_mat_2 <- (
  solve(t(X_mat_2) %*% X_mat_2) %*%
  t(X_mat_2)) %*% Y_mat
```

:::

::::

# Multivariate example (cont.) {.smaller}

:::: {.columns}

::: {.column width=50%}

We now calculate the sample variance-covariance matrices under the full and reduced models:

```{r}
#| echo: true
Sigma_mat <-
  t((Y_mat - X_mat %*% B_mat)) %*%
    (Y_mat - X_mat %*% B_mat) /
    (nrow(Y_mat))
```

```{r}
#| echo: true
Sigma_mat_2 <-
  t((Y_mat - X_mat_2 %*% B_mat_2)) %*%
    (Y_mat - X_mat_2 %*% B_mat_2) /
    (nrow(Y_mat))
```

:::

::: {.column width=50%}

- This yields the following test statistic:

```{r}
#| echo: true
n <- nrow(Y_mat)
r <- ncol(Y_mat)
q <- ncol(X_mat) - ncol(X_mat_2)
k <- ncol(X_mat)
p <- k - 1
test_stat <-
  -(n - k - 0.5 * (r - p + q + 1)) *
  log(
    det(Sigma_mat) / det(Sigma_mat_2)
  )
test_stat |> signif(2)
```

with p-value

```{r}
#| echo: true
pchisq(test_stat, r * (p - q), lower.tail = FALSE) |>
  signif(2)
```

:::

::::

# Principal components regression (PCR)

- In PCR, we replace the original $p$ (non-intercept) predictors with $m \leq p$ principal components.
- Motivations:
  - Avoid multicollinearity
  - Reduce overfitting
  - Use fewer predictors
  - Improve interpretability

# Method

Very straigthtforward...

1. Calculate the principal components of the (non-intercept) predictors.
2. Fit a multiple regression model using the first $m$ principal components as predictors.

\pause

If the number of principal components is pre-specified, then all the inferential methods of multiple regression apply.


# Relationship with ridge regression

- PCR is akin to a rough version of ridge regression [@hastie_etal09]:

\begin{figure}
\centering
\includegraphics[width=0.75\textwidth]{_data_raw/img/pcr_vs_ridge.png}
\end{figure}

# Selecting the number of principal components

- One can use the same reasoning as before (scree plots, proportion of variation captured), or cross-validation (or $R^2$, but this isn't really done).
- Note that when performing cross validation, the loadings must be computed afresh within each fold.

# Partial least squares (PLS)

- Partial least squares (PLS) is a method that is similar to PCR, in that it takes linear combinations of the input variables.
- However, instead of choosing the linear combinations to maximise the variance, it chooses them to maximise (something like) the covariance of the resultant variables with the predictor.

# Partial least squares algorithm [@hastie_etal09]

\begin{figure}
\centering
\includegraphics[width=0.75\textwidth]{_data_raw/img/pls_alg.png}
\end{figure}

# PLS versus PCR [@hastie_etal09]

- PCR and PLS optimise different objective functions:

\begin{figure}
\centering
\includegraphics[width=0.75\textwidth]{_data_raw/img/pcr_vs_pls.png}
\end{figure}

# Selecting the number of components in PCR and PLS [@hastie_etal09]

- The number of components can be chosen by cross-validation.
- @hastie_etal09 chose the number of components within one standard error of the minimum cross-validated error:

\begin{figure}
\centering
\includegraphics[width=0.75\textwidth]{_data_raw/img/cv-pcr_and_pls.png}
\end{figure}

# Resources

- Source code is on [GitHub](https://github.com/MiguelRodo/Slides23RodoHonsMult/blob/2024/SlidesHonsMultL8Reg.qmd).
  - Suppressed code (e.g. to create figures) is available there.

## References
---
title: Factor analysis
subtitle: The orthogonal factor model
author: Miguel Rodo (re-using slides from Francesca Little)
date: "2024-04-18"
bibliography: zotero.bib
format:
  beamer:
    embed-resources: true
    aspectratio: 169
    urlcolor: cyan
    linkcolor: blue
    filecolor: magenta
    include-in-header:
      file: preamble-reg.tex
nocite: |
    @johnson_wichern07
---

```{r}
#| include: false
#| # Load necessary libraries
library(ggplot2)
library(tibble)
```

# Factor analysis (FA)

- Context: $\symbf{X} \sim \; ?$
  - Goal: Explain covariance structure of $\symbf{X}$ in terms of a smaller number of unobserved variables

# Economic example of factor analysis

Suppose an economist wants to understand the factors that influence a country's economic growth. They might collect data on various macroeconomic indicators such as:

- **Gross Domestic Product (GDP)**: A measure of the economic output of a country.
- **Inflation Rate**: The rate at which the general level of prices for goods and services is rising.
- **Unemployment Rate**: The percentage of the labor force that is jobless and actively looking for employment.
- **Interest Rates**: The amount charged by lenders to borrowers for the use of assets, expressed as a percentage of the principal.
- **Foreign Direct Investment (FDI)**: Investment made by a firm or individual in one country into business interests located in another country.

# Average levels of economic variables versus factors (cont.)

```{r}
col_from_var_eco <- c(
  "GDP" = "#7fc97f",
  "Interest Rate" = "#beaed4",
  "Unemployment Rate" = "#fdc086",
  "Inflation Rate" = "#f0027f",
  "FDI" = "#386cb0"
)
# Reshape the data for plotting
var_vec <- names(col_from_var_eco)
x_vec <- rep(c(-2, 2), length(var_vec))
y_vec <- c(
  -2, 2, # GDP
  -0.1, 0.1, # Interest Rate
  0.75, -0.75, # Unemployment Rate
  0.2, -0.2, # Inflation Rate
  -1.5, 1.5 # FDI
)
df_long_f1 <- tibble::tibble(
  var = rep(var_vec, each = 2),
  x = x_vec,
  y = y_vec,
  factor = "f1"
)
# now the following tibble
# has inflation and interest
# increasing heavily with factor f2,,
# while GDP and FDI decrease slightly with f2
# and unemployment rate also decreases slightly
y_vec <- c(
  0.25, -0.25, # GDP
  -1.5, 1.5, # Interest Rate
  -0.5, 0.5, # Unemployment Rate
   2.2, -2.25, # Inflation Rate
  -0.3, 0.3 # FDI
)
df_long_f2 <- tibble::tibble(
  var = rep(var_vec, each = 2),
  x = x_vec,
  y = y_vec,
  factor = "f2"
)
df_long <- df_long_f1 |>
  dplyr::bind_rows(df_long_f2)

# Plotting
p <- ggplot(
  df_long,
  aes(x = x, y = y, colour = var)
) +
  geom_hline(yintercept = 0, colour = "gray75") +
  geom_vline(xintercept = 0, colour = "gray75") +
  geom_line(linewidth = 2) +
  facet_wrap(
    ~factor, scales = "free", ncol = 2,
    label = labeller(factor = c(f1 = "Factor 1", f2 = "Factor 2"))
    ) +
  cowplot::theme_cowplot() +
  cowplot::background_grid(major = "xy") +
  labs(
    x = "Factor score", y = "Standardized measurement"
  ) +
  scale_colour_manual(
    values = col_from_var_eco
  ) +
  theme(
    legend.position = "bottom"
  ) +
  theme(
    strip.background = element_rect(fill = "white"),
    strip.text = element_text(face = "bold"),
    legend.title = element_blank()
  )
path_p <- projr::projr_path_get(
  "cache", "fig", "p-fa-economic-variables-vs-factors.pdf"
)
cowplot::ggsave2(
  path_p,
  p,
  width = 18,
  height = 10,
  units = "cm"
)
```

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{_tmp/fig/p-fa-economic-variables-vs-factors.pdf}
\end{figure}

# Distribution of GDP given factors

```{r}
x_vec <- seq(-3, 3, length.out = 1e2)
y_vec <- dnorm(x_vec, mean = 0, sd = 1)
plot_tbl <- purrr::map_df(c(1, -1), function(f_level) {
  purrr::map_df(c("f1", "f2"), function(f_name) {
    slope <- switch(
      f_name,
      f1 = 1,
      f2 = - 1 / 6
    )
    x_vec_curr <- x_vec + f_level * slope
    tibble::tibble(
      x = x_vec_curr,
      y = y_vec,
      factor = f_name,
      score = f_level |> signif(2) |> as.character()
    )
  })
})
p <- ggplot(
  plot_tbl,
  aes(x = x, y = y, colour = score, fill = score)
) +
  geom_vline(xintercept = 0, colour = "gray60") +
  geom_polygon(linewidth = 1, alpha = 0.25) +
  facet_wrap(
    ~factor, scales = "fixed", ncol = 2,
    labeller = labeller(factor = c(f1 = "Factor 1", f2 = "Factor 2"))
  ) +
  cowplot::theme_cowplot() +
  cowplot::background_grid(major = "xy") +
  labs(
    x = "GDP", y = "Density"
  ) +
  scale_colour_manual(
    values = c("1" = "dodgerblue", "-1" = "mediumseagreen"),
    name = "Factor score"
  ) +
  scale_fill_manual(
    values = c("1" = "dodgerblue", "-1" = "mediumseagreen"),
    name = "Factor score",
    guide = "none"
  ) +
  theme(
    legend.position = "bottom"
  ) +
  theme(
    strip.background = element_rect(fill = "white"),
    strip.text = element_text(face = "bold")
  )

path_p <- projr::projr_path_get(
  "cache", "fig", "p-fa-gdp-distribution-given-factors.pdf"
)
cowplot::ggsave2(
  path_p,
  p,
  width = 18,
  height = 10,
  units = "cm"
)
```

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{_tmp/fig/p-fa-gdp-distribution-given-factors.pdf}
\end{figure}

- $\mathrm{E}[X_{\mathrm{GDP}}]=f_1 - \frac{1}{6}f_2$

# Sources of randomness {.smaller}

::::{.columns}

:::{.column width="50%"}

## Error terms

- Given the factor scores for a given observation, the observed variables are not determined - there is some variation around that value.
- For example, in the previous slide GDP had a distribution given the factors.
- So, two countries with the same levels for factors 1 and 2 might have different GDPs.

:::

:::{.column width="50%"}

## Factors

- Factors are random variables (albeit unobserved), with a new draw for each observation.
- For example, one country will randomly have factor scores of 1 and -1.5, whereas another would have factor scores of -0.5 and -1.2.

:::

::::

\pause

- Both error terms and factors typically have a mean of zero, but different approaches to factor analysis will place different assumptions regarding their variances and, especially, their covariances and distributional shape.

# Other examples

- **Psychological Assessments**
  - *Observed*: Responses to psychological questionnaires.
  - *Hidden*: Personality traits (e.g., extraversion, neuroticism).

- **Consumer Behavior**
  - *Observed*: Shopping patterns, demographic data.
  - *Hidden*: Purchase drivers (e.g., parental status, age, income).

- **Medical Diagnosis**
  - *Observed*: Transcriptomic profiles.
  - *Hidden*: Disease states (e.g., cured, infected, advanced).

# The orthogonal factor model (OFM)

- The simplest factor model is the **orthogonal factor model**.

\pause

- We'll discuss the following:
  - Assumptions
  - Model specification
  - Methods for estimation
    - Principal component method
    - Maximum likelihood
  - Factor rotation
  - Factor scores

# Assumptions

::::{.columns}

:::{.column width="50%"}

## Linearity

- The relationship between the observed variables and the factors is linear.
- For example, the relationship between GDP and the factors is linear (e.g., $\mathrm{E}[X_{\mathrm{GDP}}]=f_1 - \frac{1}{6}f_2$).

:::

\pause

:::{.column width="50%"}

## Independence

- The factors are uncorrelated with each other.
  - For example, the first factor being being high does not imply the second factor is (more likely to be) high.
- The factors are uncorrelated with the error terms.
  - For example, the first factor being high does not imply the error (change from the mean) for GDP is high.

:::

::::

# The orthogonal factor model

::::{.columns}

:::{.column width="50%"}
Let 

- $\symbf{X}:p \times 1$ be the random vector of observed variables, 
- $\symbf{F}:m\times 1$ be the random vector of unobserved factors,
- $\symbf{\epsilon}:p\times 1$ be the random vector of unobserved error terms, and
- $\symbf{L}:p\times m$ be a matrix of constants,

satisfying

$$
\symbf{X}-\symbf{\mu} = \symbf{L}\symbf{F} + \symbf{\epsilon},
$$

:::

:::{.column width="50%"}

where

- $\symbf{\mu}$ is the mean of $\symbf{X}$,
- $\mathrm{E}[\symbf{F}]= \symbf{0}$,
- $\mathrm{Var}[\symbf{F}]= \symbf{I}$,
- $\mathrm{E}[\symbf{\epsilon}]= \symbf{0}$,
- $\mathrm{Var}[\symbf{\epsilon}]= \symbf{\Psi}$, a diagonal matrix, and
- $\mathrm{Cov}[\symbf{F}, \symbf{\epsilon}]= \symbf{0}$.

:::

::::

# Example

::::{.columns}

:::{.column width="50%"}

- Let $\symbf{X} = \begin{bmatrix} X_1 \\ X_2 \end{bmatrix}$, $\symbf{F} = \begin{bmatrix} F_1 \\ F_2 \end{bmatrix}$, and $\symbf{\epsilon} = \begin{bmatrix} \epsilon_1 \\ \epsilon_2 \end{bmatrix}$.
- Then, the model can be written as

$$
\begin{bmatrix} X_1 \\ X_2 \end{bmatrix} - \begin{bmatrix} \mu_1 \\ \mu_2 \end{bmatrix} = \begin{bmatrix} l_{11} & l_{12} \\ l_{21} & l_{22} \end{bmatrix} \begin{bmatrix} F_1 \\ F_2 \end{bmatrix} + \begin{bmatrix} \epsilon_1 \\ \epsilon_2 \end{bmatrix}.
$$

:::

:::{.column width="50%"}

- For $X_1$, this implies that

$$
\begin{aligned}
X_1 - \mu_1 &= l_{11}F_1 + l_{12}F_2 + \epsilon_1 \\
X_1 &= \mu_1 + l_{11}F_1 + l_{12}F_2 + \epsilon_1.
\end{aligned}
$$

- Regarding the covariances, for example we have that

$$
\mathrm{Cov}[F_1, F_2] = 0,
$$

- and that 

$$
\mathrm{Cov}[F_1, \epsilon_1] = 0.
$$

:::

::::

# Example (cont.) {.smaller}

::::{.columns}

:::{.column width="50%"}

- We have that 

$$
\mathrm{Var}[F_1] = \mathrm{Var}[F_2] = 1,
$$

- and that 

$$
\mathrm{Var}[\epsilon_1]=\psi_1, \quad \mathrm{Var}[\epsilon_2]=\psi_2.
$$

- As such, the factors have unit variance, whereas the errors have variable-dependent variance.

:::
:::{.column width="50%"}


- The factor scores are therefore unit-free, indicating the direction and magnitude in a standardised manner.
  - For example, in the economic example a large positive second first factor level indicates above-average economic activity.
- The errors therefore help provide the correct "scaling".
  - For example, interest rates and GDP will have different variabilities (without standardisation), which will (partially) be accounted for by differing error term variances.

:::

::::

# Terminal terminology {.smaller}

- $F_j$: $j$-th common factor
  - "Common" in the sense of contributing to all the observed variables
- $\epsilon_i$: $i$-th specific factor
  - "Specific" in the sense of contributing to only one observed variable
- $l_{ij}$: loading of $i$-th variable on $j$-th common factor
  - The extent to which the $j$-th factor influences the $i$-th observed variable

# Source of correlation between observed variables

::::{.columns}

:::{.column width="50%"}

- In the OFM, the error terms are uncorrelated, the factor terms are uncorrelated and the factors are uncorrelated with the error terms.
- How do we then get correlation between the observed variables?
\pause
- The same factor contributing to two observed variables will induce correlation between them.

:::

\pause

:::{.column width="50%"}

- For example, if there is only one factor and so $\mathrm{E}[X_1]=l_{11}F_1$ and $\mathrm{E}[X_2]=l_{21}F_1$, then as long as $l_{11}$ and $l_{21}$ are both non-zero, then $X_1$ and $X_2$ will be correlated.
- This is the essence of factor analysis: to explain the correlation between observed variables in terms of a smaller number of unobserved factors.
- We'll now get a formula for the covariance between observed variables in terms of the factor loadings.

:::
::::

# Covariance between observed variables {.smaller}

::::{.columns}

:::{.column width="50%"}

- The OFM model specification implies the following formula for the covariance between the observed variables (proofs at end of the slides):

$$
\mathrm{Cov}[\symbf{X}]= \symbf{L}\symbf{L}' + \symbf{\Psi}
$$

- For a given observed variable, we have that

$$
\mathrm{Var}[\symbf{X}_i]= \sum_{j=1}^ml_{im}^2 + \psi_ii.
$$


:::

:::{.column width="50%"}

- For a pair of observed variables, we have that

$$
\mathrm{Cov}[X_i, X_j]= \sum_{k=1}^ml_{ik}l_{jk}.
$$

- The diagonal elements are the specific variances, and "inflate" the variances of the observed variables (over and above that implied by the loadings).
- As $\symbf{\psi}$ is diagonal, the off-diagonal elements of the covariance matrix are determined by the factor loadings.
:::

::::

# Accuracy of implied covariance matrix {.smaller}

::::{.columns}

:::{.column width="50%"}

- The covariance matrix between the observed variables has $p(p+1)/2$ unique elements, and is typically full rank.
- The factor model implies that the covariance matrix is equal to

$$
\symbf{L}\symbf{L}' + \symbf{\Psi}.
$$

- Whilst $\Psi$ is full rank, $\symbf{L}\symbf{L}'$ is rank $m\leq p$ and is fully responsible for the off-diagonal covariances.

:::

:::{.column width="50%"}

- We can always perfectly reproduce the diagonal elements of the covariance matrix when $m=p$ (as many factors as observed variables) by setting

$$
\symbf{L}=\symbf{V}\symbf{D}/\sqrt{n-1}, 
$$

- where $\symbf{X}=\symbf{U}\symbf{D}\symbf{V}'$.
- However, we almost always want fewer factors than observed variables, and so we will not be able to perfectly reproduce the off-diagonal elements of the covariance matrix.
- In practice, no problem - an approximation is fine.

:::

::::


# Covariance between observed and unobserved variables {.smaller}

::::{.columns}

:::{.column width="50%"}

- By the assumptions of the OFM, we have that

$$
\mathrm{Cov}[\symbf{X}, \symbf{F}]=\symbf{L}.
$$

- For a specific observed variable and factor, we have that

$$
\mathrm{Cov}[X_i, F_j]=l_{ij}.
$$


:::

:::{.column width="50%"}

- As such, the covariance between the observed variables and the factors is determined by the factor loadings.

:::

::::

# Solutions are not unique

::::{.columns}

:::{.column width="50%"}

- The solution to an OFM model consists of the loadings matrix $\symbf{L}$ and the error variances $\symbf{\Psi}$.
- Two solutions are equivalent if a) meet the assumptions of the OFM and b) imply same ovariance matrix and specific variances.
- In the OFM model, an orthonormal rotation of the loading matrix can yield the same covariance matrix and specific variances.

:::

\pause

:::{.column width="50%"}

- Suppose that $\symbf{T}$ is an $m\times m$ orthonormal matrix, i.e. $\symbf{T}\symbf{T}'=\symbf{I}_m$.

- Then, if $\symbf{L}$ is a solution to the OFM, so is $\symbf{L}^{*}=\symbf{L}\symbf{T}$.

- In other words, the solutions $\symbf{L}$

$$
\symbf{X}=\symbf{L}\symbf{F}+\symbf{\epsilon}
$$
 
- and $\symbf{LT}= \symbf{L}^{*}$

$$
\symbf{X}=(\symbf{L}\symbf{T})(\symbf{T}'\symbf{F})+\symbf{\epsilon}=\symbf{L}^{*}\symbf{F}^{*}+\symbf{\epsilon}
$$

- are equivalent.

:::

::::

# Demonstrating non-uniqueness {.smaller}

::::{.columns}

:::{.column width="50%"}

- Firstly, we show that the mean and variance assumptions are met for the common factors:
$$
\begin{aligned}
&\mathrm{E}[\symbf{F}^*]=\mathrm{E}[\symbf{T}'\symbf{F}]=\symbf{T}'\mathrm{E}[\symbf{F}]=\mathrm{0}, \text{ and } \\
&\mathrm{Cov}[\symbf{F}^*]=\mathrm{T}'\mathrm{Cov}[\symbf{F}]\mathrm{T}=\mathrm{T}'\mathrm{T}=\mathrm{I}.
\end{aligned}
$$

- Secondly, we show that the specific factors are unchanged:
$$
\begin{aligned}
\symbf{X} - \symbf{\mu} &= \symbf{L}\symbf{F} + \symbf{\epsilon} \\
&= \symbf{L}\symbf{T}\symbf{T}'\symbf{F} + \symbf{\epsilon} \text{ (as } \symbf{TT}'=\symbf{I} \text{), and}\\
&= \symbf{L}^*\symbf{F}^* + \symbf{\epsilon}, \\
\end{aligned}
$$
where $\symbf{L}^*=\symbf{L}\symbf{T}$ and $\symbf{F}^*=\symbf{T}'\symbf{F}$.



:::

:::{.column width="50%"}

- Thirdly, we show that the estimated covariance matrix also does not change:
$$
\begin{aligned}
\hat{\mathrm{Var}}[\symbf{X}] &= \symbf{LL}' + \symbf{\Psi}, \\
&=  \symbf{L}\symbf{T}\symbf{T}'\symbf{L}'+\hat{\symbf{\Psi}}, \\ 
&= \symbf{L}^*\symbf{L}^{*\prime} + \symbf{\Psi}.
\end{aligned}
$$

:::

::::

# Exploiting the lack of uniqueness

::::{.columns}

:::{.column width="50%"}

- Since the solution to the OFM is not unique, we can find initial values for $\hat{\mathbf{L}}$ and then rotate them to find a solution that is more interpretable.

:::

:::{.column width="50%"}

- By "interpretable", we mean one where each factor contributes prominently to a minority of the variables, e.g. factor 1 contributes strongly only to variables 1 and 2, factor 2 contributes strongly only to variables 2 and 5, etc.

:::

::::

\pause

\vspace{1cm}

- In the next couple of slides, we'll consider two approaches for fitting the factor analysis model to a given dataset, and then we'll consider rotating those initial solutions.

# First method of estimation: Principal component method I

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{_data_raw/img/francesca/fit-pc-short.png}
\end{figure}

# First method of estimation: Principal component method II

\begin{figure}
\centering
\includegraphics[width=0.9\textwidth]{_data_raw/img/francesca/fit-pc-2.png}
\end{figure}

# Example 1: Fitting using PC method I

\begin{figure}
\centering
\includegraphics[width=\textwidth]{_data_raw/img/francesca/fit-pc-example.png}
\end{figure}

# Example 1: Fitting using PC method II

\begin{figure}
\centering
\includegraphics[width=0.95\textwidth]{_data_raw/img/francesca/fit-pc-example_2.png}
\end{figure}

# Example 1: Fitting using PC method III

\begin{figure}
\centering
\includegraphics[width=0.85\textwidth]{_data_raw/img/francesca/fit-pc-example_3.png}
\end{figure}

# Example 2: Fitting using PC method I

\begin{figure}
\centering
\includegraphics[width=0.85\textwidth]{_data_raw/img/francesca/fit-example2-1.png}
\end{figure}

# Example 2: Fitting using PC method II

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{_data_raw/img/francesca/fit-pc-example2-2.png}
\end{figure}

# Second method of estimation: Maximum likelihood

\begin{figure}
\centering
\includegraphics[width=0.85\textwidth]{_data_raw/img/francesca/fit-ml-1.png}
\end{figure}

# Example 3: Fitting using maximum likelihood I

\begin{figure}
\centering
\includegraphics[width=0.85\textwidth]{_data_raw/img/francesca/fit-ml-2.png}
\end{figure}

# Revisiting rotation

\begin{figure}
\centering
\includegraphics[width=0.85\textwidth]{_data_raw/img/francesca/rotation-2.png}
\end{figure}

# Factor scores

\begin{figure}
\centering
\includegraphics[width=0.9\textwidth]{_data_raw/img/francesca/factor-scores.png}
\end{figure}

# Example 4: Factor scores

\begin{figure}
\centering
\includegraphics[width=0.875\textwidth]{_data_raw/img/francesca/factor-scores-example.png}
\end{figure}

# Take-aways I

- Conceptual understanding of factor analysis' relationship to PCA and multivariate regression
- Assumed mean and covariance structure of an orthogonal factor model
  - Implied form for covariance matrix
  - Communality and specific factors
  - Correlation between observed and unobserved variables
- Two methods of fitting the OFM to a given dataset
  - Principal component method (first principles)
  - Maximum likelihood (`factanal` package)

# Take-aways II

- Factor rotations
  - Theory behind why they're permitted
  - Able to justify why they may be useful
- Interpretation of results of factor analysis
- Calculation of factor scores
  - From first principles


# Resources

- Source code is on [GitHub](https://github.com/MiguelRodo/Slides23RodoHonsMult/blob/2024/SlidesHonsMultL9FA.qmd).
  - Suppressed code (e.g. to create figures) is available there.

## References


