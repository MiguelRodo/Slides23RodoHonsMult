---
title: Least-squares regression
subtitle: Multiple, multivariate, PCR and PLS approaches
author: Miguel Rodo (reworking slides by Stefan Britz)
date: "2024-04-11"
bibliography: zotero.bib
format:
  beamer:
    embed-resources: true
    aspectratio: 169
    urlcolor: cyan
    linkcolor: blue
    filecolor: magenta
    include-in-header:
      file: preamble-reg.tex
nocite: |
    @johnson_wichern07
---

# Linear regression

- Simple regresssion: one predictor, one response
  - Typical notation: $Y = \beta_0 + \beta_1 X + \epsilon$
  - Example: Ice-cream sales against temperature
\pause
- Multiple regression: multiple predictors, one response
  - Typical notation: $Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_p X_p + \epsilon$, OR $Y = \symbf{X}'\symbf{\beta} + \epsilon$
  - Example: House price against the number of rooms, the size of the garden, the distance to the city center, etc.
\pause
- Multivariate regression: multiple predictors, multiple responses
  - Typical notation: $\symbf{\mathcal{Y}} = \symbf{X}'\symbf{\mathcal{B}} + \symbf{\mathcal{E}}$, where $\symbf{\mathcal{Y}}$, $\symbf{\mathcal{B}}$ and $\symbf{\mathcal{E}}$ have $r$ columns
  - Example: Rainfall and wind speed against hurricane category, altitude and distance from the coast

# Transformations of input variables

- We've often transformed input variables individually, e.g. squaring them or taking the logarithm
  - For example, if we have a predictor $X$, we can use $X^2$ as a predictor and have $Y = \beta_0 + \beta_1 X^2 + \epsilon$
\pause
- We will also now consider taking *linear combinations* of different input variables, e.g. creating the variable $X_1^{\ast}=\alpha_1X_1+\alpha_2X_2$ and using it as a predictor in the model
  - We consider two ways of deciding on the linear combinations:
    - Principal components analysis: maximising variance
    - Partial least squares: maximising association with the response

# A noteworthy note on notable notation

- Number of...
  - Observations: $n$
  - Responses: $r$, where $r=1$ for multiple regression ($\symbf{y}:n\times 1$) but $r\geq 1$ for multivariate regression ($\symbf{Y}:n\times r$)
  - Predictors: $k$, i.e. $\symbf{\beta}:k\times 1$ and $\symbf{\mathcal{B}}:k\times r$
  - Predictors, apart from the intercept: $p=k-1$, i.e. $\symbf{x}'\symbf{\beta} = \beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p$

# Observational unit

:::: {.columns}

::: {.column width=50%}

## Multiple regression

- Each observational unit consists of a single response variable ($y_i$) and multiple predictors ($\symbf{x}_i= \begin{bmatrix} x_{i1} & x_{i2} & \ldots & x_{ip} \end{bmatrix}'$)
- Example responses per observational unit:
  - Annual revenue of a company
  - Number of pigeons at a single university
  - Blood pressure of an individual
:::

\pause

::: {.column width=50%}

## Multivariate regression

- Each observational unit consists of multiple response variables ($\symbf{y}_i= \begin{bmatrix} y_{i1} &y_{i2} & \ldots & y_{ir} \end{bmatrix}'$) and multiple predictors ($\symbf{x}_i= \begin{bmatrix} x_{i1} & x_{i2} & \ldots & x_{ip} \end{bmatrix}'$)
- Example responses per observational unit:
  - Stock prices of a single company at multiple timepoints
  - Number of pigeons and starlings at a single university
  - Measurements of 1000 genes of an individual
:::

::::

# Model assumptions {.smaller}

:::: {.columns}

::: {.column width=50%}

## Multiple regression

- **Linearity**: $\mathrm{E}[Y] = \beta_0 + \beta_1 X+1 + \ldots + \beta_p X_p$
- **Constant variance**: $\mathrm{Var}[\epsilon] = \sigma^2$
- **Independence**: $\mathrm{Cov}[\epsilon_i, \epsilon_j] = 0 \text{ for } i \neq j$
  - Different observations entirely independent
  - For example, different individuals in a study have independent BMI measurements

:::

\pause

::: {.column width=50%}

## Multivariate regression

- **Linearity**: $\mathrm{E}[\symbf{\mathcal{Y}}] = \symbf{X}'\symbf{\mathcal{B}}$
- **Constant variance**: $\mathrm{Var}[\symbf{\mathcal{E}}] = \symbf{\Sigma}$
- **Independence**: $\mathrm{Cov}[\symbf{\mathcal{E}}_i, \symbf{\mathcal{E}}_j] = \symbf{0} \text{ for } i \neq j$
  - Different observational units are independent, but different responses within the same observational unit may be correlated
  - For example, different individuals have independent BMI measurements, but multiple measurements of BMI from the same individual are clearly correlated

:::

::::

# Estimating the regression coefficients

:::: {.columns}

::: {.column width=50%}

## Multiple regression

Choose $\hat{\symbf{\beta}}$ to minimise the sum of squared residuals:

\begin{align*}
 &\sum_{i=1}^n (y_i - \hat{y}_i)^2 \\
=\; &(\symbf{y} - \hat{\symbf{y}})'(\symbf{y} - \hat{\symbf{y}}), \\
=\; &(\symbf{y} - \symbf{X}\hat{\symbf{\beta}})'(\symbf{y} - \symbf{X}\hat{\symbf{\beta}}).
\end{align*}

This implies that

$$
\hat{\symbf{\beta}} = (\symbf{X}'\symbf{X})^{-1}\symbf{X}'\symbf{y}.
$$

:::

\pause

::: {.column width=50%}

## Multivariate regression

Choose $\hat{\symbf{\mathcal{B}}}$ to minimise the sum of squared residuals:

\begin{align*}
 &\sum_{i=1}^n\sum_{j=1}^r (y_{ij} - \hat{y}_{ij})^2 \\
=\; &\mathrm{tr}[(\symbf{\mathcal{Y}} - \symbf{X}\hat{\symbf{\mathcal{B}}})'(\symbf{\mathcal{Y}} - \symbf{X}\hat{\symbf{\mathcal{B}}})].
\end{align*}

This implies that 

$$
\hat{\symbf{\mathcal{B}}} = (\symbf{X}'\symbf{X})^{-1}\symbf{X}'\symbf{\symbf{Y}}.
$$

:::

::::

# Fitted values

- The fitted values are the values of the response variable predicted by the model:
  - **Multiple regression**: $\hat{y}_i = \symbf{x}_i'\hat{\symbf{\beta}}$
  - **Multivariate regression**: $\hat{\symbf{\mathcal{Y}}}_{[i]} = \symbf{x}_i'\hat{\symbf{\mathcal{B}}}$
    - By convention, we use square brackets (e.g. $\symbf{A}_{[i]}$) to denote the $i$-th row of a matrix
\pause

# The hat matrix {.smaller}

- A special matrix appears in both cases (now predicting the response for all observations at once):
  - **Multiple regression**: $\hat{\symbf{y}} = \symbf{X}\hat{\symbf{\beta}}=\symbf{X}(\symbf{X}'\symbf{X})^{-1}\symbf{X}'\symbf{y}= \symbf{H}\symbf{y}$
  - **Multivariate regression**: $\hat{\symbf{\mathcal{Y}}}=\symbf{X}\hat{\symbf{\mathcal{B}}}=\symbf{X}(\symbf{X}'\symbf{X})^{-1}\symbf{X}'\symbf{\mathcal{Y}}= \symbf{H}\symbf{\mathcal{Y}}$
\pause
- This matrix is called the **hat matrix** and is denoted by $\symbf{H}$
  - The reason for the name is that it "puts a hat" on the response vector $\symbf{y}$ or $\symbf{\mathcal{Y}}$
- Special properties:
  - It is a projection matrix, i.e. $\symbf{H}\symbf{y}=\mathrm{proj}_{\symbf{X}}(\symbf{y})$
  - It is idempotent, i.e. $\symbf{H}^2=\symbf{H}$

# Estimator properties

:::: {.columns}

::: {.column width=50%}

## Multiple regression

- **Unbiased**: $\mathrm{E}[\hat{\symbf{\beta}}] = \symbf{\beta}$
- **Zero-mean residual**: $\mathrm{E}[\symbf{\epsilon}]=\mathrm{E}[\symbf{X}'\symbf{\beta} - \symbf{X}'\hat{\symbf{\beta}}] = \symbf{0}$
- **Correlated estimation errors**: $\mathrm{Cov}[\hat{\symbf{\beta}}] = \sigma^2(\symbf{X}'\symbf{X})^{-1}$
- **Correlated residuals**: $\mathrm{Cov}[\hat{\symbf{\epsilon}}] = \sigma^2(\symbf{I} - \symbf{X}(\symbf{X}'\symbf{X})^{-1}\symbf{X}')  =\sigma^2(\symbf{I} - \symbf{H})$

:::

::: {.column width=50%}

## Multivariate regression

- **Unbiased**: $\mathrm{E}[\hat{\symbf{\mathcal{B}}}] = \symbf{\mathcal{B}}$
- **Zero-mean residual**: $\mathrm{E}[\symbf{\mathcal{E}}]=\mathrm{E}[\symbf{X}'\symbf{\mathcal{B}} - \symbf{X}'\hat{\symbf{\mathcal{B}}}] = \symbf{0}$
- **Correlated estimation errors**: $\mathrm{Cov}[\hat{\symbf{\mathcal{B}}}_i, \hat{\symbf{\mathcal{B}}}_j] = \sigma^2_{ij}(\symbf{X}'\symbf{X})^{-1}$ for $i,j\in\{1,\ldots,r\}$, where $\symbf{\mathcal{B}}_i$ is the $i$th column of $\symbf{\mathcal{B}}$
- **Correlated residuals**: $\mathrm{Cov}[\hat{\symbf{\mathcal{E}}}_i, \hat{\symbf{\mathcal{E}}}_j] = \sigma_{ij}(\symbf{I} - \symbf{H})$ for $i,j\in\{1,\ldots,r\}$
  - Now the correlation is both between different responses (columns of $\symbf{\mathcal{Y}}$) and between different observations (rows of $\symbf{\mathcal{Y}}$)

:::

::::

# Estimating the variance

:::: {.columns}

::: {.column width=50%}

## Multiple regression

- The estimator of the variance is

$$
\hat{\sigma^2} = s^2 = \frac{1}{n-k}(\symbf{y} - \symbf{X}\hat{\symbf{\beta}})'(\symbf{y} - \symbf{X}\hat{\symbf{\beta}}).
$$

- Properties:
  - **Unbiased**: $\mathrm{E}[s^2] = \sigma^2$
  - **Independent of $\hat{\symbf{\beta}}$**: $\mathrm{Cov}[s^2, \hat{\symbf{\beta}}] = \symbf{0}$

:::

\pause

::: {.column width=50%}

## Multivariate regression

- The estimator of the variance $\symbf{\Sigma}$ is

$$
\hat{\symbf{\Sigma}} = \symbf{\symbf{S}}= \frac{1}{n-k}(\symbf{\mathcal{Y}} - \symbf{X}\hat{\symbf{\mathcal{B}}})'(\symbf{\mathcal{Y}} - \symbf{X}\hat{\symbf{\mathcal{B}}}).
$$

- Properties:
  - **Unbiased**: $\mathrm{E}[\symbf{S}] = \symbf{\Sigma}$
  - **Independent of $\hat{\symbf{\mathcal{B}}}$**: $\symbf{S}$ and $\hat{\symbf{\mathcal{B}}}$ are independent

:::

::::

# Sums of squares: Multiple regression {.smaller}

:::: {.columns}

::: {.column width=50%}

The total sum of squares is

$$
\mathrm{SST}= \sum_{i=1}^n(y_i - \bar{y})^2,
$$

and can be decomposed into

$$
\mathrm{SSR} + \mathrm{SSE} =
\sum_{i=1}^n(\hat{y}_i - \bar{y})^2 + \sum_{i=1}^n(y_i - \hat{y}_i)^2,
$$

where SST, SSR and SSE are the sums of squares due to the total, regression and error, respectively.

:::

\pause

::: {.column width=50%}

In matrix notation, these are given by

\begin{align*}
\mathrm{SST} &= \symbf{y}'\symbf{y} - n\bar{y}^2, \\
\mathrm{SSR} &= \hat{\symbf{y}}'\hat{\symbf{y}} - n\bar{y}^2, \text{ and }\\
\mathrm{SSE} &= \symbf{y}'\symbf{y} - \hat{\symbf{y}}'\hat{\symbf{y}}.
\end{align*}

:::

::::

# Sums of squares: Multivariate regression {.smaller}

In multivariate regression, there is no universal definition of the sums of squares, but we can consider the following:

$$
\mathrm{SST}= \sum_{i=1}^n\sum_{j=1}^r(y_{ij} - \bar{y}_j)^2,
$$

which can be decomposed into

$$
\mathrm{SSR} + \mathrm{SSE} =
\sum_{i=1}^n\sum_{j=1}^r(\hat{y}_{ij} - \bar{y}_j)^2 + \sum_{i=1}^n\sum_{j=1}^r(y_{ij} - \hat{y}_{ij})^2,
$$

where SST, SSR and SSE are the sums of squares due to the total, regression and error, respectively.

# Assessment of the model: $R^2$ {.smaller}

:::: {.columns}

::: {.column width=50%}

## Multiple regression

The $R^2$ statistic is defined as

$$
R^2 = \frac{\mathrm{SSR}}{\mathrm{SST}} = 1 - \frac{\mathrm{SSE}}{\mathrm{SST}}.
$$

- $R^2$ is the proportion of the total variance explained by the model
- $0 \leq R^2 \leq 1$

:::

\pause

::: {.column width=50%}

## Multivariate regression

One can envision an appropriate multivariate extension to $R^2$, but it is not universally defined.

- One possibility is to consider the proportion of the total variance explained by the model for each response variable, and average this.

*Note*: we won't really use the sums of squares or $R^2$ for multivariate regression in this module.

:::

::::

# Multiple regression example {.smaller}

:::: {.columns}

::: {.column width=50%}

We model the sales price of a house against its size and assessed value.

We load the data as follows:

```{r}
#| echo: true
data(
  "data_tidy_house_price", 
  package = "DataTidy23RodoHonsMult"
)
```

```{r}
#| results: asis
data_tidy_house_price[1:4, ] |> knitr::kable()
```

:::

\pause

::: {.column width=50%}

We extract the response matrix, $\symbf{y}$:

```{r}
#| echo: true
y_vec <- data_tidy_house_price |> 
  dplyr::pull(selling_price)
```

We extract the data for the predictor matrix, $\symbf{X}$, and prepend a column of 1s:

```{r}
#| echo: true
X_mat <- cbind(
  1,
  data_tidy_house_price |>
    dplyr::select(-selling_price)
) |>
  as.matrix()
colnames(X_mat)[1] <- "intercept"
```

:::

::::


# Multiple regression example (cont.)

:::: {.columns}

::: {.column width=50%}

To fit the model, we first obtain the estimated regression coefficients:

```{r}
#| echo: true
beta_vec <- (
  solve(t(X_mat) %*% X_mat) %*%
  t(X_mat)
  ) %*%
  y_vec
colnames(beta_vec) <- "estimate"
```

```{r}
#| results: asis
beta_vec |> signif(2) |> knitr::kable()
```

:::

\pause

:::{.column width=50%}

This matches the output from the `lm` function:

```{r}
#| echo: true
#| results: asis
lm(
  selling_price ~ .,
  data = data_tidy_house_price
) |> 
  coef() |> signif(2) |> knitr::kable()
```

:::

::::



# Multiple regression example (cont.) {.smaller}

:::: {.columns}

::: {.column width=50%}

To assess the model, we calculate the $R^2$ statistic:

```{r}
#| echo: true
SST <- sum((y_vec - mean(y_vec))^2)
SSE <- (y_vec - X_mat %*% beta_vec)^2 |>
  sum()
SSR <- SST - SSE
R2 <- SSR / SST
R2 |> signif(2)
```

:::

\pause

::: {.column width=50%}

- An $R^2$ of 0.83 indicates that 83% of the variance in house prices is explained by the size and assessed value of the house.
- This is a high value, suggesting that the model explains the response "well".

:::

::::

# Multivariate example {.smaller}

Consider a dataset of four wood-pulp variables and four paper properties:

```{r}
#| echo: true
data(
  "data_tidy_paper", 
  package = "DataTidy23RodoHonsMult"
)
```

```{r}
#| results: asis
data_tidy_paper[1:4, ] |> knitr::kable()
```


# Multivariate example (cont.) {.smaller}

:::: {.columns}

::: {.column width=50%}

We extract the response matrix, $\symbf{Y}$:

```{r}
#| echo: true
Y_mat <- data_tidy_paper |> 
  dplyr::select(-starts_with("pulp")) |> 
  as.matrix()
```

```{r}
#| results: asis
Y_mat[1:4, ] |> signif(2) |> knitr::kable()
```

:::

\pause

::: {.column width=50%}

We extract the data for the predictor matrix, $\symbf{X}$, and prepend a column of 1s:

```{r}
#| echo: true
X_mat <- as.matrix(cbind(1,
  data_tidy_paper |>
    dplyr::select(starts_with("pulp"))
))
colnames(X_mat)[1] <- "intercept"
```

```{r}
#| results: asis
X_mat[1:3, , drop = FALSE] |> signif(2) |> knitr::kable()
```

:::

::::

# Multivariate example (cont.) {.smaller}

:::: {.columns}

::: {.column width=50%}

We first obtain the estimated regression coefficients:

```{r}
#| echo: true
B_mat <- (
  solve(t(X_mat) %*% X_mat) %*%
  t(X_mat)) %*% Y_mat
```

```{r}
#| results: asis
B_mat[, 1:3] |> signif(2) |> knitr::kable()
```

:::

\pause

::: {.column width=50%}

This matches the output from the `lm` function (exc. fourth column for space):

```{r}
#| echo: true
#| results: asis
lm(
  cbind(paper_1, paper_2, paper_3, paper_4) ~ .,
  data = data_tidy_paper
)$coefficients[, 1:3] |> signif(2) |> knitr::kable()
```

:::

::::

# Inference for multiple regression

:::: {.columns}

::: {.column width=50%}

- Up to now, we have not assumed a full distribution for the errors ($\symbf{\epsilon}$).
- To perform inference (calculate CI's and p-values), we make the additional assumption that the errors ($\symbf{\epsilon}$) are normally distributed, implying that

$$
\symbf{\epsilon} \sim \mathcal{N}(\symbf{0}, \sigma^2\symbf{I}).
$$

:::

\pause

:::{.column width=50%}

- This implies that

$$
\hat{\symbf{\beta}} \sim \mathcal{N}(\symbf{\beta}, \sigma^2(\symbf{X}'\symbf{X})^{-1}).
$$

- As well as that

$$
\frac{n-k}{\sigma^2}s^2 \sim \chi^2_{n-k}.
$$

- We note again that $\hat{\symbf{\beta}}$ and $s^2$ are independent.

:::

::::

# Confidence intervals for multiple regression {.smaller}

Using the previous results, a $100(1-\alpha)\%$ confidence ellipsoid for $\symbf{\beta}$ is given by

$$
\frac{
  (\symbf{\beta} - \hat{\symbf{\beta}})'
  \symbf{X}'\symbf{X}
  (\symbf{\beta} - \hat{\symbf{\beta}})
}{
  ks^2
}
\leq F_{k, n-k}^{1-\alpha},
$$

implying that a confidence interval for $\beta_j$ is

$$
\hat{\beta}_j \pm t_{n-k}^{\alpha/2}\sqrt{s^2(\symbf{X}'\symbf{X})^{-1}_{jj}}.
$$

# Hypothesis testing {.smaller}

:::: {.columns}

::: {.column width=50%}

## Univariate

- To test the null hypothesis $H_0: \beta_j = 0$, we use the test statistic

$$
t = \frac{\hat{\beta}_j}{\sqrt{s^2(\symbf{X}'\symbf{X})^{-1}_{jj}}}.
$$

- and reject at significance-level $\alpha$ if

$$
|t| > t_{n-k}^{1-\alpha/2}.
$$

:::

\pause

::: {.column width=50%}

## Simultaneous

- To test the null hypothesis $H_0: \symbf{\beta}^{(1)}=\symbf{0}$ where $\symbf{\beta}^{(1)}:q\times 1$ and $\symbf{\beta}=\begin{pmatrix} \symbf{\beta}^{(1)} & \symbf{\beta}^{(2)} \end{pmatrix}'$, we use

$$
F = \frac{\hat{\symbf{\beta}}^{(1)\prime}C_{11}^{-1}\hat{\symbf{\beta}}^{(1)}}{qs^2},
$$

- where $(\symbf{X}'\symbf{X})^{-1} = \symbf{C} = \begin{bmatrix} \symbf{C}_{11} & \symbf{C}_{12} \\ \symbf{C}_{21} & \symbf{C}_{22}  \end{bmatrix}$.

- Accordingly, we reject at significance-level $\alpha$ if

$$
F \geq F_{q, n-k}^{1-\alpha}.
$$

:::

::::

# Inference example {.smaller}

Consider the dataset of four wood-pulp variables and four paper properties:

```{r}
#| echo: true
data(
  "data_tidy_paper", 
  package = "DataTidy23RodoHonsMult"
)
```

We'll select only the first paper variable:

```{r}
#| results: asis
data_tidy_paper <- data_tidy_paper |>
  dplyr::select(matches("pulp|paper_1"))
data_tidy_paper[1:4, ] |> knitr::kable()
```

# Inference example (cont.) {.smaller}

:::: {.columns}

::: {.column width=50%}

We extract the response matrix, $\symbf{Y}$:

```{r}
#| echo: true
Y_mat <- data_tidy_paper[, "paper_1", drop = FALSE] |>
  as.matrix()
colnames(Y_mat) <- "paper_1"
```

```{r}
#| results: asis
Y_mat[1:4, ] |> signif(2) |> knitr::kable()
```

:::

\pause

::: {.column width=50%}

We extract the data for the predictor matrix, $\symbf{X}$, and prepend a column of 1s:

```{r}
#| echo: true
X_mat <- as.matrix(cbind(1,
  data_tidy_paper |>
    dplyr::select(starts_with("pulp"))
))
colnames(X_mat)[1] <- "intercept"
```

```{r}
#| results: asis
X_mat[1:3, , drop = FALSE] |> signif(2) |> knitr::kable()
```

:::

::::

# Inference example (cont.) {.smaller}

::::{.columns}

:::{.column width=50%}

## Estimates

We first estimate the coefficients:

```{r}
#| echo: true
beta_vec <- 
  (solve(t(X_mat) %*% X_mat) %*%
    t(X_mat)) %*% Y_mat
colnames(beta_vec) <- "estimate"
```

```{r}
#| results: asis
beta_vec |> signif(2) |> knitr::kable()
```

:::

:::{.column width=50%}

## $\beta_{\mathrm{pulp_1}}$ confidence interval

To get the 95% confidence interval for the $\beta_{\mathrm{pulp_1}}$ coefficient, we first get the standard error:

```{r}
#| echo: true
XtX_mat <- t(X_mat) %*% X_mat
XtX_mat_inv <- solve(XtX_mat)
var_resp <- sum((Y_mat - X_mat %*% beta_vec)^2) /
  (nrow(Y_mat) - ncol(X_mat))
sd_beta_pulp1 <- sqrt(var_resp) * sqrt(XtX_mat_inv[2, 2])
sd_beta_pulp1 |> signif(2)
```

:::

::::

# Inference example (cont.) {.smaller}

:::: {.columns}

::: {.column width=50%}

## $\beta_{\mathrm{pulp_1}}$ confidence interval (cont.)

We then get the relevant $t$-distributed quantile:

```{r}
#| echo: true
t_beta_pulp1 <- qt(0.975, nrow(Y_mat) - ncol(X_mat))
t_beta_pulp1 |> signif(2)
```

The confidence interval is then:

```{r}
#| echo: true
(beta_vec[2] + c(-1, 1) * t_beta_pulp1 * sd_beta_pulp1) |>
  signif(2)
```

:::

::: {.column width=50%}

## Hypothesis testing: $H_0: \beta_{\mathrm{pulp_1}} = 0$

The test statistic is given by

```{r}
#| echo: true
t_stat <- beta_vec[2] / sd_beta_pulp1
t_stat |> signif(2)
```

The p-value is therefore:

```{r}
#| echo: true
(pt(-abs(t_stat),
  nrow(Y_mat) - ncol(X_mat)) * 2
) |> signif(2)
```

:::

::::

# Inference example (cont.) {.smaller}

:::: {.columns}

::: {.column width=50%}

## Hypothesis testing: $H_0: \symbf{\beta}^{(1)} = \symbf{0}$

We now test the hypothesis that $\symbf{\beta}^{(1)} = \beta_1=\beta_3=\beta_4=0$.

We first extract $\symbf{C}_11$:

```{r}
#| echo: true
C11_mat <- XtX_mat_inv[2:5, 2:5]
C11_mat <- C11_mat[-2, -2]
C11_mat |> signif(2)
```

:::

::: {.column width=50%}

## Hypothesis testing: $H_0: \symbf{\beta}^{(1)} = \symbf{0}$

We now extract the relevant estimates:

```{r}
#| echo: true
beta_vec_1 <- beta_vec[
  c(2, 4:5), , drop = FALSE
]
beta_vec_1 |> signif()
```

:::

::::


# Inference example (cont.) {.smaller}

:::: {.columns}

::: {.column width=50%}

## Hypothesis testing: $H_0: \symbf{\beta}^{(1)} = \symbf{0}$

The test statistic is then:

```{r}
#| echo: true
f_stat <- (t(beta_vec_1) %*%
  solve(C11_mat) %*%
  beta_vec_1) / length(beta_vec_1) /
  var_resp
f_stat |> signif(2)
```

:::

:::{.column width=50%}

## Hypothesis testing: $H_0: \symbf{\beta}^{(1)} = \symbf{0}$

This yields the following p-value:

```{r}
#| echo: true
pf(f_stat, length(beta_vec_1),
  nrow(Y_mat) - ncol(X_mat),
  lower.tail = FALSE
) |> signif(2)
```

:::

::::

# Inference for multivariate regression {.smaller}

- Multiple and multivariate regression produce *exactly* the same inference when coefficients are examined within the context of a single response variable.
- For example, suppose that we model BMI and blood pressure against age, weight and height:
  - The coefficients, p-values and confidence intervals for age, weight and height will be the same in both multiple (fitting two models) and multivariate regression.
  - The simultaneous hypothesis tests will be the same when the considering only groups of coefficients iwth respect to a particular response variable. As an example, the p-value for the hypothesis that neither age nor weight has an effect on BMI will be the same in both models.
- The main difference of interest is that we can perform simultaneous hypothesis tests for regression coefficients across different response variables in multivariate regression.
  - To continue the example above, in multivariate regression we can test that the effects of age on BMI and blood pressure are both zero.

# Hypothesis testing in multivariate regression

:::: {.columns}

::: {.column width=50%}

Suppose we wish to test that

$$
\symbf{\mathcal{B}}^{(1)} = \symbf{0},
$$

where 

$$
\symbf{\mathcal{B}} = \begin{pmatrix} \symbf{\mathcal{B}}^{(1)} \\ \symbf{\mathcal{B}}^{(2)} \end{pmatrix}
$$

and $\symbf{\mathcal{B}}^{(1)}:q\times r$.

This is the hypothesis that the first $q$ predictors have no effect on any of the $r$ responses.

:::

\pause

::: {.column width=50%}

- We'll need to fit a model without the first $q$ predictors and compare the fit of the two models.
- Let's define $\symbf{X}^{(2)}:n\times (k-q) \text{ for } i = 1,2$ as follows:

$$
\symbf{X}= \begin{pmatrix} \symbf{X}^{(1)} & \symbf{X}^{(2)} \end{pmatrix}
$$

:::

::::

# Hypothesis testing in multivariate regression {.smaller}

:::: {.columns}

::: {.column width=50%}

- Under the implicit assumption that $\symbf{\mathcal{B}}^{(1)} = \symbf{0}$, we obtain the estimated regression parameters as

$$
\hat{\symbf{\mathcal{B}}}^{(2)} = (\symbf{X}^{(2)\prime}\symbf{X}^{(2)})^{-1}\symbf{X}^{(2)\prime}\symbf{\mathcal{Y}},
$$

- and the MLE variance-covariance matrices as

$$
\hat{\Sigma}_{\mathrm{MLE}} = \frac{1}{n}(\symbf{\mathcal{Y}} - \symbf{X}\hat{\symbf{\mathcal{B}}})'(\symbf{\mathcal{Y}} - \symbf{X}\hat{\symbf{\mathcal{B}}}).
$$

- and

$$
\hat{\symbf{\Sigma}}^{(2)}_{\mathrm{MLE}} = \frac{1}{n}(\symbf{\mathcal{Y}} - \symbf{X}^{(2)}\hat{\symbf{\mathcal{B}}}^{(2)})'(\symbf{\mathcal{Y}} - \symbf{X}^{(2)}\hat{\symbf{\mathcal{B}}}^{(2)}).
$$

:::

\pause

::: {.column width=50%}

- The test statistic is then

\begin{align*}
\Lambda &= \frac{
  \displaystyle\max_{\symbf{\mathcal{B}}^{(2)},\symbf{\Sigma^{(2)}}}
  L(\symbf{\mathcal{B}}^{(2)},\symbf{\Sigma^{(2)}})
}{
  \displaystyle\max_{\symbf{\mathcal{B}},\symbf{\Sigma}}
  L(\symbf{\mathcal{B}},\symbf{\Sigma})
}, \\
&= \left(\frac{|\hat{\symbf{\Sigma}}|}{|\hat{\symbf{\Sigma^{(2)}}}|}\right)^{n/2}.
\end{align*}


- For $n-r$ and $n-k$ sufficiently large, the following approximation holds:

$$
-[n-k-0.5(r-p+q+1)]\ln(\Lambda^{2/n})\sim \chi^2_{r(p-1)}.
$$

:::

::::

# Multivariate inference example

Using the paper dataset, we will test whether pulp_2 or pulp_3 have an effect on any paper type.

Loading the data as follows:

```{r}
#| echo: true
data(
  "data_tidy_paper", 
  package = "DataTidy23RodoHonsMult"
)
```

```{r}
#| results: asis
data_tidy_paper[1:4, ] |> knitr::kable()
```

# Multivariate example (cont.) {.smaller}

:::: {.columns}

::: {.column width=50%}

We extract the response matrix, $\symbf{Y}$:

```{r}
#| echo: true
Y_mat <- data_tidy_paper |> 
  dplyr::select(-starts_with("pulp")) |> 
  as.matrix()
```

```{r}
#| results: asis
Y_mat[1:4, ] |> signif(2) |> knitr::kable()
```

:::

\pause

::: {.column width=50%}

We extract the data for the predictor matrix, $\symbf{X}$, and prepend a column of 1s:

```{r}
#| echo: true
X_mat <- as.matrix(cbind(1,
  data_tidy_paper |>
    dplyr::select(starts_with("pulp"))
))
colnames(X_mat)[1] <- "intercept"
```

```{r}
#| results: asis
X_mat[1:3, , drop = FALSE] |> signif(2) |> knitr::kable()
```

:::

::::

# Multivariate example (cont.) {.smaller}

:::: {.columns}

::: {.column width=50%}

We extract the data for the predictor matrix excluding pulp_2 and pulp_3, $\symbf{X}^{(2)}$:

```{r}
#| echo: true
X_mat_2 <- X_mat[
  , -(3:4), drop = FALSE
]
```

```{r}
#| results: asis
X_mat_2[1:3, , drop = FALSE] |> signif(2) |> knitr::kable()
```

:::

::: {.column width=50%}

We obtain estimated regression coefficients under both the full and reduced models:

```{r}
#| echo: true
B_mat <- (
  solve(t(X_mat) %*% X_mat) %*%
  t(X_mat)) %*% Y_mat
```

```{r}
#| echo: true
B_mat_2 <- (
  solve(t(X_mat_2) %*% X_mat_2) %*%
  t(X_mat_2)) %*% Y_mat
```

:::

::::

# Multivariate example (cont.) {.smaller}

:::: {.columns}

::: {.column width=50%}

We now calculate the sample variance-covariance matrices under the full and reduced models:

```{r}
#| echo: true
Sigma_mat <-
  t((Y_mat - X_mat %*% B_mat)) %*%
    (Y_mat - X_mat %*% B_mat) /
    (nrow(Y_mat))
```

```{r}
#| echo: true
Sigma_mat_2 <-
  t((Y_mat - X_mat_2 %*% B_mat_2)) %*%
    (Y_mat - X_mat_2 %*% B_mat_2) /
    (nrow(Y_mat))
```

:::

::: {.column width=50%}

- This yields the following test statistic:

```{r}
#| echo: true
n <- nrow(Y_mat)
r <- ncol(Y_mat)
q <- ncol(X_mat) - ncol(X_mat_2)
k <- ncol(X_mat)
p <- k - 1
test_stat <-
  -(n - k - 0.5 * (r - p + q + 1)) *
  log(
    det(Sigma_mat) / det(Sigma_mat_2)
  )
test_stat |> signif(2)
```

with p-value

```{r}
#| echo: true
pchisq(test_stat, r * (p - q), lower.tail = FALSE) |>
  signif(2)
```

:::

::::

# Principal components regression (PCR)

- In PCR, we replace the original $p$ (non-intercept) predictors with $m \leq p$ principal components.
- Motivations:
  - Avoid multicollinearity
  - Reduce overfitting
  - Use fewer predictors
  - Improve interpretability

# Method

Very straigthtforward...

1. Calculate the principal components of the (non-intercept) predictors.
2. Fit a multiple regression model using the first $m$ principal components as predictors.

\pause

If the number of principal components is pre-specified, then all the inferential methods of multiple regression apply.


# Relationship with ridge regression

- PCR is akin to a rough version of ridge regression [@hastie_etal09]:

\begin{figure}
\centering
\includegraphics[width=0.75\textwidth]{_data_raw/img/pcr_vs_ridge.png}
\end{figure}

# Selecting the number of principal components

- One can use the same reasoning as before (scree plots, proportion of variation captured), or cross-validation (or $R^2$, but this isn't really done).
- Note that when performing cross validation, the loadings must be computed afresh within each fold.

# Partial least squares (PLS)

- Partial least squares (PLS) is a method that is similar to PCR, in that it takes linear combinations of the input variables.
- However, instead of choosing the linear combinations to maximise the variance, it chooses them to maximise (something like) the covariance of the resultant variables with the predictor.

# Partial least squares algorithm [@hastie_etal09]

\begin{figure}
\centering
\includegraphics[width=0.75\textwidth]{_data_raw/img/pls_alg.png}
\end{figure}

# PLS versus PCR [@hastie_etal09]

- PCR and PLS optimise different objective functions:

\begin{figure}
\centering
\includegraphics[width=0.75\textwidth]{_data_raw/img/pcr_vs_pls.png}
\end{figure}

# Selecting the number of components in PCR and PLS [@hastie_etal09]

- The number of components can be chosen by cross-validation.
- @hastie_etal09 chose the number of components within one standard error of the minimum cross-validated error:

\begin{figure}
\centering
\includegraphics[width=0.75\textwidth]{_data_raw/img/cv-pcr_and_pls.png}
\end{figure}

# Resources

- Source code is on [GitHub](https://github.com/MiguelRodo/Slides23RodoHonsMult/blob/2024/SlidesHonsMultL8Reg.qmd).
  - Suppressed code (e.g. to create figures) is available there.

## References
