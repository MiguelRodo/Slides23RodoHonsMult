---
title: Principal component analysis
subtitle: Modern multivariate statistical techniques
author: Miguel Rodo
date: "2024-04-04"
bibliography: zotero.bib
format:
  beamer:
    embed-resources: true
    aspectratio: 169
    urlcolor: cyan
    linkcolor: blue
    filecolor: magenta
    include-in-header:
      file: preamble.tex
---

```{r }
#| include: false
library(tibble)
library(ggplot2)
for (x in list.files("R", full.names = TRUE)) {
  source(x)
}
```

# Context

- Multivariate analysis helps us uncover patterns and relationships within datasets containing multiple variables.
- **Example**: Meteorologists analyze temperature, humidity, wind speed, and more to understand weather systems.
  - Possible questions:
    - What sets of variable(s) are correlated?
    - Can we predict the likelihood of rain based on these variables?
    - What variables most distinguish between different weather patterns?

# Basis of approach

- **Linear combinations**:

$$
\symbf{x}_1\symbf{u}_1 + \symbf{x}_2\symbf{u}_2 + \ldots + \symbf{x}_p\symbf{u}_p
$$

- Why linear combinations?
  - Simplicity >> strong theoretical results >> robustness, speed and interpretability



# Types of multivariate analysis

- $\symbf{X} \sim \symbf{X}$: PCA, factor analysis, correspondence analysis (count data)
  - **Examples**: identify correlates sets of variables
- $\symbf{Y} \sim \symbf{X}$: Multiple/multivariate regression, canonical correlation analysis, discrimination and classification

# Principal component analysis

- Principal components analysis (PCA) identifies the directions of greatest variation, allowing you to:
  - Represent data more compactly, and
  - Interpret relationships between variables.

- PCA is a linear technique with an algebraic solution, so it is fast to fit.

# Example applications

- **Market analysis**: Identify correlated stocks to ensure a diversified portfolio
- **Image compression**: Represent images with fewer pixels while preserving accuracy
- **Bioinformatics**: Identify sets of co-expressed genes responsible for different functions in the body


# Projection onto a vector ("line")

::: {.columns}

::: {.column width="50%"}

- Suppose that we wish to project the vector $\symbf{b}$ onto the vector $\symbf{a}$.
  - The projection is the nearest point to $\symbf{b}$ on the line spanned by $\symbf{a}$.
- The projection of $\symbf{b}$ onto $\symbf{a}$ is given by:

$$
\text{proj}_{\symbf{a}}(\symbf{b}) = \frac{\symbf{a}^T \symbf{b}}{\symbf{a}^T \symbf{a}} \symbf{a}
$$

- Here is a diagram of such a projection. Suppose that we want to project the vector $\begin{pmatrix} 2 \\ 1 \end{pmatrix}$ onto the vector $\begin{pmatrix} 1 \\ 1 \end{pmatrix}$.

:::

::: {.column width="50%"}

```{r}
#| echo: false
a <- c(2, 2)
b <- c(2, 1)
proj <- as.numeric((a %*% b) / (a %*% a)) * a
plot_tbl <- tibble::tibble(
  x = c(0, a[1], 0, b[1], 0, proj[1]),
  y = c(0, a[2], 0, b[2], 0, proj[2]),
  label = rep(c("a", "b", "proj(a, b)"), each = 2)
)
plot_tbl <- tibble::tibble(
  x_start = rep(0, 3),
  y_start = rep(0, 3),
  x_end = c(a[1], b[1], proj[1]),
  y_end = c(a[2], b[2], proj[2]),
  label = rep(c("a", "b", "proj(a, b)"), each = 1)
)
plot_tbl_error <- tibble::tibble(
  x_start = b[1],
  y_start = b[2],
  x_end = proj[1],
  y_end = proj[2],
  label = "error"
)
p <- ggplot(
  plot_tbl,
  aes(x = x, y = y, label = label, group = label)
) +
  ggrepel::geom_text_repel(
    data = plot_tbl |> dplyr::filter(label %in% c("a", "b")),
    mapping = aes(x = x_end, y = y_end),
    nudge_x = -0.25, nudge_y = 0.15,
    col = "gray25", min.segment.length = 2
  ) +
  ggrepel::geom_text_repel(
    data = plot_tbl |> dplyr::filter(label == "proj(a, b)"),
    mapping = aes(x = x_end, y = y_end),
    nudge_x = -0.33, nudge_y = 0.23,
    col = "gray25", min.segment.length = 2
  ) +
  cowplot::theme_cowplot() +
  cowplot::background_grid(major = "xy") +
  geom_segment(
    data = plot_tbl |> dplyr::filter(label != "proj(a, b)"),
    aes(x = x_start, y = y_start, xend = x_end, yend = y_end),
    arrow = grid::arrow(length = unit(0.3, "cm"))
  ) +
  geom_segment(
    data = plot_tbl |> dplyr::filter(label == "proj(a, b)"),
    aes(x = x_start, y = y_start, xend = x_end, yend = y_end),
    arrow = grid::arrow(length = unit(0.3, "cm")), col = "dodgerblue"
  ) +
  geom_segment(
    data = plot_tbl_error,
    aes(x = x_start, y = y_start, xend = x_end, yend = y_end),
    col = "red", linetype = "dotted"
  ) +
  theme( 
    panel.background = element_rect(fill = "white"), 
    plot.background = element_rect(fill = "white"),
    axis.title = element_blank(),
    axis.ticks = element_blank(),
    axis.text = element_blank()
  ) +
  coord_equal()
path_p_proj_init <- projr::projr_path_get("cache", "fig", "p-proj-init.png")
cowplot::save_plot(
  plot = p,
  filename = path_p_proj_init,
  units = "cm",
  base_height = 6,
  base_width = 6
)
```

\centering
```{r}
knitr::include_graphics(path_p_proj_init)
```

:::

::::

# Projecting onto a unit vector

- If $\symbf{a}$ is a unit vector, then the projection of $\symbf{b}$ onto $\symbf{a}$ is given by:

$$
\text{proj}_{\symbf{a}}(\symbf{b}) = (\symbf{a}^T \symbf{b})\symbf{a}.
$$

- This means that the length of the projection is the dot product of $\symbf{a}$ and $\symbf{b}$.
  - This is a linear combination of the elements of $\symbf{b}$: $\sum_{i=1}^p a_i b_i$.
- Note that the length of $\symbf{a}$ does not affect the projection.

# Two-variable example

:::: {.columns}

::: {.column width="50%"}

- Suppose we have a dataset with two correlated variables, perhaps weight and height.
- Then, the scatterplot of these may look something like this:

```{r}
set.seed(2106)
plot_tbl <- tibble::tibble(
  height = rnorm(100, 160, 10)
) |>
  dplyr::mutate(
    weight = height / 2.5 + rnorm(100, 0, 3.5 * height / 160)
  ) |>
  dplyr::mutate(
    across(everything(), function(x) (x - mean(x)) / sd(x))
  ) |>
  dplyr::arrange(height)
hw_tbl <- plot_tbl
p <- ggplot(
  aes(x = height, y = weight),
  data = plot_tbl
) +
  geom_vline(
    xintercept = 0, col = "gray50", linetype = "solid"
  ) +
  geom_hline(
    yintercept = 0, col = "gray50", linetype = "solid"
  ) +
  cowplot::theme_cowplot() +
  cowplot::background_grid(major = "xy") +
  theme( 
    panel.background = element_rect(fill = "white"), 
    plot.background = element_rect(fill = "white")
  ) +
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank()
  ) +
  labs(
    x = "Height",
    y = "Weight"
  ) +
  geom_point(alpha = 0.85, col = "dodgerblue")
path_p_height_weight <- projr::projr_path_get("cache", "fig", "p-height-weight.png")
cowplot::save_plot(
  plot = p,
  filename = path_p_height_weight,
  units = "cm",
  base_height = 5,
  base_width = 5
)
```

\centering

```{r}
knitr::include_graphics(path_p_height_weight)
```

:::

::: {.column width="50%"}

- We could combine these two variables into a single variable, representing size: $a_1 \times \text{height} + a_2 \times \text{weight}$.
  - If we restrict $a_1^2 + a_2^2 = 1$, then this equivalent to projecting the height and weight vector ($\symbf{b} = [\text{height}, \text{weight}]$) onto the line spanned by the vector $\symbf{a} = [a_1, a_2]$.
  - Since the projection is independent of the length of whatever you're projecting onto, this is equivalent to projecting onto any vector in the same direction as $[a_1, a_2]$.

:::

::::

# Two-variable example (cont.) {.smaller}

:::: {.columns}

::: {.column width="50%"}

- The new variable is then the *length of the projection*.
- For example, suppose we have the point $[1.2, 0.3]$:

```{r}
set.seed(2106)
plot_tbl <- tibble::tibble(
  height = rnorm(100, 160, 10)
) |>
  dplyr::mutate(
    weight = height / 2.5 + rnorm(100, 0, 3.5 * height / 160)
  ) |>
  dplyr::mutate(
    across(everything(), function(x) (x - mean(x)) / sd(x))
  ) |>
  dplyr::arrange(height)
proj_list <- list(c(1 / sqrt(2), 1 / sqrt(2)), c(1, 0), c(0, 1))[1]
plot_tbl_ind <- tibble::tibble(
  height = 1.2,
  weight = 0.3
)
plot_tbl_proj_ind <- proj_list |>
  purrr::map_df(~ mutate_proj(plot_tbl_ind, .x))

p <- ggplot(
  data = plot_tbl
) +
  geom_vline(
    xintercept = 0, col = "gray50", linetype = "solid"
  ) +
  geom_hline(
    yintercept = 0, col = "gray50", linetype = "solid"
  ) +
  cowplot::theme_cowplot() +
  cowplot::background_grid(major = "xy") +
  theme(
    panel.background = element_rect(fill = "white"),
    plot.background = element_rect(fill = "white")
  ) +
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank()
  ) +
  labs(
    x = "Height",
    y = "Weight"
  ) +
  geom_point(aes(x = height, y = weight), alpha = 0.1, col = "dodgerblue") +
  geom_point(
    data = plot_tbl_proj_ind,
    aes(x = height, y = weight), col = "mediumseagreen"
  ) +
  geom_segment(
    data = plot_tbl_proj_ind,
    aes(x = height, y = weight, xend = proj_x, yend = proj_y, group = proj),
    col = "gray25", arrow = arrow(length = unit(0.3, "cm")), linetype = "solid"
  ) +
  geom_abline(
    data = plot_tbl_proj_ind |>
      dplyr::mutate(slope = a2 / a1) |>
      dplyr::filter(slope %in% c(0,1)),
    aes(intercept = 0, slope = slope),
    col = "purple"
  ) +
  coord_cartesian(xlim = c(0, 2.5), ylim = c(0, 2.5)) +
  ggrepel::geom_text_repel(
    data = plot_tbl_proj_ind |>
      dplyr::mutate(a1 = a1 |> signif(2), a2 = a2 |> signif(2)),
    aes(x = a1, y = a2, label = proj),
    nudge_x = 0.1, nudge_y = 0.1
  )
path_p_height_weight_ind <- projr::projr_path_get("cache", "fig", "p-height-weight-ind.png")
cowplot::save_plot(
  plot = p,
  filename = path_p_height_weight_ind,
  units = "cm",
  base_height = 5,
  base_width = 5
)
```

\centering

```{r}
#| results: asis
knitr::include_graphics(path_p_height_weight_ind)
```

:::

::: {.column width="50%"}

- If we set $\symbf{a} = [1/\sqrt{2}, 1/\sqrt{2}]$, then the new variable is $1/\sqrt{2} \times 1.2 + 1/\sqrt{2} \times 0.3$.
- We go from the matrix of vairables

```{r}
tibble::tibble(height = 1.2, weight = 0.3) |> as.data.frame()
```

- to

```{r}
tibble::tibble(size = 1.2 / sqrt(2) + 0.3 / sqrt(2)) |> as.data.frame()
```

:::

::::



# Projections and axes

:::: {.columns}

::: {.column width="50%"}

- If we set $\symbf{a} = [1, 0]$, then the "new" variable is the height.
- If we set $\symbf{a} = [0, 1]$, then the "new" variable is the weight.

- The point of these last two is that original coordinates are, in a sense, themselves projections.
- So, if we consider $[1, 0]$ and $[0, 1]$ as axes, then we can consider $[1/\sqrt{2}, 1/\sqrt{2}]$ as a new axis.

:::

::: {.column width="50%"}


```{r}
set.seed(2106)
plot_tbl <- tibble::tibble(
  height = rnorm(100, 160, 10)
) |>
  dplyr::mutate(
    weight = height / 2.5 + rnorm(100, 0, 3.5 * height / 160)
  ) |>
  dplyr::mutate(
    across(everything(), function(x) (x - mean(x)) / sd(x))
  ) |>
  dplyr::arrange(height)
proj_list <- list(c(1 / sqrt(2), 1 / sqrt(2)), c(1, 0), c(0, 1))[2:3]
plot_tbl_ind <- tibble::tibble(
  height = 1.2,
  weight = 0.3
)
plot_tbl_proj_ind <- proj_list |>
  purrr::map_df(~ mutate_proj(plot_tbl_ind, .x))

p <- ggplot(
  data = plot_tbl
) +
  geom_vline(
    xintercept = 0, col = "gray50", linetype = "solid"
  ) +
  geom_hline(
    yintercept = 0, col = "gray50", linetype = "solid"
  ) +
  cowplot::theme_cowplot() +
  cowplot::background_grid(major = "xy") +
  theme(
    panel.background = element_rect(fill = "white"),
    plot.background = element_rect(fill = "white")
  ) +
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank()
  ) +
  labs(
    x = "Height",
    y = "Weight"
  ) +
  geom_point(aes(x = height, y = weight), alpha = 0.1, col = "dodgerblue") +
  geom_point(
    data = plot_tbl_proj_ind,
    aes(x = height, y = weight), col = "mediumseagreen"
  ) +
  geom_segment(
    data = plot_tbl_proj_ind,
    aes(x = height, y = weight, xend = proj_x, yend = proj_y, group = proj),
    col = "gray25", arrow = arrow(length = unit(0.3, "cm")), linetype = "solid"
  ) +
  geom_abline(
    data = plot_tbl_proj_ind |>
      dplyr::mutate(slope = a2 / a1) |>
      dplyr::filter(slope %in% c(0,1)),
    aes(intercept = 0, slope = slope),
    col = "purple"
  ) +
  geom_vline(
    xintercept = 0,
    col = "purple"
  ) +
  coord_cartesian(xlim = c(0, 2.5), ylim = c(0, 2.5)) +
  ggrepel::geom_text_repel(
    data = plot_tbl_proj_ind |>
      dplyr::mutate(a1 = a1 |> signif(2), a2 = a2 |> signif(2)),
    aes(x = a1, y = a2, label = proj),
    nudge_x = 0.1, nudge_y = 0.1
  )
path_p_height_weight_ind_orig <- projr::projr_path_get("cache", "fig", "p-height-weight-ind-orig.png")
cowplot::save_plot(
  plot = p,
  filename = path_p_height_weight_ind_orig,
  units = "cm",
  base_height = 5,
  base_width = 5
)
```

\centering

```{r}
#| results: asis
knitr::include_graphics(path_p_height_weight_ind_orig)
```

:::

::::

# Alternate axes example

:::: {.columns}

::: {.column width="50%"}

- Clearly, if we just have one axis (e.g. $[1/\sqrt{2}, -1/\sqrt{2}]$) and two original variables (e.g. height and weight), then we cannot represent the data exactly.
- An additional linearly independent coordinate axis (e.g.  $[1/\sqrt{2}, -1/\sqrt{2}]$) would then provide us two values for each point, and be an alternative way to represent the data exactly.

:::

::: {.column width="50%"}

```{r}
set.seed(2106)
plot_tbl <- tibble::tibble(
  height = rnorm(100, 160, 10)
) |>
  dplyr::mutate(
    weight = height / 2.5 + rnorm(100, 0, 3.5 * height / 160)
  ) |>
  dplyr::mutate(
    across(everything(), function(x) (x - mean(x)) / sd(x))
  ) |>
  dplyr::arrange(height)
proj_list <- list(c(1 / sqrt(2), 1 / sqrt(2)), c(1 / sqrt(2), -1 / sqrt(2)))
plot_tbl_ind <- tibble::tibble(
  height = 1.2,
  weight = 0.3
)
plot_tbl_proj_ind <- proj_list |>
  purrr::map_df(~ mutate_proj(plot_tbl_ind, .x))

p <- ggplot(
  data = plot_tbl
) +
  geom_vline(
    xintercept = 0, col = "gray50", linetype = "solid"
  ) +
  geom_hline(
    yintercept = 0, col = "gray50", linetype = "solid"
  ) +
  cowplot::theme_cowplot() +
  cowplot::background_grid(major = "xy") +
  theme(
    panel.background = element_rect(fill = "white"),
    plot.background = element_rect(fill = "white")
  ) +
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank()
  ) +
  labs(
    x = "Height",
    y = "Weight"
  ) +
  geom_point(aes(x = height, y = weight), alpha = 0.1, col = "dodgerblue") +
  geom_point(
    data = plot_tbl_proj_ind,
    aes(x = height, y = weight), col = "mediumseagreen"
  ) +
  geom_segment(
    data = plot_tbl_proj_ind,
    aes(x = height, y = weight, xend = proj_x, yend = proj_y, group = proj),
    col = "gray25", arrow = arrow(length = unit(0.3, "cm")), linetype = "solid"
  ) +
  geom_abline(
    data = plot_tbl_proj_ind |>
      dplyr::mutate(slope = a2 / a1) ,
    aes(intercept = 0, slope = slope),
    col = "purple"
  ) +
  coord_cartesian(xlim = c(-2.5, 2.5), ylim = c(-2.5, 2.5)) +
  ggrepel::geom_text_repel(
    data = plot_tbl_proj_ind |>
      dplyr::mutate(a1 = a1 |> signif(2), a2 = a2 |> signif(2)),
    aes(x = a1, y = a2, label = proj),
    nudge_x = 0.1, nudge_y = 0.1
  )
path_p_height_weight_ind_alt <- projr::projr_path_get("cache", "fig", "p-height-weight-ind-alt.png")
cowplot::save_plot(
  plot = p,
  filename = path_p_height_weight_ind_alt,
  units = "cm",
  base_height = 5,
  base_width = 5
)
```


\centering

```{r}
#| results: asis
knitr::include_graphics(path_p_height_weight_ind_alt)
```

:::

::::


# How do we choose $a_1$ and $a_2$? {.smaller}

- Consider projecting onto the lines $[0.5, 2]$, $[1, -1]$ and $[1, 1]$:

```{r}
proj_list <- list(c(0.5, 2), c(1, 1), c(1, -1))
plot_tbl_proj <- purrr::map_df(proj_list, ~mutate_proj(plot_tbl, .x))

p <- ggplot(
  data = plot_tbl_proj
) +
  geom_vline(
    xintercept = 0, col = "gray50", linetype = "solid"
  ) +
  geom_hline(
    yintercept = 0, col = "gray50", linetype = "solid"
  ) +
  cowplot::theme_cowplot() +
  cowplot::background_grid(major = "xy") +
  theme( 
    panel.background = element_rect(fill = "white"), 
    plot.background = element_rect(fill = "white")
  ) +
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank()
  ) +
  labs(
    x = "Height",
    y = "Weight"
  ) +
  geom_point(aes(x = height, y = weight), alpha = 0.85, col = "dodgerblue") +
  geom_segment(
    data = plot_tbl_proj |>
      dplyr::group_by(proj) |>
      dplyr::slice(seq(1, 100, length.out = 5) |> round()) |>
      dplyr::ungroup(),
    aes(x = height, y = weight, xend = proj_x, yend = proj_y, group = proj),
    col = "gray25", arrow = arrow(length = unit(0.3, "cm")), linetype = "solid"
  ) +
  geom_abline(
    data = plot_tbl_proj |>
      dplyr::group_by(proj) |>
      dplyr::slice(1) |>
      dplyr::ungroup() |>
      dplyr::mutate(slope = a2 / a1),
    aes(intercept = 0, slope = slope),
  ) +
  facet_wrap(~proj, ncol = 3)
path_p_proj <- projr::projr_path_get("cache", "fig", "p-proj.png")
cowplot::save_plot(
  plot = p,
  filename = path_p_proj,
  units = "cm",
  base_height = 6,
  base_width = 12
)
```

\begin{figure}
\centering
\includegraphics[width=0.55\textwidth]{`r path_p_proj`}
\end{figure}

::::{.columns}

::: {.column width="50%"}
- Height and weight increase together, so $a_1$ and $a_2$ should both be positive.
- Beyond that, we want to choose $a_1$ and $a_2$ so that the projection captures the most variation in the data.
:::

::: {.column width="50%"}
- The variability of the (lengths of the) projections is 1.3 (0.5, 2), 0.3 (1, -1) and 1.7 (1, 1), respectively.
-The projection onto $[1, 1]$ captures the most variation.
:::

::::

# Further motivation for capturing variation

- On its own grounds, imagine that you could retain only one of two variables: one has zero variance (all the observations are the same) and the other has some positive variance. Clearly, you would choose the latter.

\pause

- Later, we will see that capturing maximal variation:
  - Minimises the distance from the original data to the projection.
  - Allows us to reconstruct the original variables with minimal error.

# Objective function: the first principal component

- Assume that our data $\symbf{X} \in \mathcal{R}^p$ are distributed with mean $\symbf{\symbf{\mu}}$ and covariance matrix $\symbf{\Sigma}$.
- We seek to find the linear combination of the variables that captures the most variation.
- For the first principal component, we seek to find $\symbf{a}_1$ such that

$$
\text{Var}(\symbf{X}\symbf{a}_1) = \symbf{a}_1^T\symbf{\Sigma}\symbf{a}_1
$$

- is maximized, subject to the constraint that $||\symbf{a}|| = 1$.
  - This is necessary as otherwise we could meaninglessly increase the variation without bound by making $\symbf{a}_1$ longer.
- Applied to the previous example, the first principal component would be a linear combination of height and weight that captures the most variation.

# First principal component for health and weight data

```{r}
set.seed(2106)
plot_tbl <- tibble::tibble(
  height = rnorm(100, 160, 10)
) |>
  dplyr::mutate(
    weight = height / 2.5 + rnorm(100, 0, 3.5 * height / 160)
  ) |>
  dplyr::mutate(
    across(everything(), function(x) (x - mean(x)) / sd(x))
  ) |>
  dplyr::arrange(height)
proj_list <- list(c(1 / sqrt(2), 1 / sqrt(2)), c(1 / sqrt(2), -1 / sqrt(2)))[1]
plot_tbl_ind <- tibble::tibble(
  height = 1.2,
  weight = 0.3
)
plot_tbl_proj_ind <- proj_list |>
  purrr::map_df(~ mutate_proj(plot_tbl_ind, .x))

p <- ggplot(
  data = plot_tbl
) +
  geom_vline(
    xintercept = 0, col = "gray50", linetype = "solid"
  ) +
  geom_hline(
    yintercept = 0, col = "gray50", linetype = "solid"
  ) +
  cowplot::theme_cowplot() +
  cowplot::background_grid(major = "xy") +
  theme(
    panel.background = element_rect(fill = "white"),
    plot.background = element_rect(fill = "white")
  ) +
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank()
  ) +
  labs(
    x = "Height",
    y = "Weight"
  ) +
  geom_point(aes(x = height, y = weight), alpha = 0.1, col = "dodgerblue") +
  geom_point(
    data = plot_tbl_proj_ind,
    aes(x = height, y = weight), col = "mediumseagreen"
  ) +
  geom_segment(
    data = plot_tbl_proj_ind,
    aes(x = height, y = weight, xend = proj_x, yend = proj_y, group = proj),
    col = "gray25", arrow = arrow(length = unit(0.3, "cm")), linetype = "solid"
  ) +
  geom_abline(
    data = plot_tbl_proj_ind |>
      dplyr::mutate(slope = a2 / a1) ,
    aes(intercept = 0, slope = slope),
    col = "purple"
  ) +
  coord_cartesian(xlim = c(-2.5, 2.5), ylim = c(-2.5, 2.5)) +
  ggrepel::geom_text_repel(
    data = plot_tbl_proj_ind |>
      dplyr::mutate(a1 = a1 |> signif(2), a2 = a2 |> signif(2)),
    aes(x = a1, y = a2, label = proj),
    nudge_x = 0.1, nudge_y = 0.1
  )
path_p_height_weight_ex_1 <- projr::projr_path_get("cache", "fig", "p-height-weight-ex-1.png")
cowplot::save_plot(
  plot = p,
  filename = path_p_height_weight_ex_1,
  units = "cm",
  base_height = 8,
  base_width = 8
)
```

\centering

```{r}
#| results: asis
knitr::include_graphics(path_p_height_weight_ex_1)
```

# Objective function: the second principal component

- The second principal component is the linear combination of the variables that captures the most variation, subject to the constraint that it is orthogonal to the first principal component.
- In other words, for the second principal component we seek to find $\symbf{a}_2$ such that

$$
\text{Var}(\symbf{X}\symbf{a}_2) = \symbf{a}_2^T\symbf{\Sigma}\symbf{a}_2
$$

- is maximized, subject to the constraints that $||\symbf{a}_2|| = 1$ and $\symbf{a}_2^T\symbf{a}_1 = 0$.

# First principal component for health and weight data

```{r}
set.seed(2106)
plot_tbl <- tibble::tibble(
  height = rnorm(100, 160, 10)
) |>
  dplyr::mutate(
    weight = height / 2.5 + rnorm(100, 0, 3.5 * height / 160)
  ) |>
  dplyr::mutate(
    across(everything(), function(x) (x - mean(x)) / sd(x))
  ) |>
  dplyr::arrange(height)
proj_list <- list(c(1 / sqrt(2), 1 / sqrt(2)), c(1 / sqrt(2), -1 / sqrt(2)))
plot_tbl_ind <- tibble::tibble(
  height = 1.2,
  weight = 0.3
)
plot_tbl_proj_ind <- proj_list |>
  purrr::map_df(~ mutate_proj(plot_tbl_ind, .x))

p <- ggplot(
  data = plot_tbl
) +
  geom_vline(
    xintercept = 0, col = "gray50", linetype = "solid"
  ) +
  geom_hline(
    yintercept = 0, col = "gray50", linetype = "solid"
  ) +
  cowplot::theme_cowplot() +
  cowplot::background_grid(major = "xy") +
  theme(
    panel.background = element_rect(fill = "white"),
    plot.background = element_rect(fill = "white")
  ) +
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank()
  ) +
  labs(
    x = "Height",
    y = "Weight"
  ) +
  geom_point(aes(x = height, y = weight), alpha = 0.1, col = "dodgerblue") +
  geom_point(
    data = plot_tbl_proj_ind,
    aes(x = height, y = weight), col = "mediumseagreen"
  ) +
  geom_segment(
    data = plot_tbl_proj_ind,
    aes(x = height, y = weight, xend = proj_x, yend = proj_y, group = proj),
    col = "gray25", arrow = arrow(length = unit(0.3, "cm")), linetype = "solid"
  ) +
  geom_abline(
    data = plot_tbl_proj_ind |>
      dplyr::mutate(slope = a2 / a1) ,
    aes(intercept = 0, slope = slope),
    col = "purple"
  ) +
  coord_cartesian(xlim = c(-2.5, 2.5), ylim = c(-2.5, 2.5)) +
  ggrepel::geom_text_repel(
    data = plot_tbl_proj_ind |>
      dplyr::mutate(a1 = a1 |> signif(2), a2 = a2 |> signif(2)),
    aes(x = a1, y = a2, label = proj),
    nudge_x = 0.1, nudge_y = 0.1
  )
path_p_height_weight_ex_2 <- projr::projr_path_get("cache", "fig", "p-height-weight-ex-2.png")
cowplot::save_plot(
  plot = p,
  filename = path_p_height_weight_ex_2,
  units = "cm",
  base_height = 8,
  base_width = 8
)
```

\centering

```{r}
#| results: asis
knitr::include_graphics(path_p_height_weight_ex_2)
```

# Objective function for all principal components

- For the $k$-th principal component (for $k\leq p$), we seek to find $\symbf{a}_k$ such that

$$
\text{Var}(\symbf{X}\symbf{a}_k) = \symbf{a}_k^T\symbf{\Sigma}\symbf{a}_k
$$

- is maximized, subject to the constraints that $||\symbf{a}_k|| = 1$ and $\symbf{a}_k^T\symbf{a}_j = 0$ for $j < k$.

\pause

- Up until this point, we've explored various options for $\symbf{a}_1$ and $\symbf{a}_2$ without any method. We'll now discuss how to compute these principal components.
- First, we'll discuss a more general results.

# Theorem 1: Maximisation of quadratic forms for points on the unit sphere

Let $\symbf{B}:p\times p$ be a positive semi-definite matrix with the $i$-th largest eigenvalue $\lambda_i$ and associated eigenvector $\symbf{e}_i$. Then we have that 

$$
\max_{\symbf{x}\neq \symbf{0}} \frac{\symbf{x}'\symbf{B}\symbf{x}}{\symbf{x}'\symbf{x}}=\lambda_1 \; (\mathrm{attained}\; \mathrm{when} \; \symbf{x}=\symbf{e}_1)
$$

and that

$$
\max_{\symbf{x}\neq \symbf{0},\symbf{x}\perp \symbf{e}_1, \symbf{e}_2, \ldots, \symbf{e}_{i-1}} \frac{\symbf{x}'\symbf{B}\symbf{x}}{\symbf{x}'\symbf{x}}=\lambda_i \; (\mathrm{attained}\; \mathrm{when} \; \symbf{x}=\symbf{e}_{i})
$$

for $i\in\{2,3, \ldots, p\}$.

# Proof of Theorem 1 {.smaller}

Let $\symbf{P}:p\times p$ be the orthogonal matrix whose $i$-th column is the $i$-th eigenvector and $\Lambda$ be the diagonal matrix with ordered eigenvalues along the diagonal. Let $\symbf{B}^{1/2}=\symbf{P}\symbf{\Lambda}^{1/2}\symbf{P}'$ and $\symbf{y}=\symbf{P}'\symbf{x}$.

First, we show that the quadratic form can never be larger than $\lambda_1$:

\begin{align*} 
\frac{\symbf{x}'\symbf{B}\symbf{x}}{\symbf{x}'\symbf{x}} &=  \frac{\symbf{x}'\symbf{B}^{1/2}\symbf{B}^{1/2}\symbf{x}}{\symbf{x}'\symbf{P}\symbf{P}'\symbf{x}} 
= \frac{\symbf{x}'\symbf{P}\symbf{\Lambda}^{1/2}\symbf{P}'\symbf{P}\symbf{\Lambda}^{1/2}\symbf{P}'\symbf{x}}{\symbf{y}'\symbf{y}}
=  \frac{\symbf{y}'\symbf{\Lambda}\symbf{y}}{\symbf{y}'\symbf{y}} \\ 
 &=  \frac{\sum_{i=1}^p\lambda_iy_i^2}{\sum_{i=1}^py_i^2} 
 \leq \lambda_1 \frac{\sum_{i=1}^py_i^2}{\sum_{i=1}^py_i^2}
 = \lambda_1 
\end{align*}

# Proof of Theorem 1 (cont.) {.smaller}

Now we show that this is actually attained for $\symbf{x}=\symbf{e}_1$. Since eigenvectors are by convention length 1, we consider $\symbf{e}_1'\symbf{B}\symbf{e}_1$.

First, let $\symbf{c}_i$ be the unit vector with a 1 in the $i$-th position (and 0's everywhere else).

Expanding $\symbf{B}$ by the eigen decomposition, we have that

$$
\symbf{e}_1'\symbf{B}\symbf{e}_1=\symbf{e}_1'\symbf{P}\symbf{\Lambda}\symbf{P}'\symbf{e}_1.
$$

Since $B$ is symmetric, the eigenvectors can be chosen orthogonal.
We do so, which implies that $\symbf{e}_i'P=\symbf{c}_i'$, where $\symbf{c}_i$ is the unit vector with a 1 in the $i$-th position (and 0 everwhere else).
Consequently,

$$
\symbf{e}_i'\symbf{P}\symbf{\Lambda}\symbf{P}'\symbf{e}_i
= \symbf{c}_i'\symbf{\Lambda}\symbf{c}_i=\lambda_i,
$$

and so $\symbf{e}_1'\symbf{P}\symbf{\Lambda}\symbf{P}'\symbf{e}_1=\lambda_1$.

# Proof of Theorem 1 (cont.) {.smaller}

Now, we consider the case where $\symbf{x}$ is orthogonal to $\symbf{e}_1, \symbf{e}_2, \ldots, \symbf{e}_{i-1}$.

Each component in the vector $\symbf{y}$ is the dot product of $x$ and an eigenvector $\symbf{e}_i$.
Since we choose $x$ orthogonal to the first $i-1$ eigenvectors, the first $i-1$ entries of $\symbf{y}$ are zero.

Returning to considering the quadratic form, we have that

\begin{align}
\frac{\symbf{x}'\symbf{B}\symbf{x}}{\symbf{x}'\symbf{x}} 
&= \frac{\sum_{j=1}^p\lambda_jy_j^2}{\sum_{j=1}^py_j^2} \\
&= \frac{\sum_{j=i}^p\lambda_jy_j^2}{\sum_{j=i}^py_j^2} 
 \leq \lambda_i \frac{\sum_{j=i}^py_j^2}{\sum_{j=i}^py_j^2} 
 = \lambda_i
\end{align}

Using the same argument as before, we can show that this is actually attained for $\symbf{x}=\symbf{e}_i$.

# Connection with principal components

- Theorem 1 has already done the heavy lifting, as we can simply set $\symbf{B}=\symbf{\Sigma}$.
- All that's left to do is place Theorem 1 in a probabilistic context, and relate it to the variance and covariance of linear combinations of a random vector.

# Theorem 2: Selecting principal components {.smaller}

Let $\symbf{X}:\symbf{p}\times 1$ be the random vector with variance-covariance matirx $\symbf{\Sigma}:p\times p$, which has the $i$-th largest eigenvalue as $\lambda_i$ and associated eigenvector $\symbf{e}_i$.

Then the $i$-th principal component (defined as before) is given by 

$$
Y_i = \symbf{e}_i'\symbf{X},
$$

implying that

\begin{align*}
\mathrm{Var}[Y_i] &= \lambda_i \;\forall \; i \in \{1,2, \ldots, p\}, \; \mathrm{and} \\
\mathrm{Cov}[Y_i, Y_j] &= 0 \; \mathrm{for} \; i\neq j.
\end{align*}


# Proof of Theorem 2 {.smaller}

From Theorem 1, we know that 

$$
\max_{\symbf{x}} \frac{\symbf{x}'\symbf{\Sigma}\symbf{x}}{\symbf{x}'\symbf{x}} = \lambda_1,
$$

which we achieve by setting $\symbf{x}=\symbf{e}_1$.

Since $||\symbf{e}_1||=1$, $\frac{\symbf{e}_1'\symbf{\Sigma}\symbf{e}_1}{\symbf{e}_1'\symbf{e}_1}=\symbf{e}_1'\symbf{\Sigma}\symbf{e}_1$, which is equal to $\mathrm{Var}[\symbf{a}'\mathrm{X}]=\mathrm{Var}[Y_1]$ if we set $\symbf{a}=\symbf{e}_1$. This implies that $\symbf{a}=\symbf{e}_1$ maximises the variance, which is $\lambda_1$.

Analagous reasoning shows that $\mathrm{Var}[\symbf{a}_i'\symbf{X}]$ has a maximum value $\lambda_i$ for $i\in\{2,3,\ldots,p\}$ that is attained when we set $\symbf{a}_i=\symbf{e}_i$, under the restriction $\symbf{a}_i$ to be orthogonal to $\symbf{a}_1, \symbf{a}_2, \ldots, \symbf{a}_{i-1}$.

For $i\neq j$, $\mathrm{Cov}[\symbf{e}_i'\symbf{X},\symbf{e}_j'\symbf{X}] = \symbf{e}_i'\symbf{\Sigma}\symbf{e}_j = 0$.

# A note on terminology

- Principal components (the $\symbf{Y}_i$'s) may also be referred to as "scores".
- The coefficient vectors $\symbf{a}_i$ may also be referred to as "loadings" (thinking algebraically) or as "principal axes"/"principal directions" (thinking geometrically).

# Theorem three: Total variance {.smaller}

Let $X' = [X_1, X_2, \ldots, X_p]$ be a data matrix with covariance matrix $\Sigma$ and eigenvalue-eigenvector pairs $(\lambda_1, e_1), \ldots, (\lambda_p, e_p)$, ordered such that $\lambda_1 \geq \lambda_2 \geq \ldots \geq \lambda_p \geq 0$. Define the principal components $Y_i = e'_iX'$ for $i = 1, \ldots, p$. Then, the sum of variances of the original variables is equal to the sum of the eigenvalues of $\Sigma$, which is also equal to the sum of variances of the principal components:
$$
\sum_{i=1}^{p} \sigma_{ii} = \sum_{i=1}^{p} \text{Var}(X_i) = \sum_{i=1}^{p} \lambda_i = \sum_{i=1}^{p} \text{Var}(Y_i).
$$

# Proof of theorem three {.smaller}

The trace of the covariance matrix $\Sigma$, which is the sum of its diagonal elements, equals the sum of its eigenvalues due to the properties of the trace operator and the orthogonality of eigenvectors:

$$
\text{trace}(\Sigma) = \sum_{i=1}^{p} \sigma_{ii} = \sum_{i=1}^{p} \lambda_i.
$$

Since $\Sigma$ can be decomposed as $\Sigma = PDP'$, where $D$ is the diagonal matrix of eigenvalues and $P$ is the matrix of eigenvectors, we have:

$$
\text{trace}(\Sigma) = \text{trace}(PDP') = \text{trace}(D) = \sum_{i=1}^{p} \lambda_i.
$$

Therefore, the total variance accounted for by the $k$-th principal component is given by the proportion:

$$
p_k = \frac{\lambda_k}{\sum_{i=1}^{p} \lambda_i},
$$

for $k = 1, \ldots, p$.



# Theorem 4: Correlation with original variables

If $Y_1 = e_1'X, Y_2 = e_2'X, \ldots, Y_p = e_p'X$ are the principal components obtained from the covariance matrix $\Sigma$, then
$\rho_{Y_iX_k} = \frac{e_{ik}\sqrt{\lambda_i}}{\sqrt{\sigma_{kk}}} , \quad i,k = 1,2,\ldots,p$
are the correlation coefficients between the components $Y_i$ and variable $X_k$, where $(\lambda_1,e_1),\ldots,(\lambda_p,e_p)$ are eigenvalue-eigenvector pairs of $\Sigma$.

\textbf{Proof:}
Let $a_k' = [0,\ldots,0,1,0,\ldots,0]$ so that $X_k = a_k'X$ and $\text{Cov}(X_k,Y_i) = \text{Cov}(a_k'e_i',e_i')=a_k'e_i'e_i'$.
Since $\Sigma e_i= \lambda_ie_i$ it follows that

$$\text{Cov}(X_k,Y_i)=a_k'e_i'=a_k'\lambda_ie_i=e_{ik}.$$

We know that $\text{Var}(Y_i)= \lambda_i$ and $\text{Var}(X_k)=\sigma_{kk}$ so

$$
\rho_{Y_iX_k}=\frac{\text{Cov}(Y_i,X_k)}{\sqrt{\text{Var}(Y_i)}\sqrt{\text{Var}
(X_k)}}= \frac{\lambda_ie_{ik}}{e_{ik}\sqrt{\lambda_i}\sqrt{\sigma_{kk}}}
$$

# Example {.smaller}

::::{.columns}

:::{.column width="50%"}

Suppose three random variables $X_1$, $X_2$, and $X_3$ are thus correlated:

```{r}
#| echo: true
sigma_mat <- matrix(
  c(1, -2, 0,
    -2, 5, 0,
    0, 0, 2),
  byrow = TRUE,
  nrow = 3
)
sigma_mat
```

:::

:::{.column width="50%"}

We apply the eigen decomposition to the covariance matrix:

```{r}
#| echo: true
eig_obj <- eigen(sigma_mat)
```

to obtain the eigenvalues:

```{r}
eig_obj$values |> signif(3)
```

and the eigenvectors:

```{r}
#| echo: true
eig_obj$vectors |> signif(3)
```

:::
::::

# Constant-density contours I {.smaller}

By the definition of the MVN PDF, the density of $X$ is constant on the ellipsoid defined by the equation

$$
(\symbf{x} - \mu)'\Sigma^{-1}(\symbf{x} - \mu) = c^2.
$$

This ellipsoid's shape, size and orientation is determined by the eigendecomposition of $\Sigma$.

WLOG, we assume $\mu = 0$. Then, the ellipsoid is defined by

$$
c^2 = x'\Sigma^{-1}x = \frac{1}{\lambda_1}(e_1'x)^2 + \frac{1}{\lambda_2}(e_2'x)^2 + \ldots + \frac{1}{\lambda_p}(e_p'x)^2,
$$

by orthogonality of the eigenvectors.

# Constant-density contours II {.smaller}

As the eigenvectors are orthogonal, the axes are given by

$$
\pm c\sqrt{\lambda_i}\symbf{e}_i, \quad i = 1,2,\ldots,p.
$$

The eigenvectors therefore define the direction and the eigenvalues the length of the axes of the ellipsoid.

Furthermore, if we set $y_i = \symbf{e}_i'\symbf{x}$ (the $i$-th principal componet for $\symbf{x}$), we have

$$
c^2 = \frac{1}{\lambda_1}y_1^2 + \ldots + \frac{1}{\lambda_p}y_p^2.
$$

The equation is therefore satisfied by the principal components of $\symbf{x}$.

# Variance standardisation

- Before applying PCA, one can standardise the variances to have unit variance.
- This may be appropriate when:
  - the variances have different units (e.g. weight in kg vs annual income in rands), or
  - the variances have different scales (e.g. some genes are extremely rare whereas others are abundantly expressed), or
  - we wish to eliminate any effect of differences in variance on downstream analyses, e.g. when applying penalised regression.
- Similar comments go for logging the data, or applying other transformations.
- All results go through in exactly the same way, with adjustments made in interpreting exactly what variables the principal components capture.

# Standardisation example

::::{.columns}

:::{.column width="50%"}

We have the following covariance matrix:

```{r}
cov_mat <- matrix(
  c(1, 4, 4, 100), nrow = 2
)
cov_mat
```

which produces the following eigendecomposition:

```{r}
eigen(cov_mat) |> lapply(function(x) signif(x, 2))
```

:::

:::{.column width="50%"}

The associated correlation matrix:

```{r}
diag_mat_sd_inv <- diag(diag(cov_mat), nrow = nrow(cov_mat)) |> sqrt() |> solve()
rho_mat <- diag_mat_sd_inv %*% cov_mat %*% diag_mat_sd_inv
rho_mat 
```

has the following eigendecomposition:

```{r}
eigen(rho_mat) |> lapply(function(x) signif(x, 2))
```

The eigenvectors are in very different directions.

:::

::::

# Sample principal components I {.smaller}

In practice, we do not know $\symbf{\Sigma}$ and must estimate it by the sample covariance matrix $\symbf{S}=n^{-1}\symbf{X}'\symbf{X}$.

For example, the eigen-decomposition of the sample covariance matrix of the height and weight data is:

```{r}
cov_mat <- hw_tbl |>
  dplyr::select(height, weight) |>
  as.matrix() |>
  cov()
eigen(cov_mat) |> lapply(function(x) signif(x, 2))
```

So, the first principal component is given by projections onto $[1, 1]$.

If $\symbf{X} \sim \mathcal{N}(\symbf{\mu}, \symbf{\Sigma})$, then we have that that

$$
Y_i \sim \mathcal{N}(0, \lambda_i) \quad \text{and} \quad \symbf{Y} \sim \mathcal{N}_p(\symbf{0}, \symbf{\Lambda}).
$$

# Approximating a matrix $\symbf{X}$

Suppose we have a matrix $\symbf{X}$ of size $n \times p$ with rank $t$ and we wish to approximate it with a matrix $\hat{\symbf{X}}$ of rank $s < t$.

In other words, we wish to find a matrix

$$
\hat{\symbf{X}} = \symbf{A}\symbf{B},
$$

where $\symbf{A}$ is of size $n \times s$ and $\symbf{B}$ is of size $s \times p$, such that

$$
\symbf{X} \approx \hat{\symbf{X}}.
$$

We want a rank $s$ approximation to the matrix $\symbf{X}$.

# Approximation example 

In the height and weight dataset, we obtained a size variable (the first principal component) that was an equally-weighted combination of weight and height, as follows:

$$
\text{size} = \frac{1}{\sqrt{2}}\text{height} + \frac{1}{\sqrt{2}}\text{weight}. = \begin{pmatrix} \text{height} && \text{weight} \end{pmatrix} \begin{pmatrix} \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} \end{pmatrix}.
$$

We'll show that can approximate the original height and weight variables using the size variable:

$$
[\text{height}, \text{weight}] \approx \text{size} \begin{pmatrix} \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \end{pmatrix}  = \begin{pmatrix} 1/\sqrt{2} \text{size} & 1/\sqrt{2} \text{size} \end{pmatrix}.
$$

We therefore have a "rule" for mapping size onto weight and height.

# Approximation example cont. {.smaller}

Suppose that a given observation has height 0.7 and weight 0.5.

This implies a first principal component (size) of

$$
\text{size} = \frac{1}{\sqrt{2}}\times 0.7 + \frac{1}{\sqrt{2}}\times 0.5 = 0.85.
$$

We can approximate the height and weight as follows:

$$
\begin{pmatrix} \hat{\text{height}} & \hat{\text{weight}} \end{pmatrix} \approx \begin{pmatrix} 1/\sqrt{2} \times 0.85 & 1/\sqrt{2} \times 0.85 \end{pmatrix} = \begin{pmatrix} 0.6 & 0.6 \end{pmatrix}.
$$

Considering the standard deviation is (by construction) 1, the error (0.1 in both cases) is small.

If weight and height were very different, the error would be larger (why?).

Whether or not PCA is a good approximation matters because a) we may actually want to reconstitute the original data after compression (e.g. image compression) and b) we want to know we're not losing too much information.


# Minimising the least squares error of the approximation

We wish to find a matrix $\hat{\symbf{X}}$ of rank $s$ that that minimises the least squares error:

$$
\text{min}_{\hat{\symbf{X}}} \sum_{i=1}^{n} \sum_{j=1}^{p} (x_{ij} - \hat{x}_{ij})^2 = \text{min}_{\hat{\symbf{X}}} \text{trace}( (\symbf{X} - \hat{\symbf{X}})(\symbf{X} - \hat{\symbf{X}})').
$$

We are going to

1. prove that SVD can be used to find $\hat{\symbf{X}}$, and
2. that the PCA and SVD are equivalent in this context.

# Theorem 5a: SVD approximation

Let $\symbf{X}$ be a matrix of size $n \times p$ with rank $t$ and SVD $\symbf{X} = \symbf{U}\symbf{D}\symbf{V}'$. Then the best rank $s$ approximation to $\symbf{X}$ is given by

$$
\hat{\symbf{X}} = \symbf{U}\symbf{D}\symbf{J}_s\symbf{V}',
$$

where $\symbf{J}_s$ is the matrix of size $p \times p$ with the first $s$ diagonal elements equal to 1 and the rest equal to 0.

This is equivalent to

$$
\hat{\symbf{X}} = \sum_{i=1}^{s} d_i\symbf{u}_i\symbf{v}_i',
$$

where $\symbf{u}_i$ and $\symbf{v}_i$ are the left- and right-singular vectors and $d_i$ is the $i$-th singular value of $\symbf{X}$.

# Proof of Theorem 5a {.smaller} 

\begin{align*}
& \text{We use } UU^* = I_m \text{ and } VV^* = I_k \text{ to write the sum of squares as} \\
& \text{tr}[(A - B)(A - B)^*] = \text{tr}[U^*(A - B)V^*(A - B)VU] \\
& = \text{tr}[U^*(A - BV)(A - BV)^*U] \\
& = \text{tr}[U^*(A - BV)U(U^*A - BV^*)] \\
& = \text{tr}[(UA-BV)(UA-BV)^*] \\
& \text{If we let } A' = UA \text{ and } C' = BV \\
& = \text{tr}[(C' - C')(C'^* - C')] \\
& = \sum_{i,j=1}^{m} (a_{ij} - c_{ij})^2 \\
& \text{which will be a minimum when } c_{ij} = 0 \text{ unless } i = j \text{ and } i \leq s \\
& \Rightarrow UB'V^* = \Lambda_s \text{ or } B \Rightarrow \sum_{i=1}^{s} \lambda_i u_i v_i^*.
\end{align*}


# Theorem 5b: PCA leads to same approximation as the SVD {.smaller}

The PCA approximation to $\symbf{X}$ is given by $\tilde{\symbf{X}}=\symbf{Y}\symbf{J}_k\symbf{P}'$, where $\symbf{J}_k$ is the matrix of size $p \times p$ with the first $k$ diagonal elements equal to 1 and the rest equal to 0.

This is the same as the SVD approximation, i.e.

$$
\hat{\symbf{X}} = \symbf{U}\symbf{D}\symbf{J}_k\symbf{V}' = \tilde{\symbf{X}} = \symbf{Y}\symbf{J}_k\symbf{P}'.
$$


# Theorom 5b: proof {.smaller}

By definition, $\symbf{P}=\symbf{V}$ as $\symbf{P}$ and $\symbf{V}$ both have the ordered eigenvectors of $\symbf{X}'\symbf{X}$ along the columns.

Therefore,
$$
\symbf{Y}=\symbf{X}\symbf{P}=\symbf{X}\symbf{V},
$$

we have that

$$
\symbf{Y}=\symbf{U}\symbf{D}\symbf{V}'\symbf{V}=\symbf{U}\symbf{D}=\hat{\symbf{X}}.
$$

# Contribution per component

Theorem 2 showed that
\begin{align*}
tr(\Sigma) &= tr(P\Lambda P') = tr(\Lambda PP') = tr(\Lambda) \\
&= \lambda_1 + \lambda_2 + ... + \lambda_p = \sum Var(Y_i).
\end{align*}

This implies that the proportion of the total variance due to the $k^{th}$ principal component is

$$
\frac{\lambda_k}{\lambda_1+\lambda_2+...+\lambda_p}, k=1,...p,
$$

and the proportion of variance accounted for by the first $r$ principal components is given by

$$
\frac{\lambda_1+\lambda_2+\ldots+\lambda_r}{\lambda_1+\lambda_2+...+\lambda_p}.
$$

# Scree plot

A scree plot of eigenvalues ($\lambda_i$) against indices $i$ can be used to determine the number of principal components to retain.

The point at which the plot levels off is the number of principal components to retain.
This point is typically referred to as an "elbow".

# Final example

:::: {.columns}

::: {.column width="57%"}

We read in data of measurements Painted Turtles [@jolicoeur_mosimann60]:

```{r}
#| eval: false
#| echo: true
if (!requireNamespace(
  "remotes", quietly = TRUE
  )) {
  install.packages("remotes")
}
"MiguelRodo/DataTidy23RodoHonsMult@2024" |>
  remotes::install_github()
```

:::

::: {.column width="43%"}

```{r}
#| echo: true
data(
  "data_tidy_turtle",
  package = "DataTidy23RodoHonsMult"
)
data_tidy_turtle
```

:::

::::

# Measurements highly correlated

::::{.columns}

:::{.column width="50%"}

```{r}
#| include: false
plot_tbl_turtle <- data_tidy_turtle |>
  tidyr::pivot_longer(
    length:width,
    names_to = "x_var",
    values_to = "x_val"
  )
p <- ggplot(
  plot_tbl_turtle,
  aes(x = x_val, y = height, col = gender)
) +
  geom_point(alpha = 0.75) +
  scale_colour_manual(
    values = c(male = "dodgerblue", female = "mediumseagreen")
   ) +
  facet_wrap(~x_var, scales = "free", ncol =1 ) +
  cowplot::theme_cowplot() +
  cowplot::background_grid(major = "xy") +
  theme( 
    panel.background = element_rect(fill = "white"), 
    plot.background = element_rect(fill = "white")
  ) +
  labs(
    x = "Measurement",
    y = "Height"
  ) +
  theme(
    legend.position = "bottom",
    legend.title = element_blank()
  ) +
  theme(
    axis.ticks = element_blank(),
    axis.text = element_blank()
  ) +
  theme(
    strip.background = element_rect(fill = "white", colour = "gray25")
  )
path_p_turtle <- projr::projr_path_get("cache", "fig", "p-turtle.png")
cowplot::save_plot(
  plot = p,
  filename = path_p_turtle,
  units = "cm",
  base_height = 7,
  base_width = 5
)
```

```{r}
#| results: "asis"
knitr::include_graphics(path_p_turtle)
```

:::

:::{.column width="50%"}

- Due to the correlation, we may likely summarise the data very well using principal components.
- We'll use the `prcomp` function in R, and do it from first principles.

:::

::::

# Using the `prcomp` function I {.smaller}

::::{.columns}

:::{.column width="70%"}

```{r}
#| echo: true
pr_obj <- prcomp(
  ~log(length) + log(width) + log(height),
  data = data_tidy_turtle,
  retx = TRUE
)
pr_obj
```

:::

:::{.column width="30%"}

- The first principal component has a markedly higher standard deviation.
- It is weighted roughly evenly across the variables, indicating it is a general "size" variable.

:::

::::

# Using the `prcomp` function II {.smaller}

::::{.columns}

:::{.column width="70%"}

```{r}
#| echo: true
summary(pr_obj)
```

:::

:::{.column width="30%"}

- The `summary` function provides the per-component standard deviation and variance, as well as the cumulative variance.

:::

::::

# Using the `prcomp` function III {.smaller}

::::{.columns}

:::{.column width="70%"}

```{r}
#| echo: true
plot(pr_obj)
```

:::

:::{.column width="30%"}

- The `plot` command provides the scree plot.
- Clearly, unless we're specifically interested in non-size variables we should only retain the first principal component.

:::

::::

# Using the `prcomp` function IV {.smaller}

Here are the principal components:

```{r}
#| echo: true
pr_obj$x |> head() |> signif(2)
```

# From first principles I {.smaller}

Using the SVD, we can obtain the same results as the `prcomp` function.

Removing gender, logging and centering the data:

```{r}
#| echo: true
data_turtle_mat <- data_tidy_turtle |>
  dplyr::select(-gender) |>
  dplyr::mutate(across(everything(), log)) |>
  dplyr::mutate(across(everything(), function(x) x - mean(x))) |>
  as.matrix()
```

We then calculate the SVD:

```{r}
#| echo: true
svd_turtle <- svd(data_turtle_mat)
```

# From first principles II {.smaller}

The eigenvalues are the squares of the singular values:

```{r}
#| echo: true
svd_turtle$d^2 |> signif(3)
```

The right-singular vectors are the eigenvectors of the covariance matrix, and hence are the loading vectors, implying the principal components are given by:

```{r}
#| echo: true
(data_turtle_mat %*% svd_turtle$v) |> head() |> signif(2)
```

# Biplots

- The axes of principal components are linear combinations of the original variables.
- This makes interpreting the principal components in terms of the original points more difficult.
- To alleviate this problem, we can plot the principal components and the original variables on the same plot.
- This is called a biplot.

# Definition of a biplot

Suppose we have a rank $s$ matrix $\hat{\symbf{X}}:n\times p$.
Then we write

$$
\hat{\symbf{X}} = \symbf{A}\symbf{B},
$$

where $\symbf{A}$ is of size $n \times s$ and $\symbf{B}$ is of size $s \times p$.

The biplot is a plot of the rows of $\symbf{A}$ and the columns of $\symbf{B}$.

Typically, we plot the rows as points and the columns as vectors.

# A PCA biplot

The PCA approximation to a matrix $\symbf{X}$ is given by

$$
\symbf{Y}\symbf{J}_k\symbf{P}'.
$$

Typically, we take the first two columns of $\symbf{Y}$ and the first two columns of $\symbf{P}$ to form the biplot.

The rows of $\symbf{Y}$ are the scores (representing the observations) and the rows of $\symbf{P}'$ are the loadings (representing the variables).

# Interpreting a biplot

- An observation has (approximately) an above-average value of a variable if it is close to the tip of the vector representing that variable.
- Two variables are (approximately) correlated if their vectors are close together.
  - They are (approximately) uncorrelated if they are orthogonal.

\centering

\includegraphics[width=0.65\textwidth]{_data_raw/img/biplot.png}
